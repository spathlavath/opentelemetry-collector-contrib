// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"fmt"
	"strconv"
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// AttributeCommand specifies the value command attribute.
type AttributeCommand int

const (
	_ AttributeCommand = iota
	AttributeCommandCommit
	AttributeCommandDelete
	AttributeCommandDeleteMulti
	AttributeCommandInsert
	AttributeCommandInsertSelect
	AttributeCommandLoad
	AttributeCommandReplace
	AttributeCommandReplaceSelect
	AttributeCommandRollback
	AttributeCommandSelect
	AttributeCommandUpdate
	AttributeCommandUpdateMulti
)

// String returns the string representation of the AttributeCommand.
func (av AttributeCommand) String() string {
	switch av {
	case AttributeCommandCommit:
		return "commit"
	case AttributeCommandDelete:
		return "delete"
	case AttributeCommandDeleteMulti:
		return "delete_multi"
	case AttributeCommandInsert:
		return "insert"
	case AttributeCommandInsertSelect:
		return "insert_select"
	case AttributeCommandLoad:
		return "load"
	case AttributeCommandReplace:
		return "replace"
	case AttributeCommandReplaceSelect:
		return "replace_select"
	case AttributeCommandRollback:
		return "rollback"
	case AttributeCommandSelect:
		return "select"
	case AttributeCommandUpdate:
		return "update"
	case AttributeCommandUpdateMulti:
		return "update_multi"
	}
	return ""
}

// MapAttributeCommand is a helper map of string to AttributeCommand attribute value.
var MapAttributeCommand = map[string]AttributeCommand{
	"commit":         AttributeCommandCommit,
	"delete":         AttributeCommandDelete,
	"delete_multi":   AttributeCommandDeleteMulti,
	"insert":         AttributeCommandInsert,
	"insert_select":  AttributeCommandInsertSelect,
	"load":           AttributeCommandLoad,
	"replace":        AttributeCommandReplace,
	"replace_select": AttributeCommandReplaceSelect,
	"rollback":       AttributeCommandRollback,
	"select":         AttributeCommandSelect,
	"update":         AttributeCommandUpdate,
	"update_multi":   AttributeCommandUpdateMulti,
}

var MetricsInfo = metricsInfo{
	NewrelicmysqlBinlogCacheDiskUse: metricInfo{
		Name: "newrelicmysql.binlog.cache_disk_use",
	},
	NewrelicmysqlBinlogCacheUse: metricInfo{
		Name: "newrelicmysql.binlog.cache_use",
	},
	NewrelicmysqlCommands: metricInfo{
		Name: "newrelicmysql.commands",
	},
	NewrelicmysqlConnectionCount: metricInfo{
		Name: "newrelicmysql.connection.count",
	},
	NewrelicmysqlDbHandlerRollback: metricInfo{
		Name: "newrelicmysql.db.handler_rollback",
	},
	NewrelicmysqlDbOpenedTables: metricInfo{
		Name: "newrelicmysql.db.opened_tables",
	},
	NewrelicmysqlGaleraWsrepCertDepsDistance: metricInfo{
		Name: "newrelicmysql.galera.wsrep_cert_deps_distance",
	},
	NewrelicmysqlGaleraWsrepClusterSize: metricInfo{
		Name: "newrelicmysql.galera.wsrep_cluster_size",
	},
	NewrelicmysqlGaleraWsrepFlowControlPaused: metricInfo{
		Name: "newrelicmysql.galera.wsrep_flow_control_paused",
	},
	NewrelicmysqlGaleraWsrepFlowControlPausedNs: metricInfo{
		Name: "newrelicmysql.galera.wsrep_flow_control_paused_ns",
	},
	NewrelicmysqlGaleraWsrepFlowControlRecv: metricInfo{
		Name: "newrelicmysql.galera.wsrep_flow_control_recv",
	},
	NewrelicmysqlGaleraWsrepFlowControlSent: metricInfo{
		Name: "newrelicmysql.galera.wsrep_flow_control_sent",
	},
	NewrelicmysqlGaleraWsrepLocalCertFailures: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_cert_failures",
	},
	NewrelicmysqlGaleraWsrepLocalRecvQueue: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_recv_queue",
	},
	NewrelicmysqlGaleraWsrepLocalRecvQueueAvg: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_recv_queue_avg",
	},
	NewrelicmysqlGaleraWsrepLocalSendQueue: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_send_queue",
	},
	NewrelicmysqlGaleraWsrepLocalSendQueueAvg: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_send_queue_avg",
	},
	NewrelicmysqlGaleraWsrepLocalState: metricInfo{
		Name: "newrelicmysql.galera.wsrep_local_state",
	},
	NewrelicmysqlGaleraWsrepReceived: metricInfo{
		Name: "newrelicmysql.galera.wsrep_received",
	},
	NewrelicmysqlGaleraWsrepReceivedBytes: metricInfo{
		Name: "newrelicmysql.galera.wsrep_received_bytes",
	},
	NewrelicmysqlGaleraWsrepReplicatedBytes: metricInfo{
		Name: "newrelicmysql.galera.wsrep_replicated_bytes",
	},
	NewrelicmysqlInnodbActiveTransactions: metricInfo{
		Name: "newrelicmysql.innodb.active_transactions",
	},
	NewrelicmysqlInnodbAdaptiveHashHashSearches: metricInfo{
		Name: "newrelicmysql.innodb.adaptive_hash_hash_searches",
	},
	NewrelicmysqlInnodbAdaptiveHashNonHashSearches: metricInfo{
		Name: "newrelicmysql.innodb.adaptive_hash_non_hash_searches",
	},
	NewrelicmysqlInnodbAdaptiveHashPagesAdded: metricInfo{
		Name: "newrelicmysql.innodb.adaptive_hash_pages_added",
	},
	NewrelicmysqlInnodbAdaptiveHashPagesRemoved: metricInfo{
		Name: "newrelicmysql.innodb.adaptive_hash_pages_removed",
	},
	NewrelicmysqlInnodbAvailableUndoLogs: metricInfo{
		Name: "newrelicmysql.innodb.available_undo_logs",
	},
	NewrelicmysqlInnodbBufferPoolBytesData: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_bytes_data",
	},
	NewrelicmysqlInnodbBufferPoolBytesDirty: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_bytes_dirty",
	},
	NewrelicmysqlInnodbBufferPoolDirty: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_dirty",
	},
	NewrelicmysqlInnodbBufferPoolFree: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_free",
	},
	NewrelicmysqlInnodbBufferPoolPagesData: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_data",
	},
	NewrelicmysqlInnodbBufferPoolPagesFlushed: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_flushed",
	},
	NewrelicmysqlInnodbBufferPoolPagesFree: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_free",
	},
	NewrelicmysqlInnodbBufferPoolPagesLruFlushed: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_lru_flushed",
	},
	NewrelicmysqlInnodbBufferPoolPagesMadeNotYoung: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_made_not_young",
	},
	NewrelicmysqlInnodbBufferPoolPagesMadeYoung: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_made_young",
	},
	NewrelicmysqlInnodbBufferPoolPagesMisc: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_misc",
	},
	NewrelicmysqlInnodbBufferPoolPagesOld: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_old",
	},
	NewrelicmysqlInnodbBufferPoolPagesTotal: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_pages_total",
	},
	NewrelicmysqlInnodbBufferPoolReadAhead: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_read_ahead",
	},
	NewrelicmysqlInnodbBufferPoolReadAheadEvicted: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_read_ahead_evicted",
	},
	NewrelicmysqlInnodbBufferPoolReadAheadRnd: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_read_ahead_rnd",
	},
	NewrelicmysqlInnodbBufferPoolReadRequests: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_read_requests",
	},
	NewrelicmysqlInnodbBufferPoolReads: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_reads",
	},
	NewrelicmysqlInnodbBufferPoolTotal: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_total",
	},
	NewrelicmysqlInnodbBufferPoolUsed: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_used",
	},
	NewrelicmysqlInnodbBufferPoolUtilization: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_utilization",
	},
	NewrelicmysqlInnodbBufferPoolWaitFree: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_wait_free",
	},
	NewrelicmysqlInnodbBufferPoolWriteRequests: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_write_requests",
	},
	NewrelicmysqlInnodbCheckpointAge: metricInfo{
		Name: "newrelicmysql.innodb.checkpoint_age",
	},
	NewrelicmysqlInnodbCurrentRowLocks: metricInfo{
		Name: "newrelicmysql.innodb.current_row_locks",
	},
	NewrelicmysqlInnodbCurrentTransactions: metricInfo{
		Name: "newrelicmysql.innodb.current_transactions",
	},
	NewrelicmysqlInnodbDataFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.data_fsyncs",
	},
	NewrelicmysqlInnodbDataPendingFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.data_pending_fsyncs",
	},
	NewrelicmysqlInnodbDataPendingReads: metricInfo{
		Name: "newrelicmysql.innodb.data_pending_reads",
	},
	NewrelicmysqlInnodbDataPendingWrites: metricInfo{
		Name: "newrelicmysql.innodb.data_pending_writes",
	},
	NewrelicmysqlInnodbDataRead: metricInfo{
		Name: "newrelicmysql.innodb.data_read",
	},
	NewrelicmysqlInnodbDataReads: metricInfo{
		Name: "newrelicmysql.innodb.data_reads",
	},
	NewrelicmysqlInnodbDataWrites: metricInfo{
		Name: "newrelicmysql.innodb.data_writes",
	},
	NewrelicmysqlInnodbDataWritten: metricInfo{
		Name: "newrelicmysql.innodb.data_written",
	},
	NewrelicmysqlInnodbDblwrPagesWritten: metricInfo{
		Name: "newrelicmysql.innodb.dblwr_pages_written",
	},
	NewrelicmysqlInnodbDblwrWrites: metricInfo{
		Name: "newrelicmysql.innodb.dblwr_writes",
	},
	NewrelicmysqlInnodbHashIndexCellsTotal: metricInfo{
		Name: "newrelicmysql.innodb.hash_index_cells_total",
	},
	NewrelicmysqlInnodbHashIndexCellsUsed: metricInfo{
		Name: "newrelicmysql.innodb.hash_index_cells_used",
	},
	NewrelicmysqlInnodbHistoryListLength: metricInfo{
		Name: "newrelicmysql.innodb.history_list_length",
	},
	NewrelicmysqlInnodbIbufFreeList: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_free_list",
	},
	NewrelicmysqlInnodbIbufMergedDeleteMarks: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_merged_delete_marks",
	},
	NewrelicmysqlInnodbIbufMergedDeletes: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_merged_deletes",
	},
	NewrelicmysqlInnodbIbufMergedInserts: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_merged_inserts",
	},
	NewrelicmysqlInnodbIbufMerges: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_merges",
	},
	NewrelicmysqlInnodbIbufSegmentSize: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_segment_size",
	},
	NewrelicmysqlInnodbIbufSize: metricInfo{
		Name: "newrelicmysql.innodb.ibuf_size",
	},
	NewrelicmysqlInnodbLockStructs: metricInfo{
		Name: "newrelicmysql.innodb.lock_structs",
	},
	NewrelicmysqlInnodbLockedTables: metricInfo{
		Name: "newrelicmysql.innodb.locked_tables",
	},
	NewrelicmysqlInnodbLockedTransactions: metricInfo{
		Name: "newrelicmysql.innodb.locked_transactions",
	},
	NewrelicmysqlInnodbLogWaits: metricInfo{
		Name: "newrelicmysql.innodb.log_waits",
	},
	NewrelicmysqlInnodbLogWriteRequests: metricInfo{
		Name: "newrelicmysql.innodb.log_write_requests",
	},
	NewrelicmysqlInnodbLogWrites: metricInfo{
		Name: "newrelicmysql.innodb.log_writes",
	},
	NewrelicmysqlInnodbLsnCurrent: metricInfo{
		Name: "newrelicmysql.innodb.lsn_current",
	},
	NewrelicmysqlInnodbLsnFlushed: metricInfo{
		Name: "newrelicmysql.innodb.lsn_flushed",
	},
	NewrelicmysqlInnodbLsnLastCheckpoint: metricInfo{
		Name: "newrelicmysql.innodb.lsn_last_checkpoint",
	},
	NewrelicmysqlInnodbMasterThreadActiveLoops: metricInfo{
		Name: "newrelicmysql.innodb.master_thread_active_loops",
	},
	NewrelicmysqlInnodbMasterThreadIdleLoops: metricInfo{
		Name: "newrelicmysql.innodb.master_thread_idle_loops",
	},
	NewrelicmysqlInnodbMemAdaptiveHash: metricInfo{
		Name: "newrelicmysql.innodb.mem_adaptive_hash",
	},
	NewrelicmysqlInnodbMemAdditionalPool: metricInfo{
		Name: "newrelicmysql.innodb.mem_additional_pool",
	},
	NewrelicmysqlInnodbMemDictionary: metricInfo{
		Name: "newrelicmysql.innodb.mem_dictionary",
	},
	NewrelicmysqlInnodbMemFileSystem: metricInfo{
		Name: "newrelicmysql.innodb.mem_file_system",
	},
	NewrelicmysqlInnodbMemLockSystem: metricInfo{
		Name: "newrelicmysql.innodb.mem_lock_system",
	},
	NewrelicmysqlInnodbMemPageHash: metricInfo{
		Name: "newrelicmysql.innodb.mem_page_hash",
	},
	NewrelicmysqlInnodbMemRecoverySystem: metricInfo{
		Name: "newrelicmysql.innodb.mem_recovery_system",
	},
	NewrelicmysqlInnodbMemThreadHash: metricInfo{
		Name: "newrelicmysql.innodb.mem_thread_hash",
	},
	NewrelicmysqlInnodbMemTotal: metricInfo{
		Name: "newrelicmysql.innodb.mem_total",
	},
	NewrelicmysqlInnodbMutexOsWaits: metricInfo{
		Name: "newrelicmysql.innodb.mutex_os_waits",
	},
	NewrelicmysqlInnodbMutexSpinRounds: metricInfo{
		Name: "newrelicmysql.innodb.mutex_spin_rounds",
	},
	NewrelicmysqlInnodbMutexSpinWaits: metricInfo{
		Name: "newrelicmysql.innodb.mutex_spin_waits",
	},
	NewrelicmysqlInnodbNumOpenFiles: metricInfo{
		Name: "newrelicmysql.innodb.num_open_files",
	},
	NewrelicmysqlInnodbOsFileFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.os_file_fsyncs",
	},
	NewrelicmysqlInnodbOsFileReads: metricInfo{
		Name: "newrelicmysql.innodb.os_file_reads",
	},
	NewrelicmysqlInnodbOsFileWrites: metricInfo{
		Name: "newrelicmysql.innodb.os_file_writes",
	},
	NewrelicmysqlInnodbOsLogFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.os_log_fsyncs",
	},
	NewrelicmysqlInnodbOsLogPendingFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.os_log_pending_fsyncs",
	},
	NewrelicmysqlInnodbOsLogPendingWrites: metricInfo{
		Name: "newrelicmysql.innodb.os_log_pending_writes",
	},
	NewrelicmysqlInnodbOsLogWritten: metricInfo{
		Name: "newrelicmysql.innodb.os_log_written",
	},
	NewrelicmysqlInnodbPageSize: metricInfo{
		Name: "newrelicmysql.innodb.page_size",
	},
	NewrelicmysqlInnodbPagesCreated: metricInfo{
		Name: "newrelicmysql.innodb.pages_created",
	},
	NewrelicmysqlInnodbPagesRead: metricInfo{
		Name: "newrelicmysql.innodb.pages_read",
	},
	NewrelicmysqlInnodbPagesWritten: metricInfo{
		Name: "newrelicmysql.innodb.pages_written",
	},
	NewrelicmysqlInnodbPendingAioLogIos: metricInfo{
		Name: "newrelicmysql.innodb.pending_aio_log_ios",
	},
	NewrelicmysqlInnodbPendingAioSyncIos: metricInfo{
		Name: "newrelicmysql.innodb.pending_aio_sync_ios",
	},
	NewrelicmysqlInnodbPendingBufferPoolFlushes: metricInfo{
		Name: "newrelicmysql.innodb.pending_buffer_pool_flushes",
	},
	NewrelicmysqlInnodbPendingCheckpointWrites: metricInfo{
		Name: "newrelicmysql.innodb.pending_checkpoint_writes",
	},
	NewrelicmysqlInnodbPendingIbufAioReads: metricInfo{
		Name: "newrelicmysql.innodb.pending_ibuf_aio_reads",
	},
	NewrelicmysqlInnodbPendingLogFlushes: metricInfo{
		Name: "newrelicmysql.innodb.pending_log_flushes",
	},
	NewrelicmysqlInnodbPendingLogWrites: metricInfo{
		Name: "newrelicmysql.innodb.pending_log_writes",
	},
	NewrelicmysqlInnodbPendingNormalAioReads: metricInfo{
		Name: "newrelicmysql.innodb.pending_normal_aio_reads",
	},
	NewrelicmysqlInnodbPendingNormalAioWrites: metricInfo{
		Name: "newrelicmysql.innodb.pending_normal_aio_writes",
	},
	NewrelicmysqlInnodbPurgeTrxID: metricInfo{
		Name: "newrelicmysql.innodb.purge_trx_id",
	},
	NewrelicmysqlInnodbPurgeUndoNo: metricInfo{
		Name: "newrelicmysql.innodb.purge_undo_no",
	},
	NewrelicmysqlInnodbQueriesInside: metricInfo{
		Name: "newrelicmysql.innodb.queries_inside",
	},
	NewrelicmysqlInnodbQueriesQueued: metricInfo{
		Name: "newrelicmysql.innodb.queries_queued",
	},
	NewrelicmysqlInnodbReadViews: metricInfo{
		Name: "newrelicmysql.innodb.read_views",
	},
	NewrelicmysqlInnodbRedoLogEnabled: metricInfo{
		Name: "newrelicmysql.innodb.redo_log_enabled",
	},
	NewrelicmysqlInnodbRowLockCurrentWaits: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_current_waits",
	},
	NewrelicmysqlInnodbRowLockTime: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_time",
	},
	NewrelicmysqlInnodbRowLockTimeAvg: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_time_avg",
	},
	NewrelicmysqlInnodbRowLockTimeMax: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_time_max",
	},
	NewrelicmysqlInnodbRowLockWaits: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_waits",
	},
	NewrelicmysqlInnodbRowsDeleted: metricInfo{
		Name: "newrelicmysql.innodb.rows_deleted",
	},
	NewrelicmysqlInnodbRowsInserted: metricInfo{
		Name: "newrelicmysql.innodb.rows_inserted",
	},
	NewrelicmysqlInnodbRowsRead: metricInfo{
		Name: "newrelicmysql.innodb.rows_read",
	},
	NewrelicmysqlInnodbRowsUpdated: metricInfo{
		Name: "newrelicmysql.innodb.rows_updated",
	},
	NewrelicmysqlInnodbSLockOsWaits: metricInfo{
		Name: "newrelicmysql.innodb.s_lock_os_waits",
	},
	NewrelicmysqlInnodbSLockSpinRounds: metricInfo{
		Name: "newrelicmysql.innodb.s_lock_spin_rounds",
	},
	NewrelicmysqlInnodbSLockSpinWaits: metricInfo{
		Name: "newrelicmysql.innodb.s_lock_spin_waits",
	},
	NewrelicmysqlInnodbSemaphoreWaitTime: metricInfo{
		Name: "newrelicmysql.innodb.semaphore_wait_time",
	},
	NewrelicmysqlInnodbSemaphoreWaits: metricInfo{
		Name: "newrelicmysql.innodb.semaphore_waits",
	},
	NewrelicmysqlInnodbTablesInUse: metricInfo{
		Name: "newrelicmysql.innodb.tables_in_use",
	},
	NewrelicmysqlInnodbTruncatedStatusWrites: metricInfo{
		Name: "newrelicmysql.innodb.truncated_status_writes",
	},
	NewrelicmysqlInnodbUndoTablespacesActive: metricInfo{
		Name: "newrelicmysql.innodb.undo_tablespaces_active",
	},
	NewrelicmysqlInnodbUndoTablespacesExplicit: metricInfo{
		Name: "newrelicmysql.innodb.undo_tablespaces_explicit",
	},
	NewrelicmysqlInnodbUndoTablespacesImplicit: metricInfo{
		Name: "newrelicmysql.innodb.undo_tablespaces_implicit",
	},
	NewrelicmysqlInnodbUndoTablespacesTotal: metricInfo{
		Name: "newrelicmysql.innodb.undo_tablespaces_total",
	},
	NewrelicmysqlInnodbXLockOsWaits: metricInfo{
		Name: "newrelicmysql.innodb.x_lock_os_waits",
	},
	NewrelicmysqlInnodbXLockSpinRounds: metricInfo{
		Name: "newrelicmysql.innodb.x_lock_spin_rounds",
	},
	NewrelicmysqlInnodbXLockSpinWaits: metricInfo{
		Name: "newrelicmysql.innodb.x_lock_spin_waits",
	},
	NewrelicmysqlMyisamKeyBufferBytesUnflushed: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_bytes_unflushed",
	},
	NewrelicmysqlMyisamKeyBufferBytesUsed: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_bytes_used",
	},
	NewrelicmysqlMyisamKeyBufferSize: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_size",
	},
	NewrelicmysqlMyisamKeyReadRequests: metricInfo{
		Name: "newrelicmysql.myisam.key_read_requests",
	},
	NewrelicmysqlMyisamKeyReads: metricInfo{
		Name: "newrelicmysql.myisam.key_reads",
	},
	NewrelicmysqlMyisamKeyWriteRequests: metricInfo{
		Name: "newrelicmysql.myisam.key_write_requests",
	},
	NewrelicmysqlMyisamKeyWrites: metricInfo{
		Name: "newrelicmysql.myisam.key_writes",
	},
	NewrelicmysqlNetAbortedClients: metricInfo{
		Name: "newrelicmysql.net.aborted_clients",
	},
	NewrelicmysqlNetAbortedConnects: metricInfo{
		Name: "newrelicmysql.net.aborted_connects",
	},
	NewrelicmysqlNetConnections: metricInfo{
		Name: "newrelicmysql.net.connections",
	},
	NewrelicmysqlNetMaxConnections: metricInfo{
		Name: "newrelicmysql.net.max_connections",
	},
	NewrelicmysqlNetMaxConnectionsAvailable: metricInfo{
		Name: "newrelicmysql.net.max_connections_available",
	},
	NewrelicmysqlNetMaxUsedConnections: metricInfo{
		Name: "newrelicmysql.net.max_used_connections",
	},
	NewrelicmysqlPerformanceBytesReceived: metricInfo{
		Name: "newrelicmysql.performance.bytes_received",
	},
	NewrelicmysqlPerformanceBytesSent: metricInfo{
		Name: "newrelicmysql.performance.bytes_sent",
	},
	NewrelicmysqlPerformanceCreatedTmpDiskTables: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_disk_tables",
	},
	NewrelicmysqlPerformanceCreatedTmpFiles: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_files",
	},
	NewrelicmysqlPerformanceCreatedTmpTables: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_tables",
	},
	NewrelicmysqlPerformanceHandlerCommit: metricInfo{
		Name: "newrelicmysql.performance.handler_commit",
	},
	NewrelicmysqlPerformanceHandlerDelete: metricInfo{
		Name: "newrelicmysql.performance.handler_delete",
	},
	NewrelicmysqlPerformanceHandlerPrepare: metricInfo{
		Name: "newrelicmysql.performance.handler_prepare",
	},
	NewrelicmysqlPerformanceHandlerReadFirst: metricInfo{
		Name: "newrelicmysql.performance.handler_read_first",
	},
	NewrelicmysqlPerformanceHandlerReadKey: metricInfo{
		Name: "newrelicmysql.performance.handler_read_key",
	},
	NewrelicmysqlPerformanceHandlerReadNext: metricInfo{
		Name: "newrelicmysql.performance.handler_read_next",
	},
	NewrelicmysqlPerformanceHandlerReadPrev: metricInfo{
		Name: "newrelicmysql.performance.handler_read_prev",
	},
	NewrelicmysqlPerformanceHandlerReadRnd: metricInfo{
		Name: "newrelicmysql.performance.handler_read_rnd",
	},
	NewrelicmysqlPerformanceHandlerReadRndNext: metricInfo{
		Name: "newrelicmysql.performance.handler_read_rnd_next",
	},
	NewrelicmysqlPerformanceHandlerRollback: metricInfo{
		Name: "newrelicmysql.performance.handler_rollback",
	},
	NewrelicmysqlPerformanceHandlerUpdate: metricInfo{
		Name: "newrelicmysql.performance.handler_update",
	},
	NewrelicmysqlPerformanceHandlerWrite: metricInfo{
		Name: "newrelicmysql.performance.handler_write",
	},
	NewrelicmysqlPerformanceKeyCacheUtilization: metricInfo{
		Name: "newrelicmysql.performance.key_cache_utilization",
	},
	NewrelicmysqlPerformanceMaxPreparedStmtCount: metricInfo{
		Name: "newrelicmysql.performance.max_prepared_stmt_count",
	},
	NewrelicmysqlPerformanceOpenFiles: metricInfo{
		Name: "newrelicmysql.performance.open_files",
	},
	NewrelicmysqlPerformanceOpenTables: metricInfo{
		Name: "newrelicmysql.performance.open_tables",
	},
	NewrelicmysqlPerformanceOpenedTables: metricInfo{
		Name: "newrelicmysql.performance.opened_tables",
	},
	NewrelicmysqlPerformancePerformanceSchemaDigestLost: metricInfo{
		Name: "newrelicmysql.performance.performance_schema_digest_lost",
	},
	NewrelicmysqlPerformancePreparedStmtCount: metricInfo{
		Name: "newrelicmysql.performance.prepared_stmt_count",
	},
	NewrelicmysqlPerformanceQcacheFreeBlocks: metricInfo{
		Name: "newrelicmysql.performance.qcache_free_blocks",
	},
	NewrelicmysqlPerformanceQcacheFreeMemory: metricInfo{
		Name: "newrelicmysql.performance.qcache_free_memory",
	},
	NewrelicmysqlPerformanceQcacheHits: metricInfo{
		Name: "newrelicmysql.performance.qcache_hits",
	},
	NewrelicmysqlPerformanceQcacheInserts: metricInfo{
		Name: "newrelicmysql.performance.qcache_inserts",
	},
	NewrelicmysqlPerformanceQcacheLowmemPrunes: metricInfo{
		Name: "newrelicmysql.performance.qcache_lowmem_prunes",
	},
	NewrelicmysqlPerformanceQcacheNotCached: metricInfo{
		Name: "newrelicmysql.performance.qcache_not_cached",
	},
	NewrelicmysqlPerformanceQcacheQueriesInCache: metricInfo{
		Name: "newrelicmysql.performance.qcache_queries_in_cache",
	},
	NewrelicmysqlPerformanceQcacheSize: metricInfo{
		Name: "newrelicmysql.performance.qcache_size",
	},
	NewrelicmysqlPerformanceQcacheTotalBlocks: metricInfo{
		Name: "newrelicmysql.performance.qcache_total_blocks",
	},
	NewrelicmysqlPerformanceQuestions: metricInfo{
		Name: "newrelicmysql.performance.questions",
	},
	NewrelicmysqlPerformanceSelectFullJoin: metricInfo{
		Name: "newrelicmysql.performance.select_full_join",
	},
	NewrelicmysqlPerformanceSelectFullRangeJoin: metricInfo{
		Name: "newrelicmysql.performance.select_full_range_join",
	},
	NewrelicmysqlPerformanceSelectRange: metricInfo{
		Name: "newrelicmysql.performance.select_range",
	},
	NewrelicmysqlPerformanceSelectRangeCheck: metricInfo{
		Name: "newrelicmysql.performance.select_range_check",
	},
	NewrelicmysqlPerformanceSelectScan: metricInfo{
		Name: "newrelicmysql.performance.select_scan",
	},
	NewrelicmysqlPerformanceSlowQueries: metricInfo{
		Name: "newrelicmysql.performance.slow_queries",
	},
	NewrelicmysqlPerformanceSortMergePasses: metricInfo{
		Name: "newrelicmysql.performance.sort_merge_passes",
	},
	NewrelicmysqlPerformanceSortRange: metricInfo{
		Name: "newrelicmysql.performance.sort_range",
	},
	NewrelicmysqlPerformanceSortRows: metricInfo{
		Name: "newrelicmysql.performance.sort_rows",
	},
	NewrelicmysqlPerformanceSortScan: metricInfo{
		Name: "newrelicmysql.performance.sort_scan",
	},
	NewrelicmysqlPerformanceTableLocksImmediate: metricInfo{
		Name: "newrelicmysql.performance.table_locks_immediate",
	},
	NewrelicmysqlPerformanceTableLocksImmediateRate: metricInfo{
		Name: "newrelicmysql.performance.table_locks_immediate.rate",
	},
	NewrelicmysqlPerformanceTableLocksWaited: metricInfo{
		Name: "newrelicmysql.performance.table_locks_waited",
	},
	NewrelicmysqlPerformanceTableOpenCache: metricInfo{
		Name: "newrelicmysql.performance.table_open_cache",
	},
	NewrelicmysqlPerformanceThreadCacheSize: metricInfo{
		Name: "newrelicmysql.performance.thread_cache_size",
	},
	NewrelicmysqlPerformanceThreadsCached: metricInfo{
		Name: "newrelicmysql.performance.threads_cached",
	},
	NewrelicmysqlPerformanceThreadsConnected: metricInfo{
		Name: "newrelicmysql.performance.threads_connected",
	},
	NewrelicmysqlPerformanceThreadsCreated: metricInfo{
		Name: "newrelicmysql.performance.threads_created",
	},
	NewrelicmysqlPerformanceThreadsRunning: metricInfo{
		Name: "newrelicmysql.performance.threads_running",
	},
	NewrelicmysqlQueryCount: metricInfo{
		Name: "newrelicmysql.query.count",
	},
	NewrelicmysqlReplicationExecMasterLogPos: metricInfo{
		Name: "newrelicmysql.replication.exec_master_log_pos",
	},
	NewrelicmysqlReplicationLastIoErrno: metricInfo{
		Name: "newrelicmysql.replication.last_io_errno",
	},
	NewrelicmysqlReplicationLastSQLErrno: metricInfo{
		Name: "newrelicmysql.replication.last_sql_errno",
	},
	NewrelicmysqlReplicationReadMasterLogPos: metricInfo{
		Name: "newrelicmysql.replication.read_master_log_pos",
	},
	NewrelicmysqlReplicationRelayLogSpace: metricInfo{
		Name: "newrelicmysql.replication.relay_log_space",
	},
	NewrelicmysqlReplicationSecondsBehindMaster: metricInfo{
		Name: "newrelicmysql.replication.seconds_behind_master",
	},
	NewrelicmysqlReplicationSlaveIoRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_io_running",
	},
	NewrelicmysqlReplicationSlaveRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_running",
	},
	NewrelicmysqlReplicationSlaveSQLRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_sql_running",
	},
	NewrelicmysqlUptime: metricInfo{
		Name: "newrelicmysql.uptime",
	},
}

type metricsInfo struct {
	NewrelicmysqlBinlogCacheDiskUse                     metricInfo
	NewrelicmysqlBinlogCacheUse                         metricInfo
	NewrelicmysqlCommands                               metricInfo
	NewrelicmysqlConnectionCount                        metricInfo
	NewrelicmysqlDbHandlerRollback                      metricInfo
	NewrelicmysqlDbOpenedTables                         metricInfo
	NewrelicmysqlGaleraWsrepCertDepsDistance            metricInfo
	NewrelicmysqlGaleraWsrepClusterSize                 metricInfo
	NewrelicmysqlGaleraWsrepFlowControlPaused           metricInfo
	NewrelicmysqlGaleraWsrepFlowControlPausedNs         metricInfo
	NewrelicmysqlGaleraWsrepFlowControlRecv             metricInfo
	NewrelicmysqlGaleraWsrepFlowControlSent             metricInfo
	NewrelicmysqlGaleraWsrepLocalCertFailures           metricInfo
	NewrelicmysqlGaleraWsrepLocalRecvQueue              metricInfo
	NewrelicmysqlGaleraWsrepLocalRecvQueueAvg           metricInfo
	NewrelicmysqlGaleraWsrepLocalSendQueue              metricInfo
	NewrelicmysqlGaleraWsrepLocalSendQueueAvg           metricInfo
	NewrelicmysqlGaleraWsrepLocalState                  metricInfo
	NewrelicmysqlGaleraWsrepReceived                    metricInfo
	NewrelicmysqlGaleraWsrepReceivedBytes               metricInfo
	NewrelicmysqlGaleraWsrepReplicatedBytes             metricInfo
	NewrelicmysqlInnodbActiveTransactions               metricInfo
	NewrelicmysqlInnodbAdaptiveHashHashSearches         metricInfo
	NewrelicmysqlInnodbAdaptiveHashNonHashSearches      metricInfo
	NewrelicmysqlInnodbAdaptiveHashPagesAdded           metricInfo
	NewrelicmysqlInnodbAdaptiveHashPagesRemoved         metricInfo
	NewrelicmysqlInnodbAvailableUndoLogs                metricInfo
	NewrelicmysqlInnodbBufferPoolBytesData              metricInfo
	NewrelicmysqlInnodbBufferPoolBytesDirty             metricInfo
	NewrelicmysqlInnodbBufferPoolDirty                  metricInfo
	NewrelicmysqlInnodbBufferPoolFree                   metricInfo
	NewrelicmysqlInnodbBufferPoolPagesData              metricInfo
	NewrelicmysqlInnodbBufferPoolPagesFlushed           metricInfo
	NewrelicmysqlInnodbBufferPoolPagesFree              metricInfo
	NewrelicmysqlInnodbBufferPoolPagesLruFlushed        metricInfo
	NewrelicmysqlInnodbBufferPoolPagesMadeNotYoung      metricInfo
	NewrelicmysqlInnodbBufferPoolPagesMadeYoung         metricInfo
	NewrelicmysqlInnodbBufferPoolPagesMisc              metricInfo
	NewrelicmysqlInnodbBufferPoolPagesOld               metricInfo
	NewrelicmysqlInnodbBufferPoolPagesTotal             metricInfo
	NewrelicmysqlInnodbBufferPoolReadAhead              metricInfo
	NewrelicmysqlInnodbBufferPoolReadAheadEvicted       metricInfo
	NewrelicmysqlInnodbBufferPoolReadAheadRnd           metricInfo
	NewrelicmysqlInnodbBufferPoolReadRequests           metricInfo
	NewrelicmysqlInnodbBufferPoolReads                  metricInfo
	NewrelicmysqlInnodbBufferPoolTotal                  metricInfo
	NewrelicmysqlInnodbBufferPoolUsed                   metricInfo
	NewrelicmysqlInnodbBufferPoolUtilization            metricInfo
	NewrelicmysqlInnodbBufferPoolWaitFree               metricInfo
	NewrelicmysqlInnodbBufferPoolWriteRequests          metricInfo
	NewrelicmysqlInnodbCheckpointAge                    metricInfo
	NewrelicmysqlInnodbCurrentRowLocks                  metricInfo
	NewrelicmysqlInnodbCurrentTransactions              metricInfo
	NewrelicmysqlInnodbDataFsyncs                       metricInfo
	NewrelicmysqlInnodbDataPendingFsyncs                metricInfo
	NewrelicmysqlInnodbDataPendingReads                 metricInfo
	NewrelicmysqlInnodbDataPendingWrites                metricInfo
	NewrelicmysqlInnodbDataRead                         metricInfo
	NewrelicmysqlInnodbDataReads                        metricInfo
	NewrelicmysqlInnodbDataWrites                       metricInfo
	NewrelicmysqlInnodbDataWritten                      metricInfo
	NewrelicmysqlInnodbDblwrPagesWritten                metricInfo
	NewrelicmysqlInnodbDblwrWrites                      metricInfo
	NewrelicmysqlInnodbHashIndexCellsTotal              metricInfo
	NewrelicmysqlInnodbHashIndexCellsUsed               metricInfo
	NewrelicmysqlInnodbHistoryListLength                metricInfo
	NewrelicmysqlInnodbIbufFreeList                     metricInfo
	NewrelicmysqlInnodbIbufMergedDeleteMarks            metricInfo
	NewrelicmysqlInnodbIbufMergedDeletes                metricInfo
	NewrelicmysqlInnodbIbufMergedInserts                metricInfo
	NewrelicmysqlInnodbIbufMerges                       metricInfo
	NewrelicmysqlInnodbIbufSegmentSize                  metricInfo
	NewrelicmysqlInnodbIbufSize                         metricInfo
	NewrelicmysqlInnodbLockStructs                      metricInfo
	NewrelicmysqlInnodbLockedTables                     metricInfo
	NewrelicmysqlInnodbLockedTransactions               metricInfo
	NewrelicmysqlInnodbLogWaits                         metricInfo
	NewrelicmysqlInnodbLogWriteRequests                 metricInfo
	NewrelicmysqlInnodbLogWrites                        metricInfo
	NewrelicmysqlInnodbLsnCurrent                       metricInfo
	NewrelicmysqlInnodbLsnFlushed                       metricInfo
	NewrelicmysqlInnodbLsnLastCheckpoint                metricInfo
	NewrelicmysqlInnodbMasterThreadActiveLoops          metricInfo
	NewrelicmysqlInnodbMasterThreadIdleLoops            metricInfo
	NewrelicmysqlInnodbMemAdaptiveHash                  metricInfo
	NewrelicmysqlInnodbMemAdditionalPool                metricInfo
	NewrelicmysqlInnodbMemDictionary                    metricInfo
	NewrelicmysqlInnodbMemFileSystem                    metricInfo
	NewrelicmysqlInnodbMemLockSystem                    metricInfo
	NewrelicmysqlInnodbMemPageHash                      metricInfo
	NewrelicmysqlInnodbMemRecoverySystem                metricInfo
	NewrelicmysqlInnodbMemThreadHash                    metricInfo
	NewrelicmysqlInnodbMemTotal                         metricInfo
	NewrelicmysqlInnodbMutexOsWaits                     metricInfo
	NewrelicmysqlInnodbMutexSpinRounds                  metricInfo
	NewrelicmysqlInnodbMutexSpinWaits                   metricInfo
	NewrelicmysqlInnodbNumOpenFiles                     metricInfo
	NewrelicmysqlInnodbOsFileFsyncs                     metricInfo
	NewrelicmysqlInnodbOsFileReads                      metricInfo
	NewrelicmysqlInnodbOsFileWrites                     metricInfo
	NewrelicmysqlInnodbOsLogFsyncs                      metricInfo
	NewrelicmysqlInnodbOsLogPendingFsyncs               metricInfo
	NewrelicmysqlInnodbOsLogPendingWrites               metricInfo
	NewrelicmysqlInnodbOsLogWritten                     metricInfo
	NewrelicmysqlInnodbPageSize                         metricInfo
	NewrelicmysqlInnodbPagesCreated                     metricInfo
	NewrelicmysqlInnodbPagesRead                        metricInfo
	NewrelicmysqlInnodbPagesWritten                     metricInfo
	NewrelicmysqlInnodbPendingAioLogIos                 metricInfo
	NewrelicmysqlInnodbPendingAioSyncIos                metricInfo
	NewrelicmysqlInnodbPendingBufferPoolFlushes         metricInfo
	NewrelicmysqlInnodbPendingCheckpointWrites          metricInfo
	NewrelicmysqlInnodbPendingIbufAioReads              metricInfo
	NewrelicmysqlInnodbPendingLogFlushes                metricInfo
	NewrelicmysqlInnodbPendingLogWrites                 metricInfo
	NewrelicmysqlInnodbPendingNormalAioReads            metricInfo
	NewrelicmysqlInnodbPendingNormalAioWrites           metricInfo
	NewrelicmysqlInnodbPurgeTrxID                       metricInfo
	NewrelicmysqlInnodbPurgeUndoNo                      metricInfo
	NewrelicmysqlInnodbQueriesInside                    metricInfo
	NewrelicmysqlInnodbQueriesQueued                    metricInfo
	NewrelicmysqlInnodbReadViews                        metricInfo
	NewrelicmysqlInnodbRedoLogEnabled                   metricInfo
	NewrelicmysqlInnodbRowLockCurrentWaits              metricInfo
	NewrelicmysqlInnodbRowLockTime                      metricInfo
	NewrelicmysqlInnodbRowLockTimeAvg                   metricInfo
	NewrelicmysqlInnodbRowLockTimeMax                   metricInfo
	NewrelicmysqlInnodbRowLockWaits                     metricInfo
	NewrelicmysqlInnodbRowsDeleted                      metricInfo
	NewrelicmysqlInnodbRowsInserted                     metricInfo
	NewrelicmysqlInnodbRowsRead                         metricInfo
	NewrelicmysqlInnodbRowsUpdated                      metricInfo
	NewrelicmysqlInnodbSLockOsWaits                     metricInfo
	NewrelicmysqlInnodbSLockSpinRounds                  metricInfo
	NewrelicmysqlInnodbSLockSpinWaits                   metricInfo
	NewrelicmysqlInnodbSemaphoreWaitTime                metricInfo
	NewrelicmysqlInnodbSemaphoreWaits                   metricInfo
	NewrelicmysqlInnodbTablesInUse                      metricInfo
	NewrelicmysqlInnodbTruncatedStatusWrites            metricInfo
	NewrelicmysqlInnodbUndoTablespacesActive            metricInfo
	NewrelicmysqlInnodbUndoTablespacesExplicit          metricInfo
	NewrelicmysqlInnodbUndoTablespacesImplicit          metricInfo
	NewrelicmysqlInnodbUndoTablespacesTotal             metricInfo
	NewrelicmysqlInnodbXLockOsWaits                     metricInfo
	NewrelicmysqlInnodbXLockSpinRounds                  metricInfo
	NewrelicmysqlInnodbXLockSpinWaits                   metricInfo
	NewrelicmysqlMyisamKeyBufferBytesUnflushed          metricInfo
	NewrelicmysqlMyisamKeyBufferBytesUsed               metricInfo
	NewrelicmysqlMyisamKeyBufferSize                    metricInfo
	NewrelicmysqlMyisamKeyReadRequests                  metricInfo
	NewrelicmysqlMyisamKeyReads                         metricInfo
	NewrelicmysqlMyisamKeyWriteRequests                 metricInfo
	NewrelicmysqlMyisamKeyWrites                        metricInfo
	NewrelicmysqlNetAbortedClients                      metricInfo
	NewrelicmysqlNetAbortedConnects                     metricInfo
	NewrelicmysqlNetConnections                         metricInfo
	NewrelicmysqlNetMaxConnections                      metricInfo
	NewrelicmysqlNetMaxConnectionsAvailable             metricInfo
	NewrelicmysqlNetMaxUsedConnections                  metricInfo
	NewrelicmysqlPerformanceBytesReceived               metricInfo
	NewrelicmysqlPerformanceBytesSent                   metricInfo
	NewrelicmysqlPerformanceCreatedTmpDiskTables        metricInfo
	NewrelicmysqlPerformanceCreatedTmpFiles             metricInfo
	NewrelicmysqlPerformanceCreatedTmpTables            metricInfo
	NewrelicmysqlPerformanceHandlerCommit               metricInfo
	NewrelicmysqlPerformanceHandlerDelete               metricInfo
	NewrelicmysqlPerformanceHandlerPrepare              metricInfo
	NewrelicmysqlPerformanceHandlerReadFirst            metricInfo
	NewrelicmysqlPerformanceHandlerReadKey              metricInfo
	NewrelicmysqlPerformanceHandlerReadNext             metricInfo
	NewrelicmysqlPerformanceHandlerReadPrev             metricInfo
	NewrelicmysqlPerformanceHandlerReadRnd              metricInfo
	NewrelicmysqlPerformanceHandlerReadRndNext          metricInfo
	NewrelicmysqlPerformanceHandlerRollback             metricInfo
	NewrelicmysqlPerformanceHandlerUpdate               metricInfo
	NewrelicmysqlPerformanceHandlerWrite                metricInfo
	NewrelicmysqlPerformanceKeyCacheUtilization         metricInfo
	NewrelicmysqlPerformanceMaxPreparedStmtCount        metricInfo
	NewrelicmysqlPerformanceOpenFiles                   metricInfo
	NewrelicmysqlPerformanceOpenTables                  metricInfo
	NewrelicmysqlPerformanceOpenedTables                metricInfo
	NewrelicmysqlPerformancePerformanceSchemaDigestLost metricInfo
	NewrelicmysqlPerformancePreparedStmtCount           metricInfo
	NewrelicmysqlPerformanceQcacheFreeBlocks            metricInfo
	NewrelicmysqlPerformanceQcacheFreeMemory            metricInfo
	NewrelicmysqlPerformanceQcacheHits                  metricInfo
	NewrelicmysqlPerformanceQcacheInserts               metricInfo
	NewrelicmysqlPerformanceQcacheLowmemPrunes          metricInfo
	NewrelicmysqlPerformanceQcacheNotCached             metricInfo
	NewrelicmysqlPerformanceQcacheQueriesInCache        metricInfo
	NewrelicmysqlPerformanceQcacheSize                  metricInfo
	NewrelicmysqlPerformanceQcacheTotalBlocks           metricInfo
	NewrelicmysqlPerformanceQuestions                   metricInfo
	NewrelicmysqlPerformanceSelectFullJoin              metricInfo
	NewrelicmysqlPerformanceSelectFullRangeJoin         metricInfo
	NewrelicmysqlPerformanceSelectRange                 metricInfo
	NewrelicmysqlPerformanceSelectRangeCheck            metricInfo
	NewrelicmysqlPerformanceSelectScan                  metricInfo
	NewrelicmysqlPerformanceSlowQueries                 metricInfo
	NewrelicmysqlPerformanceSortMergePasses             metricInfo
	NewrelicmysqlPerformanceSortRange                   metricInfo
	NewrelicmysqlPerformanceSortRows                    metricInfo
	NewrelicmysqlPerformanceSortScan                    metricInfo
	NewrelicmysqlPerformanceTableLocksImmediate         metricInfo
	NewrelicmysqlPerformanceTableLocksImmediateRate     metricInfo
	NewrelicmysqlPerformanceTableLocksWaited            metricInfo
	NewrelicmysqlPerformanceTableOpenCache              metricInfo
	NewrelicmysqlPerformanceThreadCacheSize             metricInfo
	NewrelicmysqlPerformanceThreadsCached               metricInfo
	NewrelicmysqlPerformanceThreadsConnected            metricInfo
	NewrelicmysqlPerformanceThreadsCreated              metricInfo
	NewrelicmysqlPerformanceThreadsRunning              metricInfo
	NewrelicmysqlQueryCount                             metricInfo
	NewrelicmysqlReplicationExecMasterLogPos            metricInfo
	NewrelicmysqlReplicationLastIoErrno                 metricInfo
	NewrelicmysqlReplicationLastSQLErrno                metricInfo
	NewrelicmysqlReplicationReadMasterLogPos            metricInfo
	NewrelicmysqlReplicationRelayLogSpace               metricInfo
	NewrelicmysqlReplicationSecondsBehindMaster         metricInfo
	NewrelicmysqlReplicationSlaveIoRunning              metricInfo
	NewrelicmysqlReplicationSlaveRunning                metricInfo
	NewrelicmysqlReplicationSlaveSQLRunning             metricInfo
	NewrelicmysqlUptime                                 metricInfo
}

type metricInfo struct {
	Name string
}

type metricNewrelicmysqlBinlogCacheDiskUse struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.binlog.cache_disk_use metric with initial data.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) init() {
	m.data.SetName("newrelicmysql.binlog.cache_disk_use")
	m.data.SetDescription("The number of transactions that used the temporary binary log cache but exceeded binlog_cache_size and used a temporary file.")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlBinlogCacheDiskUse) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlBinlogCacheDiskUse(cfg MetricConfig) metricNewrelicmysqlBinlogCacheDiskUse {
	m := metricNewrelicmysqlBinlogCacheDiskUse{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlBinlogCacheUse struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.binlog.cache_use metric with initial data.
func (m *metricNewrelicmysqlBinlogCacheUse) init() {
	m.data.SetName("newrelicmysql.binlog.cache_use")
	m.data.SetDescription("The number of transactions that used the binary log cache.")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlBinlogCacheUse) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlBinlogCacheUse) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlBinlogCacheUse) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlBinlogCacheUse(cfg MetricConfig) metricNewrelicmysqlBinlogCacheUse {
	m := metricNewrelicmysqlBinlogCacheUse{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlCommands struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.commands metric with initial data.
func (m *metricNewrelicmysqlCommands) init() {
	m.data.SetName("newrelicmysql.commands")
	m.data.SetDescription("The number of times each type of command has been executed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricNewrelicmysqlCommands) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlCommands) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlCommands) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlCommands(cfg MetricConfig) metricNewrelicmysqlCommands {
	m := metricNewrelicmysqlCommands{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlConnectionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.connection.count metric with initial data.
func (m *metricNewrelicmysqlConnectionCount) init() {
	m.data.SetName("newrelicmysql.connection.count")
	m.data.SetDescription("The number of connection attempts (successful or not) to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlConnectionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlConnectionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlConnectionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlConnectionCount(cfg MetricConfig) metricNewrelicmysqlConnectionCount {
	m := metricNewrelicmysqlConnectionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlDbHandlerRollback struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.db.handler_rollback metric with initial data.
func (m *metricNewrelicmysqlDbHandlerRollback) init() {
	m.data.SetName("newrelicmysql.db.handler_rollback")
	m.data.SetDescription("The number of internal ROLLBACK statements.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlDbHandlerRollback) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlDbHandlerRollback) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlDbHandlerRollback) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlDbHandlerRollback(cfg MetricConfig) metricNewrelicmysqlDbHandlerRollback {
	m := metricNewrelicmysqlDbHandlerRollback{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlDbOpenedTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.db.opened_tables metric with initial data.
func (m *metricNewrelicmysqlDbOpenedTables) init() {
	m.data.SetName("newrelicmysql.db.opened_tables")
	m.data.SetDescription("The number of tables that have been opened.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlDbOpenedTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlDbOpenedTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlDbOpenedTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlDbOpenedTables(cfg MetricConfig) metricNewrelicmysqlDbOpenedTables {
	m := metricNewrelicmysqlDbOpenedTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepCertDepsDistance struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_cert_deps_distance metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepCertDepsDistance) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_cert_deps_distance")
	m.data.SetDescription("Average distance between the lowest and highest seqno values that can be possibly applied in parallel (potential degree of parallelization).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepCertDepsDistance) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepCertDepsDistance) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepCertDepsDistance) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepCertDepsDistance(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepCertDepsDistance {
	m := metricNewrelicmysqlGaleraWsrepCertDepsDistance{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepClusterSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_cluster_size metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepClusterSize) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_cluster_size")
	m.data.SetDescription("Current number of nodes in the Galera cluster.")
	m.data.SetUnit("{nodes}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepClusterSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepClusterSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepClusterSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepClusterSize(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepClusterSize {
	m := metricNewrelicmysqlGaleraWsrepClusterSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepFlowControlPaused struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_flow_control_paused metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPaused) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_flow_control_paused")
	m.data.SetDescription("The fraction of time since the last status query that replication was paused due to flow control.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepFlowControlPaused) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPaused) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPaused) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepFlowControlPaused(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepFlowControlPaused {
	m := metricNewrelicmysqlGaleraWsrepFlowControlPaused{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepFlowControlPausedNs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_flow_control_paused_ns metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPausedNs) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_flow_control_paused_ns")
	m.data.SetDescription("The total time spent in a paused state measured in nanoseconds.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepFlowControlPausedNs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPausedNs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlPausedNs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepFlowControlPausedNs(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepFlowControlPausedNs {
	m := metricNewrelicmysqlGaleraWsrepFlowControlPausedNs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepFlowControlRecv struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_flow_control_recv metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlRecv) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_flow_control_recv")
	m.data.SetDescription("Number of FC_PAUSE events received since the last status query, including those that cause the provider to pause.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepFlowControlRecv) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlRecv) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlRecv) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepFlowControlRecv(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepFlowControlRecv {
	m := metricNewrelicmysqlGaleraWsrepFlowControlRecv{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepFlowControlSent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_flow_control_sent metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlSent) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_flow_control_sent")
	m.data.SetDescription("Number of FC_PAUSE events sent since the last status query.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepFlowControlSent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlSent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepFlowControlSent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepFlowControlSent(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepFlowControlSent {
	m := metricNewrelicmysqlGaleraWsrepFlowControlSent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalCertFailures struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_cert_failures metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalCertFailures) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_cert_failures")
	m.data.SetDescription("Number of local transactions that failed certification since the last status query.")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepLocalCertFailures) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalCertFailures) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalCertFailures) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalCertFailures(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalCertFailures {
	m := metricNewrelicmysqlGaleraWsrepLocalCertFailures{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalRecvQueue struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_recv_queue metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueue) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_recv_queue")
	m.data.SetDescription("Current length of the local received queue (number of writesets waiting to be applied).")
	m.data.SetUnit("{writesets}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueue) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueue) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueue) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalRecvQueue(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalRecvQueue {
	m := metricNewrelicmysqlGaleraWsrepLocalRecvQueue{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_recv_queue_avg metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_recv_queue_avg")
	m.data.SetDescription("Average length of the local received queue since the last status query.")
	m.data.SetUnit("{writesets}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg {
	m := metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalSendQueue struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_send_queue metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueue) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_send_queue")
	m.data.SetDescription("Current length of the send queue (number of writesets waiting to be sent).")
	m.data.SetUnit("{writesets}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueue) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueue) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueue) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalSendQueue(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalSendQueue {
	m := metricNewrelicmysqlGaleraWsrepLocalSendQueue{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_send_queue_avg metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_send_queue_avg")
	m.data.SetDescription("Average length of the send queue since the last status query.")
	m.data.SetUnit("{writesets}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalSendQueueAvg(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg {
	m := metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepLocalState struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_local_state metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepLocalState) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_local_state")
	m.data.SetDescription("Internal Galera Cluster FSM state number.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlGaleraWsrepLocalState) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepLocalState) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepLocalState) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepLocalState(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepLocalState {
	m := metricNewrelicmysqlGaleraWsrepLocalState{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepReceived struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_received metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepReceived) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_received")
	m.data.SetDescription("Total number of write-sets received from other nodes.")
	m.data.SetUnit("{writesets}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepReceived) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepReceived) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepReceived) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepReceived(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepReceived {
	m := metricNewrelicmysqlGaleraWsrepReceived{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepReceivedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_received_bytes metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepReceivedBytes) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_received_bytes")
	m.data.SetDescription("Total size in bytes of write-sets received from other nodes.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepReceivedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepReceivedBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepReceivedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepReceivedBytes(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepReceivedBytes {
	m := metricNewrelicmysqlGaleraWsrepReceivedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlGaleraWsrepReplicatedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.galera.wsrep_replicated_bytes metric with initial data.
func (m *metricNewrelicmysqlGaleraWsrepReplicatedBytes) init() {
	m.data.SetName("newrelicmysql.galera.wsrep_replicated_bytes")
	m.data.SetDescription("Total size in bytes of write-sets replicated to other nodes.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlGaleraWsrepReplicatedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlGaleraWsrepReplicatedBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlGaleraWsrepReplicatedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlGaleraWsrepReplicatedBytes(cfg MetricConfig) metricNewrelicmysqlGaleraWsrepReplicatedBytes {
	m := metricNewrelicmysqlGaleraWsrepReplicatedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbActiveTransactions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.active_transactions metric with initial data.
func (m *metricNewrelicmysqlInnodbActiveTransactions) init() {
	m.data.SetName("newrelicmysql.innodb.active_transactions")
	m.data.SetDescription("The number of currently active transactions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbActiveTransactions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbActiveTransactions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbActiveTransactions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbActiveTransactions(cfg MetricConfig) metricNewrelicmysqlInnodbActiveTransactions {
	m := metricNewrelicmysqlInnodbActiveTransactions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbAdaptiveHashHashSearches struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.adaptive_hash_hash_searches metric with initial data.
func (m *metricNewrelicmysqlInnodbAdaptiveHashHashSearches) init() {
	m.data.SetName("newrelicmysql.innodb.adaptive_hash_hash_searches")
	m.data.SetDescription("The number of successful hash searches in the adaptive hash index.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbAdaptiveHashHashSearches) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbAdaptiveHashHashSearches) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbAdaptiveHashHashSearches) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbAdaptiveHashHashSearches(cfg MetricConfig) metricNewrelicmysqlInnodbAdaptiveHashHashSearches {
	m := metricNewrelicmysqlInnodbAdaptiveHashHashSearches{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.adaptive_hash_non_hash_searches metric with initial data.
func (m *metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches) init() {
	m.data.SetName("newrelicmysql.innodb.adaptive_hash_non_hash_searches")
	m.data.SetDescription("The number of unsuccessful hash searches that fell back to B-tree searches.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbAdaptiveHashNonHashSearches(cfg MetricConfig) metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches {
	m := metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbAdaptiveHashPagesAdded struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.adaptive_hash_pages_added metric with initial data.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesAdded) init() {
	m.data.SetName("newrelicmysql.innodb.adaptive_hash_pages_added")
	m.data.SetDescription("The number of pages added to the adaptive hash index.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesAdded) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesAdded) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesAdded) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbAdaptiveHashPagesAdded(cfg MetricConfig) metricNewrelicmysqlInnodbAdaptiveHashPagesAdded {
	m := metricNewrelicmysqlInnodbAdaptiveHashPagesAdded{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.adaptive_hash_pages_removed metric with initial data.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved) init() {
	m.data.SetName("newrelicmysql.innodb.adaptive_hash_pages_removed")
	m.data.SetDescription("The number of pages removed from the adaptive hash index.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbAdaptiveHashPagesRemoved(cfg MetricConfig) metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved {
	m := metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbAvailableUndoLogs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.available_undo_logs metric with initial data.
func (m *metricNewrelicmysqlInnodbAvailableUndoLogs) init() {
	m.data.SetName("newrelicmysql.innodb.available_undo_logs")
	m.data.SetDescription("The number of available undo logs.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbAvailableUndoLogs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbAvailableUndoLogs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbAvailableUndoLogs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbAvailableUndoLogs(cfg MetricConfig) metricNewrelicmysqlInnodbAvailableUndoLogs {
	m := metricNewrelicmysqlInnodbAvailableUndoLogs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolBytesData struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_bytes_data metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesData) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_bytes_data")
	m.data.SetDescription("The total number of bytes in the InnoDB buffer pool containing data.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolBytesData) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesData) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesData) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolBytesData(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolBytesData {
	m := metricNewrelicmysqlInnodbBufferPoolBytesData{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolBytesDirty struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_bytes_dirty metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesDirty) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_bytes_dirty")
	m.data.SetDescription("The total size in bytes of dirty pages in the InnoDB buffer pool.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolBytesDirty) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesDirty) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolBytesDirty) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolBytesDirty(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolBytesDirty {
	m := metricNewrelicmysqlInnodbBufferPoolBytesDirty{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolDirty struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_dirty metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_dirty")
	m.data.SetDescription("The number of dirty pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolDirty) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolDirty(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolDirty {
	m := metricNewrelicmysqlInnodbBufferPoolDirty{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_free metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_free")
	m.data.SetDescription("The number of free pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolFree(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolFree {
	m := metricNewrelicmysqlInnodbBufferPoolFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesData struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_data metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesData) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_data")
	m.data.SetDescription("The number of pages in the InnoDB buffer pool containing data.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesData) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesData) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesData) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesData(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesData {
	m := metricNewrelicmysqlInnodbBufferPoolPagesData{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesFlushed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_flushed metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFlushed) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_flushed")
	m.data.SetDescription("The number of requests to flush pages from the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesFlushed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFlushed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFlushed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesFlushed(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesFlushed {
	m := metricNewrelicmysqlInnodbBufferPoolPagesFlushed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_free metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFree) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_free")
	m.data.SetDescription("The number of free pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesFree(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesFree {
	m := metricNewrelicmysqlInnodbBufferPoolPagesFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_lru_flushed metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_lru_flushed")
	m.data.SetDescription("The number of pages flushed from the LRU list.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesLruFlushed(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed {
	m := metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_made_not_young metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_made_not_young")
	m.data.SetDescription("The number of pages not made young in the buffer pool LRU list.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung {
	m := metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_made_young metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_made_young")
	m.data.SetDescription("The number of pages made young in the buffer pool LRU list.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesMadeYoung(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung {
	m := metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesMisc struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_misc metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMisc) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_misc")
	m.data.SetDescription("The number of pages in the InnoDB buffer pool used for miscellaneous purposes.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesMisc) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMisc) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesMisc) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesMisc(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesMisc {
	m := metricNewrelicmysqlInnodbBufferPoolPagesMisc{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesOld struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_old metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesOld) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_old")
	m.data.SetDescription("The number of old pages in the buffer pool LRU list.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesOld) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesOld) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesOld) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesOld(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesOld {
	m := metricNewrelicmysqlInnodbBufferPoolPagesOld{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolPagesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_pages_total metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesTotal) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_pages_total")
	m.data.SetDescription("The total number of pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolPagesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolPagesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolPagesTotal(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolPagesTotal {
	m := metricNewrelicmysqlInnodbBufferPoolPagesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReadAhead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_read_ahead metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAhead) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_read_ahead")
	m.data.SetDescription("The number of pages read into the InnoDB buffer pool by the read-ahead background thread.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReadAhead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAhead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAhead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReadAhead(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReadAhead {
	m := metricNewrelicmysqlInnodbBufferPoolReadAhead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_read_ahead_evicted metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_read_ahead_evicted")
	m.data.SetDescription("The number of pages read by read-ahead that were evicted without being accessed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReadAheadEvicted(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted {
	m := metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReadAheadRnd struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_read_ahead_rnd metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadRnd) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_read_ahead_rnd")
	m.data.SetDescription("The number of random read-aheads initiated by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadRnd) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadRnd) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReadAheadRnd) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReadAheadRnd(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReadAheadRnd {
	m := metricNewrelicmysqlInnodbBufferPoolReadAheadRnd{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReadRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_read_requests metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_read_requests")
	m.data.SetDescription("The number of logical read requests.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReadRequests(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReadRequests {
	m := metricNewrelicmysqlInnodbBufferPoolReadRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_reads")
	m.data.SetDescription("The number of reads that InnoDB could not satisfy from the buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReads(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReads {
	m := metricNewrelicmysqlInnodbBufferPoolReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_total metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_total")
	m.data.SetDescription("The total size of the InnoDB buffer pool in pages.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolTotal(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolTotal {
	m := metricNewrelicmysqlInnodbBufferPoolTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_used metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_used")
	m.data.SetDescription("The number of used pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolUsed(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolUsed {
	m := metricNewrelicmysqlInnodbBufferPoolUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolUtilization struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_utilization metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_utilization")
	m.data.SetDescription("The InnoDB buffer pool utilization percentage.")
	m.data.SetUnit("%")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolUtilization(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolUtilization {
	m := metricNewrelicmysqlInnodbBufferPoolUtilization{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolWaitFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_wait_free metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolWaitFree) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_wait_free")
	m.data.SetDescription("The number of times InnoDB waited for a free page in the buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolWaitFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolWaitFree) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolWaitFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolWaitFree(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolWaitFree {
	m := metricNewrelicmysqlInnodbBufferPoolWaitFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolWriteRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_write_requests metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolWriteRequests) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_write_requests")
	m.data.SetDescription("The number of write requests to the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolWriteRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolWriteRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolWriteRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolWriteRequests(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolWriteRequests {
	m := metricNewrelicmysqlInnodbBufferPoolWriteRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbCheckpointAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.checkpoint_age metric with initial data.
func (m *metricNewrelicmysqlInnodbCheckpointAge) init() {
	m.data.SetName("newrelicmysql.innodb.checkpoint_age")
	m.data.SetDescription("The age of the checkpoint (difference between current LSN and checkpoint LSN).")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbCheckpointAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbCheckpointAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbCheckpointAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbCheckpointAge(cfg MetricConfig) metricNewrelicmysqlInnodbCheckpointAge {
	m := metricNewrelicmysqlInnodbCheckpointAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbCurrentRowLocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.current_row_locks metric with initial data.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) init() {
	m.data.SetName("newrelicmysql.innodb.current_row_locks")
	m.data.SetDescription("The number of current row locks.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbCurrentRowLocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbCurrentRowLocks(cfg MetricConfig) metricNewrelicmysqlInnodbCurrentRowLocks {
	m := metricNewrelicmysqlInnodbCurrentRowLocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbCurrentTransactions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.current_transactions metric with initial data.
func (m *metricNewrelicmysqlInnodbCurrentTransactions) init() {
	m.data.SetName("newrelicmysql.innodb.current_transactions")
	m.data.SetDescription("The current number of transactions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbCurrentTransactions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbCurrentTransactions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbCurrentTransactions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbCurrentTransactions(cfg MetricConfig) metricNewrelicmysqlInnodbCurrentTransactions {
	m := metricNewrelicmysqlInnodbCurrentTransactions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbDataFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.data_fsyncs")
	m.data.SetDescription("The number of fsync() operations performed by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataFsyncs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbDataFsyncs {
	m := metricNewrelicmysqlInnodbDataFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataPendingFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_pending_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbDataPendingFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.data_pending_fsyncs")
	m.data.SetDescription("The current number of pending fsync operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbDataPendingFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataPendingFsyncs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataPendingFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataPendingFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbDataPendingFsyncs {
	m := metricNewrelicmysqlInnodbDataPendingFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataPendingReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_pending_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbDataPendingReads) init() {
	m.data.SetName("newrelicmysql.innodb.data_pending_reads")
	m.data.SetDescription("The current number of pending read operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbDataPendingReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataPendingReads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataPendingReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataPendingReads(cfg MetricConfig) metricNewrelicmysqlInnodbDataPendingReads {
	m := metricNewrelicmysqlInnodbDataPendingReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataPendingWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_pending_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbDataPendingWrites) init() {
	m.data.SetName("newrelicmysql.innodb.data_pending_writes")
	m.data.SetDescription("The current number of pending write operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbDataPendingWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataPendingWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataPendingWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataPendingWrites(cfg MetricConfig) metricNewrelicmysqlInnodbDataPendingWrites {
	m := metricNewrelicmysqlInnodbDataPendingWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_read metric with initial data.
func (m *metricNewrelicmysqlInnodbDataRead) init() {
	m.data.SetName("newrelicmysql.innodb.data_read")
	m.data.SetDescription("The total amount of data read by InnoDB.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataRead(cfg MetricConfig) metricNewrelicmysqlInnodbDataRead {
	m := metricNewrelicmysqlInnodbDataRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbDataReads) init() {
	m.data.SetName("newrelicmysql.innodb.data_reads")
	m.data.SetDescription("The amount of data read.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataReads(cfg MetricConfig) metricNewrelicmysqlInnodbDataReads {
	m := metricNewrelicmysqlInnodbDataReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbDataWrites) init() {
	m.data.SetName("newrelicmysql.innodb.data_writes")
	m.data.SetDescription("The amount of data written.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataWrites(cfg MetricConfig) metricNewrelicmysqlInnodbDataWrites {
	m := metricNewrelicmysqlInnodbDataWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_written metric with initial data.
func (m *metricNewrelicmysqlInnodbDataWritten) init() {
	m.data.SetName("newrelicmysql.innodb.data_written")
	m.data.SetDescription("The amount of data written to InnoDB tables in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataWritten(cfg MetricConfig) metricNewrelicmysqlInnodbDataWritten {
	m := metricNewrelicmysqlInnodbDataWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDblwrPagesWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.dblwr_pages_written metric with initial data.
func (m *metricNewrelicmysqlInnodbDblwrPagesWritten) init() {
	m.data.SetName("newrelicmysql.innodb.dblwr_pages_written")
	m.data.SetDescription("The number of pages written to the doublewrite buffer.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDblwrPagesWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDblwrPagesWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDblwrPagesWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDblwrPagesWritten(cfg MetricConfig) metricNewrelicmysqlInnodbDblwrPagesWritten {
	m := metricNewrelicmysqlInnodbDblwrPagesWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDblwrWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.dblwr_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbDblwrWrites) init() {
	m.data.SetName("newrelicmysql.innodb.dblwr_writes")
	m.data.SetDescription("The number of doublewrite operations performed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDblwrWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDblwrWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDblwrWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDblwrWrites(cfg MetricConfig) metricNewrelicmysqlInnodbDblwrWrites {
	m := metricNewrelicmysqlInnodbDblwrWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbHashIndexCellsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.hash_index_cells_total metric with initial data.
func (m *metricNewrelicmysqlInnodbHashIndexCellsTotal) init() {
	m.data.SetName("newrelicmysql.innodb.hash_index_cells_total")
	m.data.SetDescription("The total number of cells in the adaptive hash index.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbHashIndexCellsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbHashIndexCellsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbHashIndexCellsTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbHashIndexCellsTotal(cfg MetricConfig) metricNewrelicmysqlInnodbHashIndexCellsTotal {
	m := metricNewrelicmysqlInnodbHashIndexCellsTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbHashIndexCellsUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.hash_index_cells_used metric with initial data.
func (m *metricNewrelicmysqlInnodbHashIndexCellsUsed) init() {
	m.data.SetName("newrelicmysql.innodb.hash_index_cells_used")
	m.data.SetDescription("The number of used cells in the adaptive hash index.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbHashIndexCellsUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbHashIndexCellsUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbHashIndexCellsUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbHashIndexCellsUsed(cfg MetricConfig) metricNewrelicmysqlInnodbHashIndexCellsUsed {
	m := metricNewrelicmysqlInnodbHashIndexCellsUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbHistoryListLength struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.history_list_length metric with initial data.
func (m *metricNewrelicmysqlInnodbHistoryListLength) init() {
	m.data.SetName("newrelicmysql.innodb.history_list_length")
	m.data.SetDescription("The length of the InnoDB history list (undo log entries).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbHistoryListLength) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbHistoryListLength) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbHistoryListLength) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbHistoryListLength(cfg MetricConfig) metricNewrelicmysqlInnodbHistoryListLength {
	m := metricNewrelicmysqlInnodbHistoryListLength{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufFreeList struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_free_list metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufFreeList) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_free_list")
	m.data.SetDescription("The number of pages in the change buffer free list.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbIbufFreeList) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufFreeList) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufFreeList) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufFreeList(cfg MetricConfig) metricNewrelicmysqlInnodbIbufFreeList {
	m := metricNewrelicmysqlInnodbIbufFreeList{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufMergedDeleteMarks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_merged_delete_marks metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufMergedDeleteMarks) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_merged_delete_marks")
	m.data.SetDescription("The number of delete mark operations merged by the change buffer.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbIbufMergedDeleteMarks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufMergedDeleteMarks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufMergedDeleteMarks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufMergedDeleteMarks(cfg MetricConfig) metricNewrelicmysqlInnodbIbufMergedDeleteMarks {
	m := metricNewrelicmysqlInnodbIbufMergedDeleteMarks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufMergedDeletes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_merged_deletes metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufMergedDeletes) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_merged_deletes")
	m.data.SetDescription("The number of delete operations merged by the change buffer.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbIbufMergedDeletes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufMergedDeletes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufMergedDeletes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufMergedDeletes(cfg MetricConfig) metricNewrelicmysqlInnodbIbufMergedDeletes {
	m := metricNewrelicmysqlInnodbIbufMergedDeletes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufMergedInserts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_merged_inserts metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufMergedInserts) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_merged_inserts")
	m.data.SetDescription("The number of insert operations merged by the change buffer.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbIbufMergedInserts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufMergedInserts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufMergedInserts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufMergedInserts(cfg MetricConfig) metricNewrelicmysqlInnodbIbufMergedInserts {
	m := metricNewrelicmysqlInnodbIbufMergedInserts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufMerges struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_merges metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufMerges) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_merges")
	m.data.SetDescription("The number of merge operations performed by the change buffer.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbIbufMerges) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufMerges) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufMerges) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufMerges(cfg MetricConfig) metricNewrelicmysqlInnodbIbufMerges {
	m := metricNewrelicmysqlInnodbIbufMerges{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufSegmentSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_segment_size metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufSegmentSize) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_segment_size")
	m.data.SetDescription("The size of the change buffer segment.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbIbufSegmentSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufSegmentSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufSegmentSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufSegmentSize(cfg MetricConfig) metricNewrelicmysqlInnodbIbufSegmentSize {
	m := metricNewrelicmysqlInnodbIbufSegmentSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbIbufSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.ibuf_size metric with initial data.
func (m *metricNewrelicmysqlInnodbIbufSize) init() {
	m.data.SetName("newrelicmysql.innodb.ibuf_size")
	m.data.SetDescription("The size of the change buffer in pages.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbIbufSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbIbufSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbIbufSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbIbufSize(cfg MetricConfig) metricNewrelicmysqlInnodbIbufSize {
	m := metricNewrelicmysqlInnodbIbufSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLockStructs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.lock_structs metric with initial data.
func (m *metricNewrelicmysqlInnodbLockStructs) init() {
	m.data.SetName("newrelicmysql.innodb.lock_structs")
	m.data.SetDescription("The number of lock structures allocated.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLockStructs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLockStructs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLockStructs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLockStructs(cfg MetricConfig) metricNewrelicmysqlInnodbLockStructs {
	m := metricNewrelicmysqlInnodbLockStructs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLockedTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.locked_tables metric with initial data.
func (m *metricNewrelicmysqlInnodbLockedTables) init() {
	m.data.SetName("newrelicmysql.innodb.locked_tables")
	m.data.SetDescription("The number of locked tables.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLockedTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLockedTables) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLockedTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLockedTables(cfg MetricConfig) metricNewrelicmysqlInnodbLockedTables {
	m := metricNewrelicmysqlInnodbLockedTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLockedTransactions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.locked_transactions metric with initial data.
func (m *metricNewrelicmysqlInnodbLockedTransactions) init() {
	m.data.SetName("newrelicmysql.innodb.locked_transactions")
	m.data.SetDescription("The number of locked transactions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLockedTransactions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLockedTransactions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLockedTransactions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLockedTransactions(cfg MetricConfig) metricNewrelicmysqlInnodbLockedTransactions {
	m := metricNewrelicmysqlInnodbLockedTransactions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLogWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.log_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbLogWaits) init() {
	m.data.SetName("newrelicmysql.innodb.log_waits")
	m.data.SetDescription("The number of times the log buffer was too small and a wait was required for it to be flushed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbLogWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLogWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLogWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLogWaits(cfg MetricConfig) metricNewrelicmysqlInnodbLogWaits {
	m := metricNewrelicmysqlInnodbLogWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLogWriteRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.log_write_requests metric with initial data.
func (m *metricNewrelicmysqlInnodbLogWriteRequests) init() {
	m.data.SetName("newrelicmysql.innodb.log_write_requests")
	m.data.SetDescription("The number of write requests to the InnoDB log.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbLogWriteRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLogWriteRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLogWriteRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLogWriteRequests(cfg MetricConfig) metricNewrelicmysqlInnodbLogWriteRequests {
	m := metricNewrelicmysqlInnodbLogWriteRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLogWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.log_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbLogWrites) init() {
	m.data.SetName("newrelicmysql.innodb.log_writes")
	m.data.SetDescription("The number of physical writes to the InnoDB log file.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbLogWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLogWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLogWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLogWrites(cfg MetricConfig) metricNewrelicmysqlInnodbLogWrites {
	m := metricNewrelicmysqlInnodbLogWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLsnCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.lsn_current metric with initial data.
func (m *metricNewrelicmysqlInnodbLsnCurrent) init() {
	m.data.SetName("newrelicmysql.innodb.lsn_current")
	m.data.SetDescription("The current log sequence number.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLsnCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLsnCurrent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLsnCurrent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLsnCurrent(cfg MetricConfig) metricNewrelicmysqlInnodbLsnCurrent {
	m := metricNewrelicmysqlInnodbLsnCurrent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLsnFlushed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.lsn_flushed metric with initial data.
func (m *metricNewrelicmysqlInnodbLsnFlushed) init() {
	m.data.SetName("newrelicmysql.innodb.lsn_flushed")
	m.data.SetDescription("The log sequence number up to which all changes have been flushed to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLsnFlushed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLsnFlushed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLsnFlushed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLsnFlushed(cfg MetricConfig) metricNewrelicmysqlInnodbLsnFlushed {
	m := metricNewrelicmysqlInnodbLsnFlushed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLsnLastCheckpoint struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.lsn_last_checkpoint metric with initial data.
func (m *metricNewrelicmysqlInnodbLsnLastCheckpoint) init() {
	m.data.SetName("newrelicmysql.innodb.lsn_last_checkpoint")
	m.data.SetDescription("The log sequence number of the last checkpoint.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbLsnLastCheckpoint) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLsnLastCheckpoint) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLsnLastCheckpoint) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLsnLastCheckpoint(cfg MetricConfig) metricNewrelicmysqlInnodbLsnLastCheckpoint {
	m := metricNewrelicmysqlInnodbLsnLastCheckpoint{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMasterThreadActiveLoops struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.master_thread_active_loops metric with initial data.
func (m *metricNewrelicmysqlInnodbMasterThreadActiveLoops) init() {
	m.data.SetName("newrelicmysql.innodb.master_thread_active_loops")
	m.data.SetDescription("The number of times the master thread has gone through its active loop.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMasterThreadActiveLoops) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMasterThreadActiveLoops) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMasterThreadActiveLoops) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMasterThreadActiveLoops(cfg MetricConfig) metricNewrelicmysqlInnodbMasterThreadActiveLoops {
	m := metricNewrelicmysqlInnodbMasterThreadActiveLoops{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMasterThreadIdleLoops struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.master_thread_idle_loops metric with initial data.
func (m *metricNewrelicmysqlInnodbMasterThreadIdleLoops) init() {
	m.data.SetName("newrelicmysql.innodb.master_thread_idle_loops")
	m.data.SetDescription("The number of times the master thread has gone through its idle loop.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMasterThreadIdleLoops) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMasterThreadIdleLoops) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMasterThreadIdleLoops) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMasterThreadIdleLoops(cfg MetricConfig) metricNewrelicmysqlInnodbMasterThreadIdleLoops {
	m := metricNewrelicmysqlInnodbMasterThreadIdleLoops{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemAdaptiveHash struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_adaptive_hash metric with initial data.
func (m *metricNewrelicmysqlInnodbMemAdaptiveHash) init() {
	m.data.SetName("newrelicmysql.innodb.mem_adaptive_hash")
	m.data.SetDescription("The total memory allocated for the adaptive hash index.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemAdaptiveHash) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemAdaptiveHash) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemAdaptiveHash) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemAdaptiveHash(cfg MetricConfig) metricNewrelicmysqlInnodbMemAdaptiveHash {
	m := metricNewrelicmysqlInnodbMemAdaptiveHash{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemAdditionalPool struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_additional_pool metric with initial data.
func (m *metricNewrelicmysqlInnodbMemAdditionalPool) init() {
	m.data.SetName("newrelicmysql.innodb.mem_additional_pool")
	m.data.SetDescription("The size of the additional memory pool in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemAdditionalPool) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemAdditionalPool) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemAdditionalPool) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemAdditionalPool(cfg MetricConfig) metricNewrelicmysqlInnodbMemAdditionalPool {
	m := metricNewrelicmysqlInnodbMemAdditionalPool{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemDictionary struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_dictionary metric with initial data.
func (m *metricNewrelicmysqlInnodbMemDictionary) init() {
	m.data.SetName("newrelicmysql.innodb.mem_dictionary")
	m.data.SetDescription("The memory allocated for the InnoDB data dictionary in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemDictionary) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemDictionary) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemDictionary) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemDictionary(cfg MetricConfig) metricNewrelicmysqlInnodbMemDictionary {
	m := metricNewrelicmysqlInnodbMemDictionary{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemFileSystem struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_file_system metric with initial data.
func (m *metricNewrelicmysqlInnodbMemFileSystem) init() {
	m.data.SetName("newrelicmysql.innodb.mem_file_system")
	m.data.SetDescription("The memory allocated for the file system in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemFileSystem) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemFileSystem) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemFileSystem) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemFileSystem(cfg MetricConfig) metricNewrelicmysqlInnodbMemFileSystem {
	m := metricNewrelicmysqlInnodbMemFileSystem{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemLockSystem struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_lock_system metric with initial data.
func (m *metricNewrelicmysqlInnodbMemLockSystem) init() {
	m.data.SetName("newrelicmysql.innodb.mem_lock_system")
	m.data.SetDescription("The memory allocated for the lock system in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemLockSystem) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemLockSystem) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemLockSystem) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemLockSystem(cfg MetricConfig) metricNewrelicmysqlInnodbMemLockSystem {
	m := metricNewrelicmysqlInnodbMemLockSystem{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemPageHash struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_page_hash metric with initial data.
func (m *metricNewrelicmysqlInnodbMemPageHash) init() {
	m.data.SetName("newrelicmysql.innodb.mem_page_hash")
	m.data.SetDescription("The memory allocated for the page hash in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemPageHash) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemPageHash) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemPageHash) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemPageHash(cfg MetricConfig) metricNewrelicmysqlInnodbMemPageHash {
	m := metricNewrelicmysqlInnodbMemPageHash{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemRecoverySystem struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_recovery_system metric with initial data.
func (m *metricNewrelicmysqlInnodbMemRecoverySystem) init() {
	m.data.SetName("newrelicmysql.innodb.mem_recovery_system")
	m.data.SetDescription("The memory allocated for the recovery system in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemRecoverySystem) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemRecoverySystem) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemRecoverySystem) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemRecoverySystem(cfg MetricConfig) metricNewrelicmysqlInnodbMemRecoverySystem {
	m := metricNewrelicmysqlInnodbMemRecoverySystem{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemThreadHash struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_thread_hash metric with initial data.
func (m *metricNewrelicmysqlInnodbMemThreadHash) init() {
	m.data.SetName("newrelicmysql.innodb.mem_thread_hash")
	m.data.SetDescription("The memory allocated for the thread hash in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemThreadHash) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemThreadHash) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemThreadHash) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemThreadHash(cfg MetricConfig) metricNewrelicmysqlInnodbMemThreadHash {
	m := metricNewrelicmysqlInnodbMemThreadHash{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMemTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mem_total metric with initial data.
func (m *metricNewrelicmysqlInnodbMemTotal) init() {
	m.data.SetName("newrelicmysql.innodb.mem_total")
	m.data.SetDescription("The total memory allocated by InnoDB in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbMemTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMemTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMemTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMemTotal(cfg MetricConfig) metricNewrelicmysqlInnodbMemTotal {
	m := metricNewrelicmysqlInnodbMemTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexOsWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_os_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_os_waits")
	m.data.SetDescription("The number of mutex OS waits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexOsWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexOsWaits(cfg MetricConfig) metricNewrelicmysqlInnodbMutexOsWaits {
	m := metricNewrelicmysqlInnodbMutexOsWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexSpinRounds struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_spin_rounds metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_spin_rounds")
	m.data.SetDescription("The number of mutex spin rounds.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexSpinRounds) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexSpinRounds(cfg MetricConfig) metricNewrelicmysqlInnodbMutexSpinRounds {
	m := metricNewrelicmysqlInnodbMutexSpinRounds{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexSpinWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_spin_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_spin_waits")
	m.data.SetDescription("The number of mutex spin waits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexSpinWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexSpinWaits(cfg MetricConfig) metricNewrelicmysqlInnodbMutexSpinWaits {
	m := metricNewrelicmysqlInnodbMutexSpinWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbNumOpenFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.num_open_files metric with initial data.
func (m *metricNewrelicmysqlInnodbNumOpenFiles) init() {
	m.data.SetName("newrelicmysql.innodb.num_open_files")
	m.data.SetDescription("The number of files InnoDB currently holds open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbNumOpenFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbNumOpenFiles) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbNumOpenFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbNumOpenFiles(cfg MetricConfig) metricNewrelicmysqlInnodbNumOpenFiles {
	m := metricNewrelicmysqlInnodbNumOpenFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsFileFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_file_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbOsFileFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.os_file_fsyncs")
	m.data.SetDescription("The number of fsync calls to files.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsFileFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsFileFsyncs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsFileFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsFileFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbOsFileFsyncs {
	m := metricNewrelicmysqlInnodbOsFileFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsFileReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_file_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbOsFileReads) init() {
	m.data.SetName("newrelicmysql.innodb.os_file_reads")
	m.data.SetDescription("The number of file reads performed by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsFileReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsFileReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsFileReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsFileReads(cfg MetricConfig) metricNewrelicmysqlInnodbOsFileReads {
	m := metricNewrelicmysqlInnodbOsFileReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsFileWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_file_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbOsFileWrites) init() {
	m.data.SetName("newrelicmysql.innodb.os_file_writes")
	m.data.SetDescription("The number of file writes performed by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsFileWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsFileWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsFileWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsFileWrites(cfg MetricConfig) metricNewrelicmysqlInnodbOsFileWrites {
	m := metricNewrelicmysqlInnodbOsFileWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsLogFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_log_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.os_log_fsyncs")
	m.data.SetDescription("The number of fsync writes done to the InnoDB redo log files.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsLogFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsLogFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbOsLogFsyncs {
	m := metricNewrelicmysqlInnodbOsLogFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsLogPendingFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_log_pending_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbOsLogPendingFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.os_log_pending_fsyncs")
	m.data.SetDescription("The number of pending fsync operations for the redo log.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbOsLogPendingFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsLogPendingFsyncs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsLogPendingFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsLogPendingFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbOsLogPendingFsyncs {
	m := metricNewrelicmysqlInnodbOsLogPendingFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsLogPendingWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_log_pending_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbOsLogPendingWrites) init() {
	m.data.SetName("newrelicmysql.innodb.os_log_pending_writes")
	m.data.SetDescription("The number of pending write operations for the redo log.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbOsLogPendingWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsLogPendingWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsLogPendingWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsLogPendingWrites(cfg MetricConfig) metricNewrelicmysqlInnodbOsLogPendingWrites {
	m := metricNewrelicmysqlInnodbOsLogPendingWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsLogWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_log_written metric with initial data.
func (m *metricNewrelicmysqlInnodbOsLogWritten) init() {
	m.data.SetName("newrelicmysql.innodb.os_log_written")
	m.data.SetDescription("The total amount of data written to the redo log files.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsLogWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsLogWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsLogWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsLogWritten(cfg MetricConfig) metricNewrelicmysqlInnodbOsLogWritten {
	m := metricNewrelicmysqlInnodbOsLogWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPageSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.page_size metric with initial data.
func (m *metricNewrelicmysqlInnodbPageSize) init() {
	m.data.SetName("newrelicmysql.innodb.page_size")
	m.data.SetDescription("The compiled-in InnoDB page size.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPageSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPageSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPageSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPageSize(cfg MetricConfig) metricNewrelicmysqlInnodbPageSize {
	m := metricNewrelicmysqlInnodbPageSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPagesCreated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pages_created metric with initial data.
func (m *metricNewrelicmysqlInnodbPagesCreated) init() {
	m.data.SetName("newrelicmysql.innodb.pages_created")
	m.data.SetDescription("The number of pages created by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbPagesCreated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPagesCreated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPagesCreated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPagesCreated(cfg MetricConfig) metricNewrelicmysqlInnodbPagesCreated {
	m := metricNewrelicmysqlInnodbPagesCreated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPagesRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pages_read metric with initial data.
func (m *metricNewrelicmysqlInnodbPagesRead) init() {
	m.data.SetName("newrelicmysql.innodb.pages_read")
	m.data.SetDescription("The number of pages read by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbPagesRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPagesRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPagesRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPagesRead(cfg MetricConfig) metricNewrelicmysqlInnodbPagesRead {
	m := metricNewrelicmysqlInnodbPagesRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPagesWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pages_written metric with initial data.
func (m *metricNewrelicmysqlInnodbPagesWritten) init() {
	m.data.SetName("newrelicmysql.innodb.pages_written")
	m.data.SetDescription("The number of pages written by InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbPagesWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPagesWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPagesWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPagesWritten(cfg MetricConfig) metricNewrelicmysqlInnodbPagesWritten {
	m := metricNewrelicmysqlInnodbPagesWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingAioLogIos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_aio_log_ios metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingAioLogIos) init() {
	m.data.SetName("newrelicmysql.innodb.pending_aio_log_ios")
	m.data.SetDescription("The number of pending asynchronous I/O operations for log files.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingAioLogIos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingAioLogIos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingAioLogIos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingAioLogIos(cfg MetricConfig) metricNewrelicmysqlInnodbPendingAioLogIos {
	m := metricNewrelicmysqlInnodbPendingAioLogIos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingAioSyncIos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_aio_sync_ios metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingAioSyncIos) init() {
	m.data.SetName("newrelicmysql.innodb.pending_aio_sync_ios")
	m.data.SetDescription("The number of pending synchronous I/O operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingAioSyncIos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingAioSyncIos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingAioSyncIos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingAioSyncIos(cfg MetricConfig) metricNewrelicmysqlInnodbPendingAioSyncIos {
	m := metricNewrelicmysqlInnodbPendingAioSyncIos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingBufferPoolFlushes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_buffer_pool_flushes metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingBufferPoolFlushes) init() {
	m.data.SetName("newrelicmysql.innodb.pending_buffer_pool_flushes")
	m.data.SetDescription("The number of pending buffer pool flush operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingBufferPoolFlushes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingBufferPoolFlushes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingBufferPoolFlushes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingBufferPoolFlushes(cfg MetricConfig) metricNewrelicmysqlInnodbPendingBufferPoolFlushes {
	m := metricNewrelicmysqlInnodbPendingBufferPoolFlushes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingCheckpointWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_checkpoint_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingCheckpointWrites) init() {
	m.data.SetName("newrelicmysql.innodb.pending_checkpoint_writes")
	m.data.SetDescription("The number of pending checkpoint write operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingCheckpointWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingCheckpointWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingCheckpointWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingCheckpointWrites(cfg MetricConfig) metricNewrelicmysqlInnodbPendingCheckpointWrites {
	m := metricNewrelicmysqlInnodbPendingCheckpointWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingIbufAioReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_ibuf_aio_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingIbufAioReads) init() {
	m.data.SetName("newrelicmysql.innodb.pending_ibuf_aio_reads")
	m.data.SetDescription("The number of pending asynchronous change buffer read operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingIbufAioReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingIbufAioReads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingIbufAioReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingIbufAioReads(cfg MetricConfig) metricNewrelicmysqlInnodbPendingIbufAioReads {
	m := metricNewrelicmysqlInnodbPendingIbufAioReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingLogFlushes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_log_flushes metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingLogFlushes) init() {
	m.data.SetName("newrelicmysql.innodb.pending_log_flushes")
	m.data.SetDescription("The number of pending log flush operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingLogFlushes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingLogFlushes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingLogFlushes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingLogFlushes(cfg MetricConfig) metricNewrelicmysqlInnodbPendingLogFlushes {
	m := metricNewrelicmysqlInnodbPendingLogFlushes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingLogWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_log_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingLogWrites) init() {
	m.data.SetName("newrelicmysql.innodb.pending_log_writes")
	m.data.SetDescription("The number of pending log write operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingLogWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingLogWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingLogWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingLogWrites(cfg MetricConfig) metricNewrelicmysqlInnodbPendingLogWrites {
	m := metricNewrelicmysqlInnodbPendingLogWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingNormalAioReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_normal_aio_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingNormalAioReads) init() {
	m.data.SetName("newrelicmysql.innodb.pending_normal_aio_reads")
	m.data.SetDescription("The number of pending normal asynchronous read operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingNormalAioReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingNormalAioReads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingNormalAioReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingNormalAioReads(cfg MetricConfig) metricNewrelicmysqlInnodbPendingNormalAioReads {
	m := metricNewrelicmysqlInnodbPendingNormalAioReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPendingNormalAioWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.pending_normal_aio_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbPendingNormalAioWrites) init() {
	m.data.SetName("newrelicmysql.innodb.pending_normal_aio_writes")
	m.data.SetDescription("The number of pending normal asynchronous write operations.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPendingNormalAioWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPendingNormalAioWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPendingNormalAioWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPendingNormalAioWrites(cfg MetricConfig) metricNewrelicmysqlInnodbPendingNormalAioWrites {
	m := metricNewrelicmysqlInnodbPendingNormalAioWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPurgeTrxID struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.purge_trx_id metric with initial data.
func (m *metricNewrelicmysqlInnodbPurgeTrxID) init() {
	m.data.SetName("newrelicmysql.innodb.purge_trx_id")
	m.data.SetDescription("The transaction ID that the purge system is currently processing.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPurgeTrxID) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPurgeTrxID) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPurgeTrxID) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPurgeTrxID(cfg MetricConfig) metricNewrelicmysqlInnodbPurgeTrxID {
	m := metricNewrelicmysqlInnodbPurgeTrxID{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbPurgeUndoNo struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.purge_undo_no metric with initial data.
func (m *metricNewrelicmysqlInnodbPurgeUndoNo) init() {
	m.data.SetName("newrelicmysql.innodb.purge_undo_no")
	m.data.SetDescription("The undo number that the purge system is currently processing.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbPurgeUndoNo) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbPurgeUndoNo) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbPurgeUndoNo) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbPurgeUndoNo(cfg MetricConfig) metricNewrelicmysqlInnodbPurgeUndoNo {
	m := metricNewrelicmysqlInnodbPurgeUndoNo{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbQueriesInside struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.queries_inside metric with initial data.
func (m *metricNewrelicmysqlInnodbQueriesInside) init() {
	m.data.SetName("newrelicmysql.innodb.queries_inside")
	m.data.SetDescription("The number of queries currently being executed inside InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbQueriesInside) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbQueriesInside) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbQueriesInside) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbQueriesInside(cfg MetricConfig) metricNewrelicmysqlInnodbQueriesInside {
	m := metricNewrelicmysqlInnodbQueriesInside{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbQueriesQueued struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.queries_queued metric with initial data.
func (m *metricNewrelicmysqlInnodbQueriesQueued) init() {
	m.data.SetName("newrelicmysql.innodb.queries_queued")
	m.data.SetDescription("The number of queries queued inside InnoDB.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbQueriesQueued) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbQueriesQueued) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbQueriesQueued) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbQueriesQueued(cfg MetricConfig) metricNewrelicmysqlInnodbQueriesQueued {
	m := metricNewrelicmysqlInnodbQueriesQueued{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbReadViews struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.read_views metric with initial data.
func (m *metricNewrelicmysqlInnodbReadViews) init() {
	m.data.SetName("newrelicmysql.innodb.read_views")
	m.data.SetDescription("The number of read views currently open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbReadViews) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbReadViews) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbReadViews) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbReadViews(cfg MetricConfig) metricNewrelicmysqlInnodbReadViews {
	m := metricNewrelicmysqlInnodbReadViews{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRedoLogEnabled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.redo_log_enabled metric with initial data.
func (m *metricNewrelicmysqlInnodbRedoLogEnabled) init() {
	m.data.SetName("newrelicmysql.innodb.redo_log_enabled")
	m.data.SetDescription("Whether redo logging is enabled (1) or disabled (0).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbRedoLogEnabled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRedoLogEnabled) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRedoLogEnabled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRedoLogEnabled(cfg MetricConfig) metricNewrelicmysqlInnodbRedoLogEnabled {
	m := metricNewrelicmysqlInnodbRedoLogEnabled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockCurrentWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_current_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_current_waits")
	m.data.SetDescription("The number of row locks currently being waited for.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockCurrentWaits(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockCurrentWaits {
	m := metricNewrelicmysqlInnodbRowLockCurrentWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_time metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockTime) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_time")
	m.data.SetDescription("The total time spent in acquiring row locks.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowLockTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockTime(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockTime {
	m := metricNewrelicmysqlInnodbRowLockTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_time_avg metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockTimeAvg) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_time_avg")
	m.data.SetDescription("The average time to acquire a row lock.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbRowLockTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockTimeAvg(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockTimeAvg {
	m := metricNewrelicmysqlInnodbRowLockTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockTimeMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_time_max metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockTimeMax) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_time_max")
	m.data.SetDescription("The maximum time to acquire a row lock.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbRowLockTimeMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockTimeMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockTimeMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockTimeMax(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockTimeMax {
	m := metricNewrelicmysqlInnodbRowLockTimeMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockWaits) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_waits")
	m.data.SetDescription("The number of times operations had to wait for a row lock.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowLockWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockWaits(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockWaits {
	m := metricNewrelicmysqlInnodbRowLockWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowsDeleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.rows_deleted metric with initial data.
func (m *metricNewrelicmysqlInnodbRowsDeleted) init() {
	m.data.SetName("newrelicmysql.innodb.rows_deleted")
	m.data.SetDescription("The number of rows deleted from InnoDB tables.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowsDeleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowsDeleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowsDeleted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowsDeleted(cfg MetricConfig) metricNewrelicmysqlInnodbRowsDeleted {
	m := metricNewrelicmysqlInnodbRowsDeleted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowsInserted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.rows_inserted metric with initial data.
func (m *metricNewrelicmysqlInnodbRowsInserted) init() {
	m.data.SetName("newrelicmysql.innodb.rows_inserted")
	m.data.SetDescription("The number of rows inserted into InnoDB tables.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowsInserted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowsInserted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowsInserted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowsInserted(cfg MetricConfig) metricNewrelicmysqlInnodbRowsInserted {
	m := metricNewrelicmysqlInnodbRowsInserted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowsRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.rows_read metric with initial data.
func (m *metricNewrelicmysqlInnodbRowsRead) init() {
	m.data.SetName("newrelicmysql.innodb.rows_read")
	m.data.SetDescription("The number of rows read from InnoDB tables.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowsRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowsRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowsRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowsRead(cfg MetricConfig) metricNewrelicmysqlInnodbRowsRead {
	m := metricNewrelicmysqlInnodbRowsRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowsUpdated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.rows_updated metric with initial data.
func (m *metricNewrelicmysqlInnodbRowsUpdated) init() {
	m.data.SetName("newrelicmysql.innodb.rows_updated")
	m.data.SetDescription("The number of rows updated in InnoDB tables.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowsUpdated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowsUpdated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowsUpdated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowsUpdated(cfg MetricConfig) metricNewrelicmysqlInnodbRowsUpdated {
	m := metricNewrelicmysqlInnodbRowsUpdated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbSLockOsWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.s_lock_os_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbSLockOsWaits) init() {
	m.data.SetName("newrelicmysql.innodb.s_lock_os_waits")
	m.data.SetDescription("The number of OS waits for S-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbSLockOsWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbSLockOsWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbSLockOsWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbSLockOsWaits(cfg MetricConfig) metricNewrelicmysqlInnodbSLockOsWaits {
	m := metricNewrelicmysqlInnodbSLockOsWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbSLockSpinRounds struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.s_lock_spin_rounds metric with initial data.
func (m *metricNewrelicmysqlInnodbSLockSpinRounds) init() {
	m.data.SetName("newrelicmysql.innodb.s_lock_spin_rounds")
	m.data.SetDescription("The number of spin rounds for S-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbSLockSpinRounds) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbSLockSpinRounds) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbSLockSpinRounds) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbSLockSpinRounds(cfg MetricConfig) metricNewrelicmysqlInnodbSLockSpinRounds {
	m := metricNewrelicmysqlInnodbSLockSpinRounds{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbSLockSpinWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.s_lock_spin_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbSLockSpinWaits) init() {
	m.data.SetName("newrelicmysql.innodb.s_lock_spin_waits")
	m.data.SetDescription("The number of spin waits for S-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbSLockSpinWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbSLockSpinWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbSLockSpinWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbSLockSpinWaits(cfg MetricConfig) metricNewrelicmysqlInnodbSLockSpinWaits {
	m := metricNewrelicmysqlInnodbSLockSpinWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbSemaphoreWaitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.semaphore_wait_time metric with initial data.
func (m *metricNewrelicmysqlInnodbSemaphoreWaitTime) init() {
	m.data.SetName("newrelicmysql.innodb.semaphore_wait_time")
	m.data.SetDescription("The total time spent waiting for semaphores in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbSemaphoreWaitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbSemaphoreWaitTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbSemaphoreWaitTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbSemaphoreWaitTime(cfg MetricConfig) metricNewrelicmysqlInnodbSemaphoreWaitTime {
	m := metricNewrelicmysqlInnodbSemaphoreWaitTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbSemaphoreWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.semaphore_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbSemaphoreWaits) init() {
	m.data.SetName("newrelicmysql.innodb.semaphore_waits")
	m.data.SetDescription("The number of times a semaphore wait occurred.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbSemaphoreWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbSemaphoreWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbSemaphoreWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbSemaphoreWaits(cfg MetricConfig) metricNewrelicmysqlInnodbSemaphoreWaits {
	m := metricNewrelicmysqlInnodbSemaphoreWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbTablesInUse struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.tables_in_use metric with initial data.
func (m *metricNewrelicmysqlInnodbTablesInUse) init() {
	m.data.SetName("newrelicmysql.innodb.tables_in_use")
	m.data.SetDescription("The number of tables currently in use by transactions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbTablesInUse) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbTablesInUse) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbTablesInUse) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbTablesInUse(cfg MetricConfig) metricNewrelicmysqlInnodbTablesInUse {
	m := metricNewrelicmysqlInnodbTablesInUse{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbTruncatedStatusWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.truncated_status_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbTruncatedStatusWrites) init() {
	m.data.SetName("newrelicmysql.innodb.truncated_status_writes")
	m.data.SetDescription("The number of times output from SHOW ENGINE INNODB STATUS has been truncated.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbTruncatedStatusWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbTruncatedStatusWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbTruncatedStatusWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbTruncatedStatusWrites(cfg MetricConfig) metricNewrelicmysqlInnodbTruncatedStatusWrites {
	m := metricNewrelicmysqlInnodbTruncatedStatusWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbUndoTablespacesActive struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.undo_tablespaces_active metric with initial data.
func (m *metricNewrelicmysqlInnodbUndoTablespacesActive) init() {
	m.data.SetName("newrelicmysql.innodb.undo_tablespaces_active")
	m.data.SetDescription("The number of active undo tablespaces.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbUndoTablespacesActive) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbUndoTablespacesActive) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbUndoTablespacesActive) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbUndoTablespacesActive(cfg MetricConfig) metricNewrelicmysqlInnodbUndoTablespacesActive {
	m := metricNewrelicmysqlInnodbUndoTablespacesActive{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbUndoTablespacesExplicit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.undo_tablespaces_explicit metric with initial data.
func (m *metricNewrelicmysqlInnodbUndoTablespacesExplicit) init() {
	m.data.SetName("newrelicmysql.innodb.undo_tablespaces_explicit")
	m.data.SetDescription("The number of user-created undo tablespaces.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbUndoTablespacesExplicit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbUndoTablespacesExplicit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbUndoTablespacesExplicit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbUndoTablespacesExplicit(cfg MetricConfig) metricNewrelicmysqlInnodbUndoTablespacesExplicit {
	m := metricNewrelicmysqlInnodbUndoTablespacesExplicit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbUndoTablespacesImplicit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.undo_tablespaces_implicit metric with initial data.
func (m *metricNewrelicmysqlInnodbUndoTablespacesImplicit) init() {
	m.data.SetName("newrelicmysql.innodb.undo_tablespaces_implicit")
	m.data.SetDescription("The number of implicit undo tablespaces.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbUndoTablespacesImplicit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbUndoTablespacesImplicit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbUndoTablespacesImplicit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbUndoTablespacesImplicit(cfg MetricConfig) metricNewrelicmysqlInnodbUndoTablespacesImplicit {
	m := metricNewrelicmysqlInnodbUndoTablespacesImplicit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbUndoTablespacesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.undo_tablespaces_total metric with initial data.
func (m *metricNewrelicmysqlInnodbUndoTablespacesTotal) init() {
	m.data.SetName("newrelicmysql.innodb.undo_tablespaces_total")
	m.data.SetDescription("The total number of undo tablespaces.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbUndoTablespacesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbUndoTablespacesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbUndoTablespacesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbUndoTablespacesTotal(cfg MetricConfig) metricNewrelicmysqlInnodbUndoTablespacesTotal {
	m := metricNewrelicmysqlInnodbUndoTablespacesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbXLockOsWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.x_lock_os_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbXLockOsWaits) init() {
	m.data.SetName("newrelicmysql.innodb.x_lock_os_waits")
	m.data.SetDescription("The number of OS waits for X-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbXLockOsWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbXLockOsWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbXLockOsWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbXLockOsWaits(cfg MetricConfig) metricNewrelicmysqlInnodbXLockOsWaits {
	m := metricNewrelicmysqlInnodbXLockOsWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbXLockSpinRounds struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.x_lock_spin_rounds metric with initial data.
func (m *metricNewrelicmysqlInnodbXLockSpinRounds) init() {
	m.data.SetName("newrelicmysql.innodb.x_lock_spin_rounds")
	m.data.SetDescription("The number of spin rounds for X-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbXLockSpinRounds) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbXLockSpinRounds) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbXLockSpinRounds) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbXLockSpinRounds(cfg MetricConfig) metricNewrelicmysqlInnodbXLockSpinRounds {
	m := metricNewrelicmysqlInnodbXLockSpinRounds{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbXLockSpinWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.x_lock_spin_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbXLockSpinWaits) init() {
	m.data.SetName("newrelicmysql.innodb.x_lock_spin_waits")
	m.data.SetDescription("The number of spin waits for X-locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbXLockSpinWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbXLockSpinWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbXLockSpinWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbXLockSpinWaits(cfg MetricConfig) metricNewrelicmysqlInnodbXLockSpinWaits {
	m := metricNewrelicmysqlInnodbXLockSpinWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferBytesUnflushed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_bytes_unflushed metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_bytes_unflushed")
	m.data.SetDescription("MyISAM key buffer bytes unflushed.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferBytesUnflushed(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferBytesUnflushed {
	m := metricNewrelicmysqlMyisamKeyBufferBytesUnflushed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferBytesUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_bytes_used metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_bytes_used")
	m.data.SetDescription("MyISAM key buffer bytes used.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferBytesUsed(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferBytesUsed {
	m := metricNewrelicmysqlMyisamKeyBufferBytesUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_size metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_size")
	m.data.SetDescription("Size of the buffer used for index blocks.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferSize(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferSize {
	m := metricNewrelicmysqlMyisamKeyBufferSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyReadRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_read_requests metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) init() {
	m.data.SetName("newrelicmysql.myisam.key_read_requests")
	m.data.SetDescription("The number of requests to read a key block from the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyReadRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyReadRequests(cfg MetricConfig) metricNewrelicmysqlMyisamKeyReadRequests {
	m := metricNewrelicmysqlMyisamKeyReadRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_reads metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyReads) init() {
	m.data.SetName("newrelicmysql.myisam.key_reads")
	m.data.SetDescription("The number of physical reads of a key block from disk into the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyReads(cfg MetricConfig) metricNewrelicmysqlMyisamKeyReads {
	m := metricNewrelicmysqlMyisamKeyReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyWriteRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_write_requests metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) init() {
	m.data.SetName("newrelicmysql.myisam.key_write_requests")
	m.data.SetDescription("The number of requests to write a key block to the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyWriteRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyWriteRequests(cfg MetricConfig) metricNewrelicmysqlMyisamKeyWriteRequests {
	m := metricNewrelicmysqlMyisamKeyWriteRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_writes metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyWrites) init() {
	m.data.SetName("newrelicmysql.myisam.key_writes")
	m.data.SetDescription("The number of physical writes of a key block from the MyISAM key cache to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyWrites(cfg MetricConfig) metricNewrelicmysqlMyisamKeyWrites {
	m := metricNewrelicmysqlMyisamKeyWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetAbortedClients struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.aborted_clients metric with initial data.
func (m *metricNewrelicmysqlNetAbortedClients) init() {
	m.data.SetName("newrelicmysql.net.aborted_clients")
	m.data.SetDescription("The number of connections that were aborted because the client died without closing the connection properly.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetAbortedClients) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetAbortedClients) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetAbortedClients) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetAbortedClients(cfg MetricConfig) metricNewrelicmysqlNetAbortedClients {
	m := metricNewrelicmysqlNetAbortedClients{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetAbortedConnects struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.aborted_connects metric with initial data.
func (m *metricNewrelicmysqlNetAbortedConnects) init() {
	m.data.SetName("newrelicmysql.net.aborted_connects")
	m.data.SetDescription("The number of failed attempts to connect to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetAbortedConnects) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetAbortedConnects) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetAbortedConnects) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetAbortedConnects(cfg MetricConfig) metricNewrelicmysqlNetAbortedConnects {
	m := metricNewrelicmysqlNetAbortedConnects{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.connections metric with initial data.
func (m *metricNewrelicmysqlNetConnections) init() {
	m.data.SetName("newrelicmysql.net.connections")
	m.data.SetDescription("The number of connection attempts (successful or not) to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetConnections(cfg MetricConfig) metricNewrelicmysqlNetConnections {
	m := metricNewrelicmysqlNetConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_connections metric with initial data.
func (m *metricNewrelicmysqlNetMaxConnections) init() {
	m.data.SetName("newrelicmysql.net.max_connections")
	m.data.SetDescription("The maximum permitted number of simultaneous client connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxConnections(cfg MetricConfig) metricNewrelicmysqlNetMaxConnections {
	m := metricNewrelicmysqlNetMaxConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxConnectionsAvailable struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_connections_available metric with initial data.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) init() {
	m.data.SetName("newrelicmysql.net.max_connections_available")
	m.data.SetDescription("The number of available connections (max_connections - threads_connected).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxConnectionsAvailable(cfg MetricConfig) metricNewrelicmysqlNetMaxConnectionsAvailable {
	m := metricNewrelicmysqlNetMaxConnectionsAvailable{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxUsedConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_used_connections metric with initial data.
func (m *metricNewrelicmysqlNetMaxUsedConnections) init() {
	m.data.SetName("newrelicmysql.net.max_used_connections")
	m.data.SetDescription("The maximum number of connections that have been in use simultaneously since the server started.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxUsedConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxUsedConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxUsedConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxUsedConnections(cfg MetricConfig) metricNewrelicmysqlNetMaxUsedConnections {
	m := metricNewrelicmysqlNetMaxUsedConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceBytesReceived struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.bytes_received metric with initial data.
func (m *metricNewrelicmysqlPerformanceBytesReceived) init() {
	m.data.SetName("newrelicmysql.performance.bytes_received")
	m.data.SetDescription("The number of bytes received from all clients.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceBytesReceived) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceBytesReceived) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceBytesReceived) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceBytesReceived(cfg MetricConfig) metricNewrelicmysqlPerformanceBytesReceived {
	m := metricNewrelicmysqlPerformanceBytesReceived{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceBytesSent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.bytes_sent metric with initial data.
func (m *metricNewrelicmysqlPerformanceBytesSent) init() {
	m.data.SetName("newrelicmysql.performance.bytes_sent")
	m.data.SetDescription("The number of bytes sent to all clients.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceBytesSent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceBytesSent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceBytesSent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceBytesSent(cfg MetricConfig) metricNewrelicmysqlPerformanceBytesSent {
	m := metricNewrelicmysqlPerformanceBytesSent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpDiskTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_disk_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_disk_tables")
	m.data.SetDescription("The number of internal on-disk temporary tables created by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpDiskTables(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpDiskTables {
	m := metricNewrelicmysqlPerformanceCreatedTmpDiskTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_files metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_files")
	m.data.SetDescription("How many temporary files mysqld has created.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpFiles(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpFiles {
	m := metricNewrelicmysqlPerformanceCreatedTmpFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_tables")
	m.data.SetDescription("The number of internal temporary tables created by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpTables(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpTables {
	m := metricNewrelicmysqlPerformanceCreatedTmpTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerCommit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_commit metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) init() {
	m.data.SetName("newrelicmysql.performance.handler_commit")
	m.data.SetDescription("The number of internal COMMIT statements.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerCommit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerCommit(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerCommit {
	m := metricNewrelicmysqlPerformanceHandlerCommit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerDelete struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_delete metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) init() {
	m.data.SetName("newrelicmysql.performance.handler_delete")
	m.data.SetDescription("The number of times that rows have been deleted from tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerDelete) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerDelete(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerDelete {
	m := metricNewrelicmysqlPerformanceHandlerDelete{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerPrepare struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_prepare metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) init() {
	m.data.SetName("newrelicmysql.performance.handler_prepare")
	m.data.SetDescription("A counter for the prepare phase of two-phase commit operations.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerPrepare) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerPrepare(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerPrepare {
	m := metricNewrelicmysqlPerformanceHandlerPrepare{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadFirst struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_first metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_first")
	m.data.SetDescription("The number of times the first entry in an index was read.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadFirst(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadFirst {
	m := metricNewrelicmysqlPerformanceHandlerReadFirst{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadKey struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_key metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_key")
	m.data.SetDescription("The number of requests to read a row based on a key.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadKey) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadKey(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadKey {
	m := metricNewrelicmysqlPerformanceHandlerReadKey{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadNext struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_next metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_next")
	m.data.SetDescription("The number of requests to read the next row in key order.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadNext) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadNext(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadNext {
	m := metricNewrelicmysqlPerformanceHandlerReadNext{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadPrev struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_prev metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_prev")
	m.data.SetDescription("The number of requests to read the previous row in key order.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadPrev(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadPrev {
	m := metricNewrelicmysqlPerformanceHandlerReadPrev{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadRnd struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_rnd metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_rnd")
	m.data.SetDescription("The number of requests to read a row based on a fixed position.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadRnd(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadRnd {
	m := metricNewrelicmysqlPerformanceHandlerReadRnd{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadRndNext struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_rnd_next metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_rnd_next")
	m.data.SetDescription("The number of requests to read the next row in the data file.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadRndNext(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadRndNext {
	m := metricNewrelicmysqlPerformanceHandlerReadRndNext{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerRollback struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_rollback metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) init() {
	m.data.SetName("newrelicmysql.performance.handler_rollback")
	m.data.SetDescription("The number of requests for a storage engine to perform a rollback operation.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerRollback) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerRollback(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerRollback {
	m := metricNewrelicmysqlPerformanceHandlerRollback{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerUpdate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_update metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) init() {
	m.data.SetName("newrelicmysql.performance.handler_update")
	m.data.SetDescription("The number of requests to update a row in a table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerUpdate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerUpdate(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerUpdate {
	m := metricNewrelicmysqlPerformanceHandlerUpdate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerWrite struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_write metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) init() {
	m.data.SetName("newrelicmysql.performance.handler_write")
	m.data.SetDescription("The number of requests to insert a row in a table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerWrite) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerWrite(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerWrite {
	m := metricNewrelicmysqlPerformanceHandlerWrite{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceKeyCacheUtilization struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.key_cache_utilization metric with initial data.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) init() {
	m.data.SetName("newrelicmysql.performance.key_cache_utilization")
	m.data.SetDescription("The key cache utilization percentage.")
	m.data.SetUnit("%")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceKeyCacheUtilization(cfg MetricConfig) metricNewrelicmysqlPerformanceKeyCacheUtilization {
	m := metricNewrelicmysqlPerformanceKeyCacheUtilization{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceMaxPreparedStmtCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.max_prepared_stmt_count metric with initial data.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) init() {
	m.data.SetName("newrelicmysql.performance.max_prepared_stmt_count")
	m.data.SetDescription("Maximum number of prepared statements.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceMaxPreparedStmtCount(cfg MetricConfig) metricNewrelicmysqlPerformanceMaxPreparedStmtCount {
	m := metricNewrelicmysqlPerformanceMaxPreparedStmtCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.open_files metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenFiles) init() {
	m.data.SetName("newrelicmysql.performance.open_files")
	m.data.SetDescription("The number of files that are currently open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceOpenFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenFiles) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenFiles(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenFiles {
	m := metricNewrelicmysqlPerformanceOpenFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.open_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenTables) init() {
	m.data.SetName("newrelicmysql.performance.open_tables")
	m.data.SetDescription("The number of tables that are currently open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceOpenTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenTables) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenTables(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenTables {
	m := metricNewrelicmysqlPerformanceOpenTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenedTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.opened_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenedTables) init() {
	m.data.SetName("newrelicmysql.performance.opened_tables")
	m.data.SetDescription("The number of tables that have been opened. If this value is large, your table_open_cache value is probably too small.")
	m.data.SetUnit("{tables}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceOpenedTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenedTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenedTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenedTables(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenedTables {
	m := metricNewrelicmysqlPerformanceOpenedTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformancePerformanceSchemaDigestLost struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.performance_schema_digest_lost metric with initial data.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) init() {
	m.data.SetName("newrelicmysql.performance.performance_schema_digest_lost")
	m.data.SetDescription("Number of digest lost in performance schema.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformancePerformanceSchemaDigestLost(cfg MetricConfig) metricNewrelicmysqlPerformancePerformanceSchemaDigestLost {
	m := metricNewrelicmysqlPerformancePerformanceSchemaDigestLost{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformancePreparedStmtCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.prepared_stmt_count metric with initial data.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) init() {
	m.data.SetName("newrelicmysql.performance.prepared_stmt_count")
	m.data.SetDescription("Current number of prepared statements.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformancePreparedStmtCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformancePreparedStmtCount(cfg MetricConfig) metricNewrelicmysqlPerformancePreparedStmtCount {
	m := metricNewrelicmysqlPerformancePreparedStmtCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheFreeBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_free_blocks metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) init() {
	m.data.SetName("newrelicmysql.performance.qcache_free_blocks")
	m.data.SetDescription("The number of free memory blocks in the query cache.")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheFreeBlocks(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheFreeBlocks {
	m := metricNewrelicmysqlPerformanceQcacheFreeBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheFreeMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_free_memory metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) init() {
	m.data.SetName("newrelicmysql.performance.qcache_free_memory")
	m.data.SetDescription("The amount of free memory for the query cache.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheFreeMemory(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheFreeMemory {
	m := metricNewrelicmysqlPerformanceQcacheFreeMemory{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_hits metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheHits) init() {
	m.data.SetName("newrelicmysql.performance.qcache_hits")
	m.data.SetDescription("The number of query cache hits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheHits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheHits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheHits(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheHits {
	m := metricNewrelicmysqlPerformanceQcacheHits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheInserts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_inserts metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) init() {
	m.data.SetName("newrelicmysql.performance.qcache_inserts")
	m.data.SetDescription("The number of queries added to the query cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheInserts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheInserts(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheInserts {
	m := metricNewrelicmysqlPerformanceQcacheInserts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheLowmemPrunes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_lowmem_prunes metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) init() {
	m.data.SetName("newrelicmysql.performance.qcache_lowmem_prunes")
	m.data.SetDescription("The number of queries that were deleted from the query cache because of low memory.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheLowmemPrunes(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheLowmemPrunes {
	m := metricNewrelicmysqlPerformanceQcacheLowmemPrunes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheNotCached struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_not_cached metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) init() {
	m.data.SetName("newrelicmysql.performance.qcache_not_cached")
	m.data.SetDescription("The number of queries that were not cached in the query cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheNotCached) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheNotCached(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheNotCached {
	m := metricNewrelicmysqlPerformanceQcacheNotCached{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheQueriesInCache struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_queries_in_cache metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) init() {
	m.data.SetName("newrelicmysql.performance.qcache_queries_in_cache")
	m.data.SetDescription("The number of queries registered in the query cache.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheQueriesInCache(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheQueriesInCache {
	m := metricNewrelicmysqlPerformanceQcacheQueriesInCache{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_size metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheSize) init() {
	m.data.SetName("newrelicmysql.performance.qcache_size")
	m.data.SetDescription("The amount of memory allocated for the query cache.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheSize(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheSize {
	m := metricNewrelicmysqlPerformanceQcacheSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheTotalBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_total_blocks metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) init() {
	m.data.SetName("newrelicmysql.performance.qcache_total_blocks")
	m.data.SetDescription("The total number of blocks in the query cache.")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheTotalBlocks(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheTotalBlocks {
	m := metricNewrelicmysqlPerformanceQcacheTotalBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQuestions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.questions metric with initial data.
func (m *metricNewrelicmysqlPerformanceQuestions) init() {
	m.data.SetName("newrelicmysql.performance.questions")
	m.data.SetDescription("The number of statements executed by the server (sent by clients).")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQuestions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQuestions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQuestions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQuestions(cfg MetricConfig) metricNewrelicmysqlPerformanceQuestions {
	m := metricNewrelicmysqlPerformanceQuestions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectFullJoin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_full_join metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) init() {
	m.data.SetName("newrelicmysql.performance.select_full_join")
	m.data.SetDescription("The number of joins that perform table scans because they do not use indexes. If this value is not 0, you should carefully check the indexes of your tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectFullJoin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectFullJoin(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectFullJoin {
	m := metricNewrelicmysqlPerformanceSelectFullJoin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectFullRangeJoin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_full_range_join metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) init() {
	m.data.SetName("newrelicmysql.performance.select_full_range_join")
	m.data.SetDescription("The number of joins that used a range search on a reference table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectFullRangeJoin(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectFullRangeJoin {
	m := metricNewrelicmysqlPerformanceSelectFullRangeJoin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectRange struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_range metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectRange) init() {
	m.data.SetName("newrelicmysql.performance.select_range")
	m.data.SetDescription("The number of joins that used ranges on the first table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectRange) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectRange) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectRange) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectRange(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectRange {
	m := metricNewrelicmysqlPerformanceSelectRange{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectRangeCheck struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_range_check metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) init() {
	m.data.SetName("newrelicmysql.performance.select_range_check")
	m.data.SetDescription("The number of joins without keys that check for key usage after each row. If this is not 0, you should carefully check the indexes of your tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectRangeCheck(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectRangeCheck {
	m := metricNewrelicmysqlPerformanceSelectRangeCheck{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectScan struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_scan metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectScan) init() {
	m.data.SetName("newrelicmysql.performance.select_scan")
	m.data.SetDescription("The number of joins that did a full scan of the first table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectScan) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectScan) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectScan) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectScan(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectScan {
	m := metricNewrelicmysqlPerformanceSelectScan{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSlowQueries struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.slow_queries metric with initial data.
func (m *metricNewrelicmysqlPerformanceSlowQueries) init() {
	m.data.SetName("newrelicmysql.performance.slow_queries")
	m.data.SetDescription("The number of queries that have taken more than long_query_time seconds.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSlowQueries) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSlowQueries) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSlowQueries) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSlowQueries(cfg MetricConfig) metricNewrelicmysqlPerformanceSlowQueries {
	m := metricNewrelicmysqlPerformanceSlowQueries{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortMergePasses struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_merge_passes metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) init() {
	m.data.SetName("newrelicmysql.performance.sort_merge_passes")
	m.data.SetDescription("The number of merge passes that the sort algorithm has had to do. If this value is large, you should consider increasing the value of the sort_buffer_size system variable.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortMergePasses) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortMergePasses(cfg MetricConfig) metricNewrelicmysqlPerformanceSortMergePasses {
	m := metricNewrelicmysqlPerformanceSortMergePasses{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortRange struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_range metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortRange) init() {
	m.data.SetName("newrelicmysql.performance.sort_range")
	m.data.SetDescription("The number of sorts that were done using ranges.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortRange) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortRange) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortRange) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortRange(cfg MetricConfig) metricNewrelicmysqlPerformanceSortRange {
	m := metricNewrelicmysqlPerformanceSortRange{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_rows metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortRows) init() {
	m.data.SetName("newrelicmysql.performance.sort_rows")
	m.data.SetDescription("The number of sorted rows.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortRows) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortRows(cfg MetricConfig) metricNewrelicmysqlPerformanceSortRows {
	m := metricNewrelicmysqlPerformanceSortRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortScan struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_scan metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortScan) init() {
	m.data.SetName("newrelicmysql.performance.sort_scan")
	m.data.SetDescription("The number of sorts that were done by scanning the table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortScan) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortScan) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortScan) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortScan(cfg MetricConfig) metricNewrelicmysqlPerformanceSortScan {
	m := metricNewrelicmysqlPerformanceSortScan{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksImmediate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_immediate metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_immediate")
	m.data.SetDescription("The number of times that a request for a table lock could be granted immediately.")
	m.data.SetUnit("{locks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksImmediate(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksImmediate {
	m := metricNewrelicmysqlPerformanceTableLocksImmediate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksImmediateRate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_immediate.rate metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_immediate.rate")
	m.data.SetDescription("The rate of times that a request for a table lock could be granted immediately.")
	m.data.SetUnit("{locks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksImmediateRate(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksImmediateRate {
	m := metricNewrelicmysqlPerformanceTableLocksImmediateRate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksWaited struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_waited metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_waited")
	m.data.SetDescription("The total number of times that a request for a table lock could not be granted immediately.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceTableLocksWaited) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksWaited(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksWaited {
	m := metricNewrelicmysqlPerformanceTableLocksWaited{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableOpenCache struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_open_cache metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) init() {
	m.data.SetName("newrelicmysql.performance.table_open_cache")
	m.data.SetDescription("The number of open tables for all threads.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceTableOpenCache) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableOpenCache(cfg MetricConfig) metricNewrelicmysqlPerformanceTableOpenCache {
	m := metricNewrelicmysqlPerformanceTableOpenCache{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadCacheSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.thread_cache_size metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) init() {
	m.data.SetName("newrelicmysql.performance.thread_cache_size")
	m.data.SetDescription("Thread cache size.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadCacheSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadCacheSize(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadCacheSize {
	m := metricNewrelicmysqlPerformanceThreadCacheSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsCached struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_cached metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsCached) init() {
	m.data.SetName("newrelicmysql.performance.threads_cached")
	m.data.SetDescription("The number of threads in the thread cache.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsCached) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsCached) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsCached) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsCached(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsCached {
	m := metricNewrelicmysqlPerformanceThreadsCached{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsConnected struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_connected metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) init() {
	m.data.SetName("newrelicmysql.performance.threads_connected")
	m.data.SetDescription("The number of currently open connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsConnected) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsConnected(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsConnected {
	m := metricNewrelicmysqlPerformanceThreadsConnected{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsCreated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_created metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) init() {
	m.data.SetName("newrelicmysql.performance.threads_created")
	m.data.SetDescription("The number of threads created to handle connections. If this value is large, you may want to increase the thread_cache_size value.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceThreadsCreated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsCreated(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsCreated {
	m := metricNewrelicmysqlPerformanceThreadsCreated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_running metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) init() {
	m.data.SetName("newrelicmysql.performance.threads_running")
	m.data.SetDescription("The number of threads that are not sleeping.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsRunning(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsRunning {
	m := metricNewrelicmysqlPerformanceThreadsRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.query.count metric with initial data.
func (m *metricNewrelicmysqlQueryCount) init() {
	m.data.SetName("newrelicmysql.query.count")
	m.data.SetDescription("The number of statements executed by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlQueryCount(cfg MetricConfig) metricNewrelicmysqlQueryCount {
	m := metricNewrelicmysqlQueryCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationExecMasterLogPos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.exec_master_log_pos metric with initial data.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) init() {
	m.data.SetName("newrelicmysql.replication.exec_master_log_pos")
	m.data.SetDescription("The position in the current master binary log file to which the replication SQL thread has read and executed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationExecMasterLogPos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationExecMasterLogPos(cfg MetricConfig) metricNewrelicmysqlReplicationExecMasterLogPos {
	m := metricNewrelicmysqlReplicationExecMasterLogPos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationLastIoErrno struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.last_io_errno metric with initial data.
func (m *metricNewrelicmysqlReplicationLastIoErrno) init() {
	m.data.SetName("newrelicmysql.replication.last_io_errno")
	m.data.SetDescription("The error number of the most recent error that caused the replication I/O thread to stop.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationLastIoErrno) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationLastIoErrno) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationLastIoErrno) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationLastIoErrno(cfg MetricConfig) metricNewrelicmysqlReplicationLastIoErrno {
	m := metricNewrelicmysqlReplicationLastIoErrno{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationLastSQLErrno struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.last_sql_errno metric with initial data.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) init() {
	m.data.SetName("newrelicmysql.replication.last_sql_errno")
	m.data.SetDescription("The error number of the most recent error that caused the replication SQL thread to stop.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationLastSQLErrno) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationLastSQLErrno(cfg MetricConfig) metricNewrelicmysqlReplicationLastSQLErrno {
	m := metricNewrelicmysqlReplicationLastSQLErrno{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationReadMasterLogPos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.read_master_log_pos metric with initial data.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) init() {
	m.data.SetName("newrelicmysql.replication.read_master_log_pos")
	m.data.SetDescription("The position in the current master binary log file up to which the replication I/O thread has read.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationReadMasterLogPos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationReadMasterLogPos(cfg MetricConfig) metricNewrelicmysqlReplicationReadMasterLogPos {
	m := metricNewrelicmysqlReplicationReadMasterLogPos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationRelayLogSpace struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.relay_log_space metric with initial data.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) init() {
	m.data.SetName("newrelicmysql.replication.relay_log_space")
	m.data.SetDescription("The total combined size of all existing relay log files.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationRelayLogSpace) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationRelayLogSpace(cfg MetricConfig) metricNewrelicmysqlReplicationRelayLogSpace {
	m := metricNewrelicmysqlReplicationRelayLogSpace{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSecondsBehindMaster struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.seconds_behind_master metric with initial data.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) init() {
	m.data.SetName("newrelicmysql.replication.seconds_behind_master")
	m.data.SetDescription("The number of seconds that the replica SQL thread is behind processing the master binary log.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSecondsBehindMaster(cfg MetricConfig) metricNewrelicmysqlReplicationSecondsBehindMaster {
	m := metricNewrelicmysqlReplicationSecondsBehindMaster{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveIoRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_io_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_io_running")
	m.data.SetDescription("Status of the replication I/O thread. 0=No/Stopped, 1=Yes/Running, 2=Connecting.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveIoRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveIoRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveIoRunning {
	m := metricNewrelicmysqlReplicationSlaveIoRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_running")
	m.data.SetDescription("Whether the replica is currently running (both I/O and SQL threads are running). 1=Both running, 0=One or both stopped.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveRunning {
	m := metricNewrelicmysqlReplicationSlaveRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveSQLRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_sql_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_sql_running")
	m.data.SetDescription("Status of the replication SQL thread. 0=No/Stopped, 1=Yes/Running.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveSQLRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveSQLRunning {
	m := metricNewrelicmysqlReplicationSlaveSQLRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlUptime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.uptime metric with initial data.
func (m *metricNewrelicmysqlUptime) init() {
	m.data.SetName("newrelicmysql.uptime")
	m.data.SetDescription("The number of seconds that the server has been up.")
	m.data.SetUnit("s")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlUptime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlUptime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlUptime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlUptime(cfg MetricConfig) metricNewrelicmysqlUptime {
	m := metricNewrelicmysqlUptime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                                    MetricsBuilderConfig // config of the metrics builder.
	startTime                                                 pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                           int                  // maximum observed number of metrics per resource.
	metricsBuffer                                             pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                                 component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter                            map[string]filter.Filter
	resourceAttributeExcludeFilter                            map[string]filter.Filter
	metricNewrelicmysqlBinlogCacheDiskUse                     metricNewrelicmysqlBinlogCacheDiskUse
	metricNewrelicmysqlBinlogCacheUse                         metricNewrelicmysqlBinlogCacheUse
	metricNewrelicmysqlCommands                               metricNewrelicmysqlCommands
	metricNewrelicmysqlConnectionCount                        metricNewrelicmysqlConnectionCount
	metricNewrelicmysqlDbHandlerRollback                      metricNewrelicmysqlDbHandlerRollback
	metricNewrelicmysqlDbOpenedTables                         metricNewrelicmysqlDbOpenedTables
	metricNewrelicmysqlGaleraWsrepCertDepsDistance            metricNewrelicmysqlGaleraWsrepCertDepsDistance
	metricNewrelicmysqlGaleraWsrepClusterSize                 metricNewrelicmysqlGaleraWsrepClusterSize
	metricNewrelicmysqlGaleraWsrepFlowControlPaused           metricNewrelicmysqlGaleraWsrepFlowControlPaused
	metricNewrelicmysqlGaleraWsrepFlowControlPausedNs         metricNewrelicmysqlGaleraWsrepFlowControlPausedNs
	metricNewrelicmysqlGaleraWsrepFlowControlRecv             metricNewrelicmysqlGaleraWsrepFlowControlRecv
	metricNewrelicmysqlGaleraWsrepFlowControlSent             metricNewrelicmysqlGaleraWsrepFlowControlSent
	metricNewrelicmysqlGaleraWsrepLocalCertFailures           metricNewrelicmysqlGaleraWsrepLocalCertFailures
	metricNewrelicmysqlGaleraWsrepLocalRecvQueue              metricNewrelicmysqlGaleraWsrepLocalRecvQueue
	metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg           metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg
	metricNewrelicmysqlGaleraWsrepLocalSendQueue              metricNewrelicmysqlGaleraWsrepLocalSendQueue
	metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg           metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg
	metricNewrelicmysqlGaleraWsrepLocalState                  metricNewrelicmysqlGaleraWsrepLocalState
	metricNewrelicmysqlGaleraWsrepReceived                    metricNewrelicmysqlGaleraWsrepReceived
	metricNewrelicmysqlGaleraWsrepReceivedBytes               metricNewrelicmysqlGaleraWsrepReceivedBytes
	metricNewrelicmysqlGaleraWsrepReplicatedBytes             metricNewrelicmysqlGaleraWsrepReplicatedBytes
	metricNewrelicmysqlInnodbActiveTransactions               metricNewrelicmysqlInnodbActiveTransactions
	metricNewrelicmysqlInnodbAdaptiveHashHashSearches         metricNewrelicmysqlInnodbAdaptiveHashHashSearches
	metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches      metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches
	metricNewrelicmysqlInnodbAdaptiveHashPagesAdded           metricNewrelicmysqlInnodbAdaptiveHashPagesAdded
	metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved         metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved
	metricNewrelicmysqlInnodbAvailableUndoLogs                metricNewrelicmysqlInnodbAvailableUndoLogs
	metricNewrelicmysqlInnodbBufferPoolBytesData              metricNewrelicmysqlInnodbBufferPoolBytesData
	metricNewrelicmysqlInnodbBufferPoolBytesDirty             metricNewrelicmysqlInnodbBufferPoolBytesDirty
	metricNewrelicmysqlInnodbBufferPoolDirty                  metricNewrelicmysqlInnodbBufferPoolDirty
	metricNewrelicmysqlInnodbBufferPoolFree                   metricNewrelicmysqlInnodbBufferPoolFree
	metricNewrelicmysqlInnodbBufferPoolPagesData              metricNewrelicmysqlInnodbBufferPoolPagesData
	metricNewrelicmysqlInnodbBufferPoolPagesFlushed           metricNewrelicmysqlInnodbBufferPoolPagesFlushed
	metricNewrelicmysqlInnodbBufferPoolPagesFree              metricNewrelicmysqlInnodbBufferPoolPagesFree
	metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed        metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed
	metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung      metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung
	metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung         metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung
	metricNewrelicmysqlInnodbBufferPoolPagesMisc              metricNewrelicmysqlInnodbBufferPoolPagesMisc
	metricNewrelicmysqlInnodbBufferPoolPagesOld               metricNewrelicmysqlInnodbBufferPoolPagesOld
	metricNewrelicmysqlInnodbBufferPoolPagesTotal             metricNewrelicmysqlInnodbBufferPoolPagesTotal
	metricNewrelicmysqlInnodbBufferPoolReadAhead              metricNewrelicmysqlInnodbBufferPoolReadAhead
	metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted       metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted
	metricNewrelicmysqlInnodbBufferPoolReadAheadRnd           metricNewrelicmysqlInnodbBufferPoolReadAheadRnd
	metricNewrelicmysqlInnodbBufferPoolReadRequests           metricNewrelicmysqlInnodbBufferPoolReadRequests
	metricNewrelicmysqlInnodbBufferPoolReads                  metricNewrelicmysqlInnodbBufferPoolReads
	metricNewrelicmysqlInnodbBufferPoolTotal                  metricNewrelicmysqlInnodbBufferPoolTotal
	metricNewrelicmysqlInnodbBufferPoolUsed                   metricNewrelicmysqlInnodbBufferPoolUsed
	metricNewrelicmysqlInnodbBufferPoolUtilization            metricNewrelicmysqlInnodbBufferPoolUtilization
	metricNewrelicmysqlInnodbBufferPoolWaitFree               metricNewrelicmysqlInnodbBufferPoolWaitFree
	metricNewrelicmysqlInnodbBufferPoolWriteRequests          metricNewrelicmysqlInnodbBufferPoolWriteRequests
	metricNewrelicmysqlInnodbCheckpointAge                    metricNewrelicmysqlInnodbCheckpointAge
	metricNewrelicmysqlInnodbCurrentRowLocks                  metricNewrelicmysqlInnodbCurrentRowLocks
	metricNewrelicmysqlInnodbCurrentTransactions              metricNewrelicmysqlInnodbCurrentTransactions
	metricNewrelicmysqlInnodbDataFsyncs                       metricNewrelicmysqlInnodbDataFsyncs
	metricNewrelicmysqlInnodbDataPendingFsyncs                metricNewrelicmysqlInnodbDataPendingFsyncs
	metricNewrelicmysqlInnodbDataPendingReads                 metricNewrelicmysqlInnodbDataPendingReads
	metricNewrelicmysqlInnodbDataPendingWrites                metricNewrelicmysqlInnodbDataPendingWrites
	metricNewrelicmysqlInnodbDataRead                         metricNewrelicmysqlInnodbDataRead
	metricNewrelicmysqlInnodbDataReads                        metricNewrelicmysqlInnodbDataReads
	metricNewrelicmysqlInnodbDataWrites                       metricNewrelicmysqlInnodbDataWrites
	metricNewrelicmysqlInnodbDataWritten                      metricNewrelicmysqlInnodbDataWritten
	metricNewrelicmysqlInnodbDblwrPagesWritten                metricNewrelicmysqlInnodbDblwrPagesWritten
	metricNewrelicmysqlInnodbDblwrWrites                      metricNewrelicmysqlInnodbDblwrWrites
	metricNewrelicmysqlInnodbHashIndexCellsTotal              metricNewrelicmysqlInnodbHashIndexCellsTotal
	metricNewrelicmysqlInnodbHashIndexCellsUsed               metricNewrelicmysqlInnodbHashIndexCellsUsed
	metricNewrelicmysqlInnodbHistoryListLength                metricNewrelicmysqlInnodbHistoryListLength
	metricNewrelicmysqlInnodbIbufFreeList                     metricNewrelicmysqlInnodbIbufFreeList
	metricNewrelicmysqlInnodbIbufMergedDeleteMarks            metricNewrelicmysqlInnodbIbufMergedDeleteMarks
	metricNewrelicmysqlInnodbIbufMergedDeletes                metricNewrelicmysqlInnodbIbufMergedDeletes
	metricNewrelicmysqlInnodbIbufMergedInserts                metricNewrelicmysqlInnodbIbufMergedInserts
	metricNewrelicmysqlInnodbIbufMerges                       metricNewrelicmysqlInnodbIbufMerges
	metricNewrelicmysqlInnodbIbufSegmentSize                  metricNewrelicmysqlInnodbIbufSegmentSize
	metricNewrelicmysqlInnodbIbufSize                         metricNewrelicmysqlInnodbIbufSize
	metricNewrelicmysqlInnodbLockStructs                      metricNewrelicmysqlInnodbLockStructs
	metricNewrelicmysqlInnodbLockedTables                     metricNewrelicmysqlInnodbLockedTables
	metricNewrelicmysqlInnodbLockedTransactions               metricNewrelicmysqlInnodbLockedTransactions
	metricNewrelicmysqlInnodbLogWaits                         metricNewrelicmysqlInnodbLogWaits
	metricNewrelicmysqlInnodbLogWriteRequests                 metricNewrelicmysqlInnodbLogWriteRequests
	metricNewrelicmysqlInnodbLogWrites                        metricNewrelicmysqlInnodbLogWrites
	metricNewrelicmysqlInnodbLsnCurrent                       metricNewrelicmysqlInnodbLsnCurrent
	metricNewrelicmysqlInnodbLsnFlushed                       metricNewrelicmysqlInnodbLsnFlushed
	metricNewrelicmysqlInnodbLsnLastCheckpoint                metricNewrelicmysqlInnodbLsnLastCheckpoint
	metricNewrelicmysqlInnodbMasterThreadActiveLoops          metricNewrelicmysqlInnodbMasterThreadActiveLoops
	metricNewrelicmysqlInnodbMasterThreadIdleLoops            metricNewrelicmysqlInnodbMasterThreadIdleLoops
	metricNewrelicmysqlInnodbMemAdaptiveHash                  metricNewrelicmysqlInnodbMemAdaptiveHash
	metricNewrelicmysqlInnodbMemAdditionalPool                metricNewrelicmysqlInnodbMemAdditionalPool
	metricNewrelicmysqlInnodbMemDictionary                    metricNewrelicmysqlInnodbMemDictionary
	metricNewrelicmysqlInnodbMemFileSystem                    metricNewrelicmysqlInnodbMemFileSystem
	metricNewrelicmysqlInnodbMemLockSystem                    metricNewrelicmysqlInnodbMemLockSystem
	metricNewrelicmysqlInnodbMemPageHash                      metricNewrelicmysqlInnodbMemPageHash
	metricNewrelicmysqlInnodbMemRecoverySystem                metricNewrelicmysqlInnodbMemRecoverySystem
	metricNewrelicmysqlInnodbMemThreadHash                    metricNewrelicmysqlInnodbMemThreadHash
	metricNewrelicmysqlInnodbMemTotal                         metricNewrelicmysqlInnodbMemTotal
	metricNewrelicmysqlInnodbMutexOsWaits                     metricNewrelicmysqlInnodbMutexOsWaits
	metricNewrelicmysqlInnodbMutexSpinRounds                  metricNewrelicmysqlInnodbMutexSpinRounds
	metricNewrelicmysqlInnodbMutexSpinWaits                   metricNewrelicmysqlInnodbMutexSpinWaits
	metricNewrelicmysqlInnodbNumOpenFiles                     metricNewrelicmysqlInnodbNumOpenFiles
	metricNewrelicmysqlInnodbOsFileFsyncs                     metricNewrelicmysqlInnodbOsFileFsyncs
	metricNewrelicmysqlInnodbOsFileReads                      metricNewrelicmysqlInnodbOsFileReads
	metricNewrelicmysqlInnodbOsFileWrites                     metricNewrelicmysqlInnodbOsFileWrites
	metricNewrelicmysqlInnodbOsLogFsyncs                      metricNewrelicmysqlInnodbOsLogFsyncs
	metricNewrelicmysqlInnodbOsLogPendingFsyncs               metricNewrelicmysqlInnodbOsLogPendingFsyncs
	metricNewrelicmysqlInnodbOsLogPendingWrites               metricNewrelicmysqlInnodbOsLogPendingWrites
	metricNewrelicmysqlInnodbOsLogWritten                     metricNewrelicmysqlInnodbOsLogWritten
	metricNewrelicmysqlInnodbPageSize                         metricNewrelicmysqlInnodbPageSize
	metricNewrelicmysqlInnodbPagesCreated                     metricNewrelicmysqlInnodbPagesCreated
	metricNewrelicmysqlInnodbPagesRead                        metricNewrelicmysqlInnodbPagesRead
	metricNewrelicmysqlInnodbPagesWritten                     metricNewrelicmysqlInnodbPagesWritten
	metricNewrelicmysqlInnodbPendingAioLogIos                 metricNewrelicmysqlInnodbPendingAioLogIos
	metricNewrelicmysqlInnodbPendingAioSyncIos                metricNewrelicmysqlInnodbPendingAioSyncIos
	metricNewrelicmysqlInnodbPendingBufferPoolFlushes         metricNewrelicmysqlInnodbPendingBufferPoolFlushes
	metricNewrelicmysqlInnodbPendingCheckpointWrites          metricNewrelicmysqlInnodbPendingCheckpointWrites
	metricNewrelicmysqlInnodbPendingIbufAioReads              metricNewrelicmysqlInnodbPendingIbufAioReads
	metricNewrelicmysqlInnodbPendingLogFlushes                metricNewrelicmysqlInnodbPendingLogFlushes
	metricNewrelicmysqlInnodbPendingLogWrites                 metricNewrelicmysqlInnodbPendingLogWrites
	metricNewrelicmysqlInnodbPendingNormalAioReads            metricNewrelicmysqlInnodbPendingNormalAioReads
	metricNewrelicmysqlInnodbPendingNormalAioWrites           metricNewrelicmysqlInnodbPendingNormalAioWrites
	metricNewrelicmysqlInnodbPurgeTrxID                       metricNewrelicmysqlInnodbPurgeTrxID
	metricNewrelicmysqlInnodbPurgeUndoNo                      metricNewrelicmysqlInnodbPurgeUndoNo
	metricNewrelicmysqlInnodbQueriesInside                    metricNewrelicmysqlInnodbQueriesInside
	metricNewrelicmysqlInnodbQueriesQueued                    metricNewrelicmysqlInnodbQueriesQueued
	metricNewrelicmysqlInnodbReadViews                        metricNewrelicmysqlInnodbReadViews
	metricNewrelicmysqlInnodbRedoLogEnabled                   metricNewrelicmysqlInnodbRedoLogEnabled
	metricNewrelicmysqlInnodbRowLockCurrentWaits              metricNewrelicmysqlInnodbRowLockCurrentWaits
	metricNewrelicmysqlInnodbRowLockTime                      metricNewrelicmysqlInnodbRowLockTime
	metricNewrelicmysqlInnodbRowLockTimeAvg                   metricNewrelicmysqlInnodbRowLockTimeAvg
	metricNewrelicmysqlInnodbRowLockTimeMax                   metricNewrelicmysqlInnodbRowLockTimeMax
	metricNewrelicmysqlInnodbRowLockWaits                     metricNewrelicmysqlInnodbRowLockWaits
	metricNewrelicmysqlInnodbRowsDeleted                      metricNewrelicmysqlInnodbRowsDeleted
	metricNewrelicmysqlInnodbRowsInserted                     metricNewrelicmysqlInnodbRowsInserted
	metricNewrelicmysqlInnodbRowsRead                         metricNewrelicmysqlInnodbRowsRead
	metricNewrelicmysqlInnodbRowsUpdated                      metricNewrelicmysqlInnodbRowsUpdated
	metricNewrelicmysqlInnodbSLockOsWaits                     metricNewrelicmysqlInnodbSLockOsWaits
	metricNewrelicmysqlInnodbSLockSpinRounds                  metricNewrelicmysqlInnodbSLockSpinRounds
	metricNewrelicmysqlInnodbSLockSpinWaits                   metricNewrelicmysqlInnodbSLockSpinWaits
	metricNewrelicmysqlInnodbSemaphoreWaitTime                metricNewrelicmysqlInnodbSemaphoreWaitTime
	metricNewrelicmysqlInnodbSemaphoreWaits                   metricNewrelicmysqlInnodbSemaphoreWaits
	metricNewrelicmysqlInnodbTablesInUse                      metricNewrelicmysqlInnodbTablesInUse
	metricNewrelicmysqlInnodbTruncatedStatusWrites            metricNewrelicmysqlInnodbTruncatedStatusWrites
	metricNewrelicmysqlInnodbUndoTablespacesActive            metricNewrelicmysqlInnodbUndoTablespacesActive
	metricNewrelicmysqlInnodbUndoTablespacesExplicit          metricNewrelicmysqlInnodbUndoTablespacesExplicit
	metricNewrelicmysqlInnodbUndoTablespacesImplicit          metricNewrelicmysqlInnodbUndoTablespacesImplicit
	metricNewrelicmysqlInnodbUndoTablespacesTotal             metricNewrelicmysqlInnodbUndoTablespacesTotal
	metricNewrelicmysqlInnodbXLockOsWaits                     metricNewrelicmysqlInnodbXLockOsWaits
	metricNewrelicmysqlInnodbXLockSpinRounds                  metricNewrelicmysqlInnodbXLockSpinRounds
	metricNewrelicmysqlInnodbXLockSpinWaits                   metricNewrelicmysqlInnodbXLockSpinWaits
	metricNewrelicmysqlMyisamKeyBufferBytesUnflushed          metricNewrelicmysqlMyisamKeyBufferBytesUnflushed
	metricNewrelicmysqlMyisamKeyBufferBytesUsed               metricNewrelicmysqlMyisamKeyBufferBytesUsed
	metricNewrelicmysqlMyisamKeyBufferSize                    metricNewrelicmysqlMyisamKeyBufferSize
	metricNewrelicmysqlMyisamKeyReadRequests                  metricNewrelicmysqlMyisamKeyReadRequests
	metricNewrelicmysqlMyisamKeyReads                         metricNewrelicmysqlMyisamKeyReads
	metricNewrelicmysqlMyisamKeyWriteRequests                 metricNewrelicmysqlMyisamKeyWriteRequests
	metricNewrelicmysqlMyisamKeyWrites                        metricNewrelicmysqlMyisamKeyWrites
	metricNewrelicmysqlNetAbortedClients                      metricNewrelicmysqlNetAbortedClients
	metricNewrelicmysqlNetAbortedConnects                     metricNewrelicmysqlNetAbortedConnects
	metricNewrelicmysqlNetConnections                         metricNewrelicmysqlNetConnections
	metricNewrelicmysqlNetMaxConnections                      metricNewrelicmysqlNetMaxConnections
	metricNewrelicmysqlNetMaxConnectionsAvailable             metricNewrelicmysqlNetMaxConnectionsAvailable
	metricNewrelicmysqlNetMaxUsedConnections                  metricNewrelicmysqlNetMaxUsedConnections
	metricNewrelicmysqlPerformanceBytesReceived               metricNewrelicmysqlPerformanceBytesReceived
	metricNewrelicmysqlPerformanceBytesSent                   metricNewrelicmysqlPerformanceBytesSent
	metricNewrelicmysqlPerformanceCreatedTmpDiskTables        metricNewrelicmysqlPerformanceCreatedTmpDiskTables
	metricNewrelicmysqlPerformanceCreatedTmpFiles             metricNewrelicmysqlPerformanceCreatedTmpFiles
	metricNewrelicmysqlPerformanceCreatedTmpTables            metricNewrelicmysqlPerformanceCreatedTmpTables
	metricNewrelicmysqlPerformanceHandlerCommit               metricNewrelicmysqlPerformanceHandlerCommit
	metricNewrelicmysqlPerformanceHandlerDelete               metricNewrelicmysqlPerformanceHandlerDelete
	metricNewrelicmysqlPerformanceHandlerPrepare              metricNewrelicmysqlPerformanceHandlerPrepare
	metricNewrelicmysqlPerformanceHandlerReadFirst            metricNewrelicmysqlPerformanceHandlerReadFirst
	metricNewrelicmysqlPerformanceHandlerReadKey              metricNewrelicmysqlPerformanceHandlerReadKey
	metricNewrelicmysqlPerformanceHandlerReadNext             metricNewrelicmysqlPerformanceHandlerReadNext
	metricNewrelicmysqlPerformanceHandlerReadPrev             metricNewrelicmysqlPerformanceHandlerReadPrev
	metricNewrelicmysqlPerformanceHandlerReadRnd              metricNewrelicmysqlPerformanceHandlerReadRnd
	metricNewrelicmysqlPerformanceHandlerReadRndNext          metricNewrelicmysqlPerformanceHandlerReadRndNext
	metricNewrelicmysqlPerformanceHandlerRollback             metricNewrelicmysqlPerformanceHandlerRollback
	metricNewrelicmysqlPerformanceHandlerUpdate               metricNewrelicmysqlPerformanceHandlerUpdate
	metricNewrelicmysqlPerformanceHandlerWrite                metricNewrelicmysqlPerformanceHandlerWrite
	metricNewrelicmysqlPerformanceKeyCacheUtilization         metricNewrelicmysqlPerformanceKeyCacheUtilization
	metricNewrelicmysqlPerformanceMaxPreparedStmtCount        metricNewrelicmysqlPerformanceMaxPreparedStmtCount
	metricNewrelicmysqlPerformanceOpenFiles                   metricNewrelicmysqlPerformanceOpenFiles
	metricNewrelicmysqlPerformanceOpenTables                  metricNewrelicmysqlPerformanceOpenTables
	metricNewrelicmysqlPerformanceOpenedTables                metricNewrelicmysqlPerformanceOpenedTables
	metricNewrelicmysqlPerformancePerformanceSchemaDigestLost metricNewrelicmysqlPerformancePerformanceSchemaDigestLost
	metricNewrelicmysqlPerformancePreparedStmtCount           metricNewrelicmysqlPerformancePreparedStmtCount
	metricNewrelicmysqlPerformanceQcacheFreeBlocks            metricNewrelicmysqlPerformanceQcacheFreeBlocks
	metricNewrelicmysqlPerformanceQcacheFreeMemory            metricNewrelicmysqlPerformanceQcacheFreeMemory
	metricNewrelicmysqlPerformanceQcacheHits                  metricNewrelicmysqlPerformanceQcacheHits
	metricNewrelicmysqlPerformanceQcacheInserts               metricNewrelicmysqlPerformanceQcacheInserts
	metricNewrelicmysqlPerformanceQcacheLowmemPrunes          metricNewrelicmysqlPerformanceQcacheLowmemPrunes
	metricNewrelicmysqlPerformanceQcacheNotCached             metricNewrelicmysqlPerformanceQcacheNotCached
	metricNewrelicmysqlPerformanceQcacheQueriesInCache        metricNewrelicmysqlPerformanceQcacheQueriesInCache
	metricNewrelicmysqlPerformanceQcacheSize                  metricNewrelicmysqlPerformanceQcacheSize
	metricNewrelicmysqlPerformanceQcacheTotalBlocks           metricNewrelicmysqlPerformanceQcacheTotalBlocks
	metricNewrelicmysqlPerformanceQuestions                   metricNewrelicmysqlPerformanceQuestions
	metricNewrelicmysqlPerformanceSelectFullJoin              metricNewrelicmysqlPerformanceSelectFullJoin
	metricNewrelicmysqlPerformanceSelectFullRangeJoin         metricNewrelicmysqlPerformanceSelectFullRangeJoin
	metricNewrelicmysqlPerformanceSelectRange                 metricNewrelicmysqlPerformanceSelectRange
	metricNewrelicmysqlPerformanceSelectRangeCheck            metricNewrelicmysqlPerformanceSelectRangeCheck
	metricNewrelicmysqlPerformanceSelectScan                  metricNewrelicmysqlPerformanceSelectScan
	metricNewrelicmysqlPerformanceSlowQueries                 metricNewrelicmysqlPerformanceSlowQueries
	metricNewrelicmysqlPerformanceSortMergePasses             metricNewrelicmysqlPerformanceSortMergePasses
	metricNewrelicmysqlPerformanceSortRange                   metricNewrelicmysqlPerformanceSortRange
	metricNewrelicmysqlPerformanceSortRows                    metricNewrelicmysqlPerformanceSortRows
	metricNewrelicmysqlPerformanceSortScan                    metricNewrelicmysqlPerformanceSortScan
	metricNewrelicmysqlPerformanceTableLocksImmediate         metricNewrelicmysqlPerformanceTableLocksImmediate
	metricNewrelicmysqlPerformanceTableLocksImmediateRate     metricNewrelicmysqlPerformanceTableLocksImmediateRate
	metricNewrelicmysqlPerformanceTableLocksWaited            metricNewrelicmysqlPerformanceTableLocksWaited
	metricNewrelicmysqlPerformanceTableOpenCache              metricNewrelicmysqlPerformanceTableOpenCache
	metricNewrelicmysqlPerformanceThreadCacheSize             metricNewrelicmysqlPerformanceThreadCacheSize
	metricNewrelicmysqlPerformanceThreadsCached               metricNewrelicmysqlPerformanceThreadsCached
	metricNewrelicmysqlPerformanceThreadsConnected            metricNewrelicmysqlPerformanceThreadsConnected
	metricNewrelicmysqlPerformanceThreadsCreated              metricNewrelicmysqlPerformanceThreadsCreated
	metricNewrelicmysqlPerformanceThreadsRunning              metricNewrelicmysqlPerformanceThreadsRunning
	metricNewrelicmysqlQueryCount                             metricNewrelicmysqlQueryCount
	metricNewrelicmysqlReplicationExecMasterLogPos            metricNewrelicmysqlReplicationExecMasterLogPos
	metricNewrelicmysqlReplicationLastIoErrno                 metricNewrelicmysqlReplicationLastIoErrno
	metricNewrelicmysqlReplicationLastSQLErrno                metricNewrelicmysqlReplicationLastSQLErrno
	metricNewrelicmysqlReplicationReadMasterLogPos            metricNewrelicmysqlReplicationReadMasterLogPos
	metricNewrelicmysqlReplicationRelayLogSpace               metricNewrelicmysqlReplicationRelayLogSpace
	metricNewrelicmysqlReplicationSecondsBehindMaster         metricNewrelicmysqlReplicationSecondsBehindMaster
	metricNewrelicmysqlReplicationSlaveIoRunning              metricNewrelicmysqlReplicationSlaveIoRunning
	metricNewrelicmysqlReplicationSlaveRunning                metricNewrelicmysqlReplicationSlaveRunning
	metricNewrelicmysqlReplicationSlaveSQLRunning             metricNewrelicmysqlReplicationSlaveSQLRunning
	metricNewrelicmysqlUptime                                 metricNewrelicmysqlUptime
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                                    mbc,
		startTime:                                                 pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                             pmetric.NewMetrics(),
		buildInfo:                                                 settings.BuildInfo,
		metricNewrelicmysqlBinlogCacheDiskUse:                     newMetricNewrelicmysqlBinlogCacheDiskUse(mbc.Metrics.NewrelicmysqlBinlogCacheDiskUse),
		metricNewrelicmysqlBinlogCacheUse:                         newMetricNewrelicmysqlBinlogCacheUse(mbc.Metrics.NewrelicmysqlBinlogCacheUse),
		metricNewrelicmysqlCommands:                               newMetricNewrelicmysqlCommands(mbc.Metrics.NewrelicmysqlCommands),
		metricNewrelicmysqlConnectionCount:                        newMetricNewrelicmysqlConnectionCount(mbc.Metrics.NewrelicmysqlConnectionCount),
		metricNewrelicmysqlDbHandlerRollback:                      newMetricNewrelicmysqlDbHandlerRollback(mbc.Metrics.NewrelicmysqlDbHandlerRollback),
		metricNewrelicmysqlDbOpenedTables:                         newMetricNewrelicmysqlDbOpenedTables(mbc.Metrics.NewrelicmysqlDbOpenedTables),
		metricNewrelicmysqlGaleraWsrepCertDepsDistance:            newMetricNewrelicmysqlGaleraWsrepCertDepsDistance(mbc.Metrics.NewrelicmysqlGaleraWsrepCertDepsDistance),
		metricNewrelicmysqlGaleraWsrepClusterSize:                 newMetricNewrelicmysqlGaleraWsrepClusterSize(mbc.Metrics.NewrelicmysqlGaleraWsrepClusterSize),
		metricNewrelicmysqlGaleraWsrepFlowControlPaused:           newMetricNewrelicmysqlGaleraWsrepFlowControlPaused(mbc.Metrics.NewrelicmysqlGaleraWsrepFlowControlPaused),
		metricNewrelicmysqlGaleraWsrepFlowControlPausedNs:         newMetricNewrelicmysqlGaleraWsrepFlowControlPausedNs(mbc.Metrics.NewrelicmysqlGaleraWsrepFlowControlPausedNs),
		metricNewrelicmysqlGaleraWsrepFlowControlRecv:             newMetricNewrelicmysqlGaleraWsrepFlowControlRecv(mbc.Metrics.NewrelicmysqlGaleraWsrepFlowControlRecv),
		metricNewrelicmysqlGaleraWsrepFlowControlSent:             newMetricNewrelicmysqlGaleraWsrepFlowControlSent(mbc.Metrics.NewrelicmysqlGaleraWsrepFlowControlSent),
		metricNewrelicmysqlGaleraWsrepLocalCertFailures:           newMetricNewrelicmysqlGaleraWsrepLocalCertFailures(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalCertFailures),
		metricNewrelicmysqlGaleraWsrepLocalRecvQueue:              newMetricNewrelicmysqlGaleraWsrepLocalRecvQueue(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalRecvQueue),
		metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg:           newMetricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalRecvQueueAvg),
		metricNewrelicmysqlGaleraWsrepLocalSendQueue:              newMetricNewrelicmysqlGaleraWsrepLocalSendQueue(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalSendQueue),
		metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg:           newMetricNewrelicmysqlGaleraWsrepLocalSendQueueAvg(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalSendQueueAvg),
		metricNewrelicmysqlGaleraWsrepLocalState:                  newMetricNewrelicmysqlGaleraWsrepLocalState(mbc.Metrics.NewrelicmysqlGaleraWsrepLocalState),
		metricNewrelicmysqlGaleraWsrepReceived:                    newMetricNewrelicmysqlGaleraWsrepReceived(mbc.Metrics.NewrelicmysqlGaleraWsrepReceived),
		metricNewrelicmysqlGaleraWsrepReceivedBytes:               newMetricNewrelicmysqlGaleraWsrepReceivedBytes(mbc.Metrics.NewrelicmysqlGaleraWsrepReceivedBytes),
		metricNewrelicmysqlGaleraWsrepReplicatedBytes:             newMetricNewrelicmysqlGaleraWsrepReplicatedBytes(mbc.Metrics.NewrelicmysqlGaleraWsrepReplicatedBytes),
		metricNewrelicmysqlInnodbActiveTransactions:               newMetricNewrelicmysqlInnodbActiveTransactions(mbc.Metrics.NewrelicmysqlInnodbActiveTransactions),
		metricNewrelicmysqlInnodbAdaptiveHashHashSearches:         newMetricNewrelicmysqlInnodbAdaptiveHashHashSearches(mbc.Metrics.NewrelicmysqlInnodbAdaptiveHashHashSearches),
		metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches:      newMetricNewrelicmysqlInnodbAdaptiveHashNonHashSearches(mbc.Metrics.NewrelicmysqlInnodbAdaptiveHashNonHashSearches),
		metricNewrelicmysqlInnodbAdaptiveHashPagesAdded:           newMetricNewrelicmysqlInnodbAdaptiveHashPagesAdded(mbc.Metrics.NewrelicmysqlInnodbAdaptiveHashPagesAdded),
		metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved:         newMetricNewrelicmysqlInnodbAdaptiveHashPagesRemoved(mbc.Metrics.NewrelicmysqlInnodbAdaptiveHashPagesRemoved),
		metricNewrelicmysqlInnodbAvailableUndoLogs:                newMetricNewrelicmysqlInnodbAvailableUndoLogs(mbc.Metrics.NewrelicmysqlInnodbAvailableUndoLogs),
		metricNewrelicmysqlInnodbBufferPoolBytesData:              newMetricNewrelicmysqlInnodbBufferPoolBytesData(mbc.Metrics.NewrelicmysqlInnodbBufferPoolBytesData),
		metricNewrelicmysqlInnodbBufferPoolBytesDirty:             newMetricNewrelicmysqlInnodbBufferPoolBytesDirty(mbc.Metrics.NewrelicmysqlInnodbBufferPoolBytesDirty),
		metricNewrelicmysqlInnodbBufferPoolDirty:                  newMetricNewrelicmysqlInnodbBufferPoolDirty(mbc.Metrics.NewrelicmysqlInnodbBufferPoolDirty),
		metricNewrelicmysqlInnodbBufferPoolFree:                   newMetricNewrelicmysqlInnodbBufferPoolFree(mbc.Metrics.NewrelicmysqlInnodbBufferPoolFree),
		metricNewrelicmysqlInnodbBufferPoolPagesData:              newMetricNewrelicmysqlInnodbBufferPoolPagesData(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesData),
		metricNewrelicmysqlInnodbBufferPoolPagesFlushed:           newMetricNewrelicmysqlInnodbBufferPoolPagesFlushed(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesFlushed),
		metricNewrelicmysqlInnodbBufferPoolPagesFree:              newMetricNewrelicmysqlInnodbBufferPoolPagesFree(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesFree),
		metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed:        newMetricNewrelicmysqlInnodbBufferPoolPagesLruFlushed(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesLruFlushed),
		metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung:      newMetricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesMadeNotYoung),
		metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung:         newMetricNewrelicmysqlInnodbBufferPoolPagesMadeYoung(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesMadeYoung),
		metricNewrelicmysqlInnodbBufferPoolPagesMisc:              newMetricNewrelicmysqlInnodbBufferPoolPagesMisc(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesMisc),
		metricNewrelicmysqlInnodbBufferPoolPagesOld:               newMetricNewrelicmysqlInnodbBufferPoolPagesOld(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesOld),
		metricNewrelicmysqlInnodbBufferPoolPagesTotal:             newMetricNewrelicmysqlInnodbBufferPoolPagesTotal(mbc.Metrics.NewrelicmysqlInnodbBufferPoolPagesTotal),
		metricNewrelicmysqlInnodbBufferPoolReadAhead:              newMetricNewrelicmysqlInnodbBufferPoolReadAhead(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReadAhead),
		metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted:       newMetricNewrelicmysqlInnodbBufferPoolReadAheadEvicted(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReadAheadEvicted),
		metricNewrelicmysqlInnodbBufferPoolReadAheadRnd:           newMetricNewrelicmysqlInnodbBufferPoolReadAheadRnd(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReadAheadRnd),
		metricNewrelicmysqlInnodbBufferPoolReadRequests:           newMetricNewrelicmysqlInnodbBufferPoolReadRequests(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReadRequests),
		metricNewrelicmysqlInnodbBufferPoolReads:                  newMetricNewrelicmysqlInnodbBufferPoolReads(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReads),
		metricNewrelicmysqlInnodbBufferPoolTotal:                  newMetricNewrelicmysqlInnodbBufferPoolTotal(mbc.Metrics.NewrelicmysqlInnodbBufferPoolTotal),
		metricNewrelicmysqlInnodbBufferPoolUsed:                   newMetricNewrelicmysqlInnodbBufferPoolUsed(mbc.Metrics.NewrelicmysqlInnodbBufferPoolUsed),
		metricNewrelicmysqlInnodbBufferPoolUtilization:            newMetricNewrelicmysqlInnodbBufferPoolUtilization(mbc.Metrics.NewrelicmysqlInnodbBufferPoolUtilization),
		metricNewrelicmysqlInnodbBufferPoolWaitFree:               newMetricNewrelicmysqlInnodbBufferPoolWaitFree(mbc.Metrics.NewrelicmysqlInnodbBufferPoolWaitFree),
		metricNewrelicmysqlInnodbBufferPoolWriteRequests:          newMetricNewrelicmysqlInnodbBufferPoolWriteRequests(mbc.Metrics.NewrelicmysqlInnodbBufferPoolWriteRequests),
		metricNewrelicmysqlInnodbCheckpointAge:                    newMetricNewrelicmysqlInnodbCheckpointAge(mbc.Metrics.NewrelicmysqlInnodbCheckpointAge),
		metricNewrelicmysqlInnodbCurrentRowLocks:                  newMetricNewrelicmysqlInnodbCurrentRowLocks(mbc.Metrics.NewrelicmysqlInnodbCurrentRowLocks),
		metricNewrelicmysqlInnodbCurrentTransactions:              newMetricNewrelicmysqlInnodbCurrentTransactions(mbc.Metrics.NewrelicmysqlInnodbCurrentTransactions),
		metricNewrelicmysqlInnodbDataFsyncs:                       newMetricNewrelicmysqlInnodbDataFsyncs(mbc.Metrics.NewrelicmysqlInnodbDataFsyncs),
		metricNewrelicmysqlInnodbDataPendingFsyncs:                newMetricNewrelicmysqlInnodbDataPendingFsyncs(mbc.Metrics.NewrelicmysqlInnodbDataPendingFsyncs),
		metricNewrelicmysqlInnodbDataPendingReads:                 newMetricNewrelicmysqlInnodbDataPendingReads(mbc.Metrics.NewrelicmysqlInnodbDataPendingReads),
		metricNewrelicmysqlInnodbDataPendingWrites:                newMetricNewrelicmysqlInnodbDataPendingWrites(mbc.Metrics.NewrelicmysqlInnodbDataPendingWrites),
		metricNewrelicmysqlInnodbDataRead:                         newMetricNewrelicmysqlInnodbDataRead(mbc.Metrics.NewrelicmysqlInnodbDataRead),
		metricNewrelicmysqlInnodbDataReads:                        newMetricNewrelicmysqlInnodbDataReads(mbc.Metrics.NewrelicmysqlInnodbDataReads),
		metricNewrelicmysqlInnodbDataWrites:                       newMetricNewrelicmysqlInnodbDataWrites(mbc.Metrics.NewrelicmysqlInnodbDataWrites),
		metricNewrelicmysqlInnodbDataWritten:                      newMetricNewrelicmysqlInnodbDataWritten(mbc.Metrics.NewrelicmysqlInnodbDataWritten),
		metricNewrelicmysqlInnodbDblwrPagesWritten:                newMetricNewrelicmysqlInnodbDblwrPagesWritten(mbc.Metrics.NewrelicmysqlInnodbDblwrPagesWritten),
		metricNewrelicmysqlInnodbDblwrWrites:                      newMetricNewrelicmysqlInnodbDblwrWrites(mbc.Metrics.NewrelicmysqlInnodbDblwrWrites),
		metricNewrelicmysqlInnodbHashIndexCellsTotal:              newMetricNewrelicmysqlInnodbHashIndexCellsTotal(mbc.Metrics.NewrelicmysqlInnodbHashIndexCellsTotal),
		metricNewrelicmysqlInnodbHashIndexCellsUsed:               newMetricNewrelicmysqlInnodbHashIndexCellsUsed(mbc.Metrics.NewrelicmysqlInnodbHashIndexCellsUsed),
		metricNewrelicmysqlInnodbHistoryListLength:                newMetricNewrelicmysqlInnodbHistoryListLength(mbc.Metrics.NewrelicmysqlInnodbHistoryListLength),
		metricNewrelicmysqlInnodbIbufFreeList:                     newMetricNewrelicmysqlInnodbIbufFreeList(mbc.Metrics.NewrelicmysqlInnodbIbufFreeList),
		metricNewrelicmysqlInnodbIbufMergedDeleteMarks:            newMetricNewrelicmysqlInnodbIbufMergedDeleteMarks(mbc.Metrics.NewrelicmysqlInnodbIbufMergedDeleteMarks),
		metricNewrelicmysqlInnodbIbufMergedDeletes:                newMetricNewrelicmysqlInnodbIbufMergedDeletes(mbc.Metrics.NewrelicmysqlInnodbIbufMergedDeletes),
		metricNewrelicmysqlInnodbIbufMergedInserts:                newMetricNewrelicmysqlInnodbIbufMergedInserts(mbc.Metrics.NewrelicmysqlInnodbIbufMergedInserts),
		metricNewrelicmysqlInnodbIbufMerges:                       newMetricNewrelicmysqlInnodbIbufMerges(mbc.Metrics.NewrelicmysqlInnodbIbufMerges),
		metricNewrelicmysqlInnodbIbufSegmentSize:                  newMetricNewrelicmysqlInnodbIbufSegmentSize(mbc.Metrics.NewrelicmysqlInnodbIbufSegmentSize),
		metricNewrelicmysqlInnodbIbufSize:                         newMetricNewrelicmysqlInnodbIbufSize(mbc.Metrics.NewrelicmysqlInnodbIbufSize),
		metricNewrelicmysqlInnodbLockStructs:                      newMetricNewrelicmysqlInnodbLockStructs(mbc.Metrics.NewrelicmysqlInnodbLockStructs),
		metricNewrelicmysqlInnodbLockedTables:                     newMetricNewrelicmysqlInnodbLockedTables(mbc.Metrics.NewrelicmysqlInnodbLockedTables),
		metricNewrelicmysqlInnodbLockedTransactions:               newMetricNewrelicmysqlInnodbLockedTransactions(mbc.Metrics.NewrelicmysqlInnodbLockedTransactions),
		metricNewrelicmysqlInnodbLogWaits:                         newMetricNewrelicmysqlInnodbLogWaits(mbc.Metrics.NewrelicmysqlInnodbLogWaits),
		metricNewrelicmysqlInnodbLogWriteRequests:                 newMetricNewrelicmysqlInnodbLogWriteRequests(mbc.Metrics.NewrelicmysqlInnodbLogWriteRequests),
		metricNewrelicmysqlInnodbLogWrites:                        newMetricNewrelicmysqlInnodbLogWrites(mbc.Metrics.NewrelicmysqlInnodbLogWrites),
		metricNewrelicmysqlInnodbLsnCurrent:                       newMetricNewrelicmysqlInnodbLsnCurrent(mbc.Metrics.NewrelicmysqlInnodbLsnCurrent),
		metricNewrelicmysqlInnodbLsnFlushed:                       newMetricNewrelicmysqlInnodbLsnFlushed(mbc.Metrics.NewrelicmysqlInnodbLsnFlushed),
		metricNewrelicmysqlInnodbLsnLastCheckpoint:                newMetricNewrelicmysqlInnodbLsnLastCheckpoint(mbc.Metrics.NewrelicmysqlInnodbLsnLastCheckpoint),
		metricNewrelicmysqlInnodbMasterThreadActiveLoops:          newMetricNewrelicmysqlInnodbMasterThreadActiveLoops(mbc.Metrics.NewrelicmysqlInnodbMasterThreadActiveLoops),
		metricNewrelicmysqlInnodbMasterThreadIdleLoops:            newMetricNewrelicmysqlInnodbMasterThreadIdleLoops(mbc.Metrics.NewrelicmysqlInnodbMasterThreadIdleLoops),
		metricNewrelicmysqlInnodbMemAdaptiveHash:                  newMetricNewrelicmysqlInnodbMemAdaptiveHash(mbc.Metrics.NewrelicmysqlInnodbMemAdaptiveHash),
		metricNewrelicmysqlInnodbMemAdditionalPool:                newMetricNewrelicmysqlInnodbMemAdditionalPool(mbc.Metrics.NewrelicmysqlInnodbMemAdditionalPool),
		metricNewrelicmysqlInnodbMemDictionary:                    newMetricNewrelicmysqlInnodbMemDictionary(mbc.Metrics.NewrelicmysqlInnodbMemDictionary),
		metricNewrelicmysqlInnodbMemFileSystem:                    newMetricNewrelicmysqlInnodbMemFileSystem(mbc.Metrics.NewrelicmysqlInnodbMemFileSystem),
		metricNewrelicmysqlInnodbMemLockSystem:                    newMetricNewrelicmysqlInnodbMemLockSystem(mbc.Metrics.NewrelicmysqlInnodbMemLockSystem),
		metricNewrelicmysqlInnodbMemPageHash:                      newMetricNewrelicmysqlInnodbMemPageHash(mbc.Metrics.NewrelicmysqlInnodbMemPageHash),
		metricNewrelicmysqlInnodbMemRecoverySystem:                newMetricNewrelicmysqlInnodbMemRecoverySystem(mbc.Metrics.NewrelicmysqlInnodbMemRecoverySystem),
		metricNewrelicmysqlInnodbMemThreadHash:                    newMetricNewrelicmysqlInnodbMemThreadHash(mbc.Metrics.NewrelicmysqlInnodbMemThreadHash),
		metricNewrelicmysqlInnodbMemTotal:                         newMetricNewrelicmysqlInnodbMemTotal(mbc.Metrics.NewrelicmysqlInnodbMemTotal),
		metricNewrelicmysqlInnodbMutexOsWaits:                     newMetricNewrelicmysqlInnodbMutexOsWaits(mbc.Metrics.NewrelicmysqlInnodbMutexOsWaits),
		metricNewrelicmysqlInnodbMutexSpinRounds:                  newMetricNewrelicmysqlInnodbMutexSpinRounds(mbc.Metrics.NewrelicmysqlInnodbMutexSpinRounds),
		metricNewrelicmysqlInnodbMutexSpinWaits:                   newMetricNewrelicmysqlInnodbMutexSpinWaits(mbc.Metrics.NewrelicmysqlInnodbMutexSpinWaits),
		metricNewrelicmysqlInnodbNumOpenFiles:                     newMetricNewrelicmysqlInnodbNumOpenFiles(mbc.Metrics.NewrelicmysqlInnodbNumOpenFiles),
		metricNewrelicmysqlInnodbOsFileFsyncs:                     newMetricNewrelicmysqlInnodbOsFileFsyncs(mbc.Metrics.NewrelicmysqlInnodbOsFileFsyncs),
		metricNewrelicmysqlInnodbOsFileReads:                      newMetricNewrelicmysqlInnodbOsFileReads(mbc.Metrics.NewrelicmysqlInnodbOsFileReads),
		metricNewrelicmysqlInnodbOsFileWrites:                     newMetricNewrelicmysqlInnodbOsFileWrites(mbc.Metrics.NewrelicmysqlInnodbOsFileWrites),
		metricNewrelicmysqlInnodbOsLogFsyncs:                      newMetricNewrelicmysqlInnodbOsLogFsyncs(mbc.Metrics.NewrelicmysqlInnodbOsLogFsyncs),
		metricNewrelicmysqlInnodbOsLogPendingFsyncs:               newMetricNewrelicmysqlInnodbOsLogPendingFsyncs(mbc.Metrics.NewrelicmysqlInnodbOsLogPendingFsyncs),
		metricNewrelicmysqlInnodbOsLogPendingWrites:               newMetricNewrelicmysqlInnodbOsLogPendingWrites(mbc.Metrics.NewrelicmysqlInnodbOsLogPendingWrites),
		metricNewrelicmysqlInnodbOsLogWritten:                     newMetricNewrelicmysqlInnodbOsLogWritten(mbc.Metrics.NewrelicmysqlInnodbOsLogWritten),
		metricNewrelicmysqlInnodbPageSize:                         newMetricNewrelicmysqlInnodbPageSize(mbc.Metrics.NewrelicmysqlInnodbPageSize),
		metricNewrelicmysqlInnodbPagesCreated:                     newMetricNewrelicmysqlInnodbPagesCreated(mbc.Metrics.NewrelicmysqlInnodbPagesCreated),
		metricNewrelicmysqlInnodbPagesRead:                        newMetricNewrelicmysqlInnodbPagesRead(mbc.Metrics.NewrelicmysqlInnodbPagesRead),
		metricNewrelicmysqlInnodbPagesWritten:                     newMetricNewrelicmysqlInnodbPagesWritten(mbc.Metrics.NewrelicmysqlInnodbPagesWritten),
		metricNewrelicmysqlInnodbPendingAioLogIos:                 newMetricNewrelicmysqlInnodbPendingAioLogIos(mbc.Metrics.NewrelicmysqlInnodbPendingAioLogIos),
		metricNewrelicmysqlInnodbPendingAioSyncIos:                newMetricNewrelicmysqlInnodbPendingAioSyncIos(mbc.Metrics.NewrelicmysqlInnodbPendingAioSyncIos),
		metricNewrelicmysqlInnodbPendingBufferPoolFlushes:         newMetricNewrelicmysqlInnodbPendingBufferPoolFlushes(mbc.Metrics.NewrelicmysqlInnodbPendingBufferPoolFlushes),
		metricNewrelicmysqlInnodbPendingCheckpointWrites:          newMetricNewrelicmysqlInnodbPendingCheckpointWrites(mbc.Metrics.NewrelicmysqlInnodbPendingCheckpointWrites),
		metricNewrelicmysqlInnodbPendingIbufAioReads:              newMetricNewrelicmysqlInnodbPendingIbufAioReads(mbc.Metrics.NewrelicmysqlInnodbPendingIbufAioReads),
		metricNewrelicmysqlInnodbPendingLogFlushes:                newMetricNewrelicmysqlInnodbPendingLogFlushes(mbc.Metrics.NewrelicmysqlInnodbPendingLogFlushes),
		metricNewrelicmysqlInnodbPendingLogWrites:                 newMetricNewrelicmysqlInnodbPendingLogWrites(mbc.Metrics.NewrelicmysqlInnodbPendingLogWrites),
		metricNewrelicmysqlInnodbPendingNormalAioReads:            newMetricNewrelicmysqlInnodbPendingNormalAioReads(mbc.Metrics.NewrelicmysqlInnodbPendingNormalAioReads),
		metricNewrelicmysqlInnodbPendingNormalAioWrites:           newMetricNewrelicmysqlInnodbPendingNormalAioWrites(mbc.Metrics.NewrelicmysqlInnodbPendingNormalAioWrites),
		metricNewrelicmysqlInnodbPurgeTrxID:                       newMetricNewrelicmysqlInnodbPurgeTrxID(mbc.Metrics.NewrelicmysqlInnodbPurgeTrxID),
		metricNewrelicmysqlInnodbPurgeUndoNo:                      newMetricNewrelicmysqlInnodbPurgeUndoNo(mbc.Metrics.NewrelicmysqlInnodbPurgeUndoNo),
		metricNewrelicmysqlInnodbQueriesInside:                    newMetricNewrelicmysqlInnodbQueriesInside(mbc.Metrics.NewrelicmysqlInnodbQueriesInside),
		metricNewrelicmysqlInnodbQueriesQueued:                    newMetricNewrelicmysqlInnodbQueriesQueued(mbc.Metrics.NewrelicmysqlInnodbQueriesQueued),
		metricNewrelicmysqlInnodbReadViews:                        newMetricNewrelicmysqlInnodbReadViews(mbc.Metrics.NewrelicmysqlInnodbReadViews),
		metricNewrelicmysqlInnodbRedoLogEnabled:                   newMetricNewrelicmysqlInnodbRedoLogEnabled(mbc.Metrics.NewrelicmysqlInnodbRedoLogEnabled),
		metricNewrelicmysqlInnodbRowLockCurrentWaits:              newMetricNewrelicmysqlInnodbRowLockCurrentWaits(mbc.Metrics.NewrelicmysqlInnodbRowLockCurrentWaits),
		metricNewrelicmysqlInnodbRowLockTime:                      newMetricNewrelicmysqlInnodbRowLockTime(mbc.Metrics.NewrelicmysqlInnodbRowLockTime),
		metricNewrelicmysqlInnodbRowLockTimeAvg:                   newMetricNewrelicmysqlInnodbRowLockTimeAvg(mbc.Metrics.NewrelicmysqlInnodbRowLockTimeAvg),
		metricNewrelicmysqlInnodbRowLockTimeMax:                   newMetricNewrelicmysqlInnodbRowLockTimeMax(mbc.Metrics.NewrelicmysqlInnodbRowLockTimeMax),
		metricNewrelicmysqlInnodbRowLockWaits:                     newMetricNewrelicmysqlInnodbRowLockWaits(mbc.Metrics.NewrelicmysqlInnodbRowLockWaits),
		metricNewrelicmysqlInnodbRowsDeleted:                      newMetricNewrelicmysqlInnodbRowsDeleted(mbc.Metrics.NewrelicmysqlInnodbRowsDeleted),
		metricNewrelicmysqlInnodbRowsInserted:                     newMetricNewrelicmysqlInnodbRowsInserted(mbc.Metrics.NewrelicmysqlInnodbRowsInserted),
		metricNewrelicmysqlInnodbRowsRead:                         newMetricNewrelicmysqlInnodbRowsRead(mbc.Metrics.NewrelicmysqlInnodbRowsRead),
		metricNewrelicmysqlInnodbRowsUpdated:                      newMetricNewrelicmysqlInnodbRowsUpdated(mbc.Metrics.NewrelicmysqlInnodbRowsUpdated),
		metricNewrelicmysqlInnodbSLockOsWaits:                     newMetricNewrelicmysqlInnodbSLockOsWaits(mbc.Metrics.NewrelicmysqlInnodbSLockOsWaits),
		metricNewrelicmysqlInnodbSLockSpinRounds:                  newMetricNewrelicmysqlInnodbSLockSpinRounds(mbc.Metrics.NewrelicmysqlInnodbSLockSpinRounds),
		metricNewrelicmysqlInnodbSLockSpinWaits:                   newMetricNewrelicmysqlInnodbSLockSpinWaits(mbc.Metrics.NewrelicmysqlInnodbSLockSpinWaits),
		metricNewrelicmysqlInnodbSemaphoreWaitTime:                newMetricNewrelicmysqlInnodbSemaphoreWaitTime(mbc.Metrics.NewrelicmysqlInnodbSemaphoreWaitTime),
		metricNewrelicmysqlInnodbSemaphoreWaits:                   newMetricNewrelicmysqlInnodbSemaphoreWaits(mbc.Metrics.NewrelicmysqlInnodbSemaphoreWaits),
		metricNewrelicmysqlInnodbTablesInUse:                      newMetricNewrelicmysqlInnodbTablesInUse(mbc.Metrics.NewrelicmysqlInnodbTablesInUse),
		metricNewrelicmysqlInnodbTruncatedStatusWrites:            newMetricNewrelicmysqlInnodbTruncatedStatusWrites(mbc.Metrics.NewrelicmysqlInnodbTruncatedStatusWrites),
		metricNewrelicmysqlInnodbUndoTablespacesActive:            newMetricNewrelicmysqlInnodbUndoTablespacesActive(mbc.Metrics.NewrelicmysqlInnodbUndoTablespacesActive),
		metricNewrelicmysqlInnodbUndoTablespacesExplicit:          newMetricNewrelicmysqlInnodbUndoTablespacesExplicit(mbc.Metrics.NewrelicmysqlInnodbUndoTablespacesExplicit),
		metricNewrelicmysqlInnodbUndoTablespacesImplicit:          newMetricNewrelicmysqlInnodbUndoTablespacesImplicit(mbc.Metrics.NewrelicmysqlInnodbUndoTablespacesImplicit),
		metricNewrelicmysqlInnodbUndoTablespacesTotal:             newMetricNewrelicmysqlInnodbUndoTablespacesTotal(mbc.Metrics.NewrelicmysqlInnodbUndoTablespacesTotal),
		metricNewrelicmysqlInnodbXLockOsWaits:                     newMetricNewrelicmysqlInnodbXLockOsWaits(mbc.Metrics.NewrelicmysqlInnodbXLockOsWaits),
		metricNewrelicmysqlInnodbXLockSpinRounds:                  newMetricNewrelicmysqlInnodbXLockSpinRounds(mbc.Metrics.NewrelicmysqlInnodbXLockSpinRounds),
		metricNewrelicmysqlInnodbXLockSpinWaits:                   newMetricNewrelicmysqlInnodbXLockSpinWaits(mbc.Metrics.NewrelicmysqlInnodbXLockSpinWaits),
		metricNewrelicmysqlMyisamKeyBufferBytesUnflushed:          newMetricNewrelicmysqlMyisamKeyBufferBytesUnflushed(mbc.Metrics.NewrelicmysqlMyisamKeyBufferBytesUnflushed),
		metricNewrelicmysqlMyisamKeyBufferBytesUsed:               newMetricNewrelicmysqlMyisamKeyBufferBytesUsed(mbc.Metrics.NewrelicmysqlMyisamKeyBufferBytesUsed),
		metricNewrelicmysqlMyisamKeyBufferSize:                    newMetricNewrelicmysqlMyisamKeyBufferSize(mbc.Metrics.NewrelicmysqlMyisamKeyBufferSize),
		metricNewrelicmysqlMyisamKeyReadRequests:                  newMetricNewrelicmysqlMyisamKeyReadRequests(mbc.Metrics.NewrelicmysqlMyisamKeyReadRequests),
		metricNewrelicmysqlMyisamKeyReads:                         newMetricNewrelicmysqlMyisamKeyReads(mbc.Metrics.NewrelicmysqlMyisamKeyReads),
		metricNewrelicmysqlMyisamKeyWriteRequests:                 newMetricNewrelicmysqlMyisamKeyWriteRequests(mbc.Metrics.NewrelicmysqlMyisamKeyWriteRequests),
		metricNewrelicmysqlMyisamKeyWrites:                        newMetricNewrelicmysqlMyisamKeyWrites(mbc.Metrics.NewrelicmysqlMyisamKeyWrites),
		metricNewrelicmysqlNetAbortedClients:                      newMetricNewrelicmysqlNetAbortedClients(mbc.Metrics.NewrelicmysqlNetAbortedClients),
		metricNewrelicmysqlNetAbortedConnects:                     newMetricNewrelicmysqlNetAbortedConnects(mbc.Metrics.NewrelicmysqlNetAbortedConnects),
		metricNewrelicmysqlNetConnections:                         newMetricNewrelicmysqlNetConnections(mbc.Metrics.NewrelicmysqlNetConnections),
		metricNewrelicmysqlNetMaxConnections:                      newMetricNewrelicmysqlNetMaxConnections(mbc.Metrics.NewrelicmysqlNetMaxConnections),
		metricNewrelicmysqlNetMaxConnectionsAvailable:             newMetricNewrelicmysqlNetMaxConnectionsAvailable(mbc.Metrics.NewrelicmysqlNetMaxConnectionsAvailable),
		metricNewrelicmysqlNetMaxUsedConnections:                  newMetricNewrelicmysqlNetMaxUsedConnections(mbc.Metrics.NewrelicmysqlNetMaxUsedConnections),
		metricNewrelicmysqlPerformanceBytesReceived:               newMetricNewrelicmysqlPerformanceBytesReceived(mbc.Metrics.NewrelicmysqlPerformanceBytesReceived),
		metricNewrelicmysqlPerformanceBytesSent:                   newMetricNewrelicmysqlPerformanceBytesSent(mbc.Metrics.NewrelicmysqlPerformanceBytesSent),
		metricNewrelicmysqlPerformanceCreatedTmpDiskTables:        newMetricNewrelicmysqlPerformanceCreatedTmpDiskTables(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpDiskTables),
		metricNewrelicmysqlPerformanceCreatedTmpFiles:             newMetricNewrelicmysqlPerformanceCreatedTmpFiles(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpFiles),
		metricNewrelicmysqlPerformanceCreatedTmpTables:            newMetricNewrelicmysqlPerformanceCreatedTmpTables(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpTables),
		metricNewrelicmysqlPerformanceHandlerCommit:               newMetricNewrelicmysqlPerformanceHandlerCommit(mbc.Metrics.NewrelicmysqlPerformanceHandlerCommit),
		metricNewrelicmysqlPerformanceHandlerDelete:               newMetricNewrelicmysqlPerformanceHandlerDelete(mbc.Metrics.NewrelicmysqlPerformanceHandlerDelete),
		metricNewrelicmysqlPerformanceHandlerPrepare:              newMetricNewrelicmysqlPerformanceHandlerPrepare(mbc.Metrics.NewrelicmysqlPerformanceHandlerPrepare),
		metricNewrelicmysqlPerformanceHandlerReadFirst:            newMetricNewrelicmysqlPerformanceHandlerReadFirst(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadFirst),
		metricNewrelicmysqlPerformanceHandlerReadKey:              newMetricNewrelicmysqlPerformanceHandlerReadKey(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadKey),
		metricNewrelicmysqlPerformanceHandlerReadNext:             newMetricNewrelicmysqlPerformanceHandlerReadNext(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadNext),
		metricNewrelicmysqlPerformanceHandlerReadPrev:             newMetricNewrelicmysqlPerformanceHandlerReadPrev(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadPrev),
		metricNewrelicmysqlPerformanceHandlerReadRnd:              newMetricNewrelicmysqlPerformanceHandlerReadRnd(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadRnd),
		metricNewrelicmysqlPerformanceHandlerReadRndNext:          newMetricNewrelicmysqlPerformanceHandlerReadRndNext(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadRndNext),
		metricNewrelicmysqlPerformanceHandlerRollback:             newMetricNewrelicmysqlPerformanceHandlerRollback(mbc.Metrics.NewrelicmysqlPerformanceHandlerRollback),
		metricNewrelicmysqlPerformanceHandlerUpdate:               newMetricNewrelicmysqlPerformanceHandlerUpdate(mbc.Metrics.NewrelicmysqlPerformanceHandlerUpdate),
		metricNewrelicmysqlPerformanceHandlerWrite:                newMetricNewrelicmysqlPerformanceHandlerWrite(mbc.Metrics.NewrelicmysqlPerformanceHandlerWrite),
		metricNewrelicmysqlPerformanceKeyCacheUtilization:         newMetricNewrelicmysqlPerformanceKeyCacheUtilization(mbc.Metrics.NewrelicmysqlPerformanceKeyCacheUtilization),
		metricNewrelicmysqlPerformanceMaxPreparedStmtCount:        newMetricNewrelicmysqlPerformanceMaxPreparedStmtCount(mbc.Metrics.NewrelicmysqlPerformanceMaxPreparedStmtCount),
		metricNewrelicmysqlPerformanceOpenFiles:                   newMetricNewrelicmysqlPerformanceOpenFiles(mbc.Metrics.NewrelicmysqlPerformanceOpenFiles),
		metricNewrelicmysqlPerformanceOpenTables:                  newMetricNewrelicmysqlPerformanceOpenTables(mbc.Metrics.NewrelicmysqlPerformanceOpenTables),
		metricNewrelicmysqlPerformanceOpenedTables:                newMetricNewrelicmysqlPerformanceOpenedTables(mbc.Metrics.NewrelicmysqlPerformanceOpenedTables),
		metricNewrelicmysqlPerformancePerformanceSchemaDigestLost: newMetricNewrelicmysqlPerformancePerformanceSchemaDigestLost(mbc.Metrics.NewrelicmysqlPerformancePerformanceSchemaDigestLost),
		metricNewrelicmysqlPerformancePreparedStmtCount:           newMetricNewrelicmysqlPerformancePreparedStmtCount(mbc.Metrics.NewrelicmysqlPerformancePreparedStmtCount),
		metricNewrelicmysqlPerformanceQcacheFreeBlocks:            newMetricNewrelicmysqlPerformanceQcacheFreeBlocks(mbc.Metrics.NewrelicmysqlPerformanceQcacheFreeBlocks),
		metricNewrelicmysqlPerformanceQcacheFreeMemory:            newMetricNewrelicmysqlPerformanceQcacheFreeMemory(mbc.Metrics.NewrelicmysqlPerformanceQcacheFreeMemory),
		metricNewrelicmysqlPerformanceQcacheHits:                  newMetricNewrelicmysqlPerformanceQcacheHits(mbc.Metrics.NewrelicmysqlPerformanceQcacheHits),
		metricNewrelicmysqlPerformanceQcacheInserts:               newMetricNewrelicmysqlPerformanceQcacheInserts(mbc.Metrics.NewrelicmysqlPerformanceQcacheInserts),
		metricNewrelicmysqlPerformanceQcacheLowmemPrunes:          newMetricNewrelicmysqlPerformanceQcacheLowmemPrunes(mbc.Metrics.NewrelicmysqlPerformanceQcacheLowmemPrunes),
		metricNewrelicmysqlPerformanceQcacheNotCached:             newMetricNewrelicmysqlPerformanceQcacheNotCached(mbc.Metrics.NewrelicmysqlPerformanceQcacheNotCached),
		metricNewrelicmysqlPerformanceQcacheQueriesInCache:        newMetricNewrelicmysqlPerformanceQcacheQueriesInCache(mbc.Metrics.NewrelicmysqlPerformanceQcacheQueriesInCache),
		metricNewrelicmysqlPerformanceQcacheSize:                  newMetricNewrelicmysqlPerformanceQcacheSize(mbc.Metrics.NewrelicmysqlPerformanceQcacheSize),
		metricNewrelicmysqlPerformanceQcacheTotalBlocks:           newMetricNewrelicmysqlPerformanceQcacheTotalBlocks(mbc.Metrics.NewrelicmysqlPerformanceQcacheTotalBlocks),
		metricNewrelicmysqlPerformanceQuestions:                   newMetricNewrelicmysqlPerformanceQuestions(mbc.Metrics.NewrelicmysqlPerformanceQuestions),
		metricNewrelicmysqlPerformanceSelectFullJoin:              newMetricNewrelicmysqlPerformanceSelectFullJoin(mbc.Metrics.NewrelicmysqlPerformanceSelectFullJoin),
		metricNewrelicmysqlPerformanceSelectFullRangeJoin:         newMetricNewrelicmysqlPerformanceSelectFullRangeJoin(mbc.Metrics.NewrelicmysqlPerformanceSelectFullRangeJoin),
		metricNewrelicmysqlPerformanceSelectRange:                 newMetricNewrelicmysqlPerformanceSelectRange(mbc.Metrics.NewrelicmysqlPerformanceSelectRange),
		metricNewrelicmysqlPerformanceSelectRangeCheck:            newMetricNewrelicmysqlPerformanceSelectRangeCheck(mbc.Metrics.NewrelicmysqlPerformanceSelectRangeCheck),
		metricNewrelicmysqlPerformanceSelectScan:                  newMetricNewrelicmysqlPerformanceSelectScan(mbc.Metrics.NewrelicmysqlPerformanceSelectScan),
		metricNewrelicmysqlPerformanceSlowQueries:                 newMetricNewrelicmysqlPerformanceSlowQueries(mbc.Metrics.NewrelicmysqlPerformanceSlowQueries),
		metricNewrelicmysqlPerformanceSortMergePasses:             newMetricNewrelicmysqlPerformanceSortMergePasses(mbc.Metrics.NewrelicmysqlPerformanceSortMergePasses),
		metricNewrelicmysqlPerformanceSortRange:                   newMetricNewrelicmysqlPerformanceSortRange(mbc.Metrics.NewrelicmysqlPerformanceSortRange),
		metricNewrelicmysqlPerformanceSortRows:                    newMetricNewrelicmysqlPerformanceSortRows(mbc.Metrics.NewrelicmysqlPerformanceSortRows),
		metricNewrelicmysqlPerformanceSortScan:                    newMetricNewrelicmysqlPerformanceSortScan(mbc.Metrics.NewrelicmysqlPerformanceSortScan),
		metricNewrelicmysqlPerformanceTableLocksImmediate:         newMetricNewrelicmysqlPerformanceTableLocksImmediate(mbc.Metrics.NewrelicmysqlPerformanceTableLocksImmediate),
		metricNewrelicmysqlPerformanceTableLocksImmediateRate:     newMetricNewrelicmysqlPerformanceTableLocksImmediateRate(mbc.Metrics.NewrelicmysqlPerformanceTableLocksImmediateRate),
		metricNewrelicmysqlPerformanceTableLocksWaited:            newMetricNewrelicmysqlPerformanceTableLocksWaited(mbc.Metrics.NewrelicmysqlPerformanceTableLocksWaited),
		metricNewrelicmysqlPerformanceTableOpenCache:              newMetricNewrelicmysqlPerformanceTableOpenCache(mbc.Metrics.NewrelicmysqlPerformanceTableOpenCache),
		metricNewrelicmysqlPerformanceThreadCacheSize:             newMetricNewrelicmysqlPerformanceThreadCacheSize(mbc.Metrics.NewrelicmysqlPerformanceThreadCacheSize),
		metricNewrelicmysqlPerformanceThreadsCached:               newMetricNewrelicmysqlPerformanceThreadsCached(mbc.Metrics.NewrelicmysqlPerformanceThreadsCached),
		metricNewrelicmysqlPerformanceThreadsConnected:            newMetricNewrelicmysqlPerformanceThreadsConnected(mbc.Metrics.NewrelicmysqlPerformanceThreadsConnected),
		metricNewrelicmysqlPerformanceThreadsCreated:              newMetricNewrelicmysqlPerformanceThreadsCreated(mbc.Metrics.NewrelicmysqlPerformanceThreadsCreated),
		metricNewrelicmysqlPerformanceThreadsRunning:              newMetricNewrelicmysqlPerformanceThreadsRunning(mbc.Metrics.NewrelicmysqlPerformanceThreadsRunning),
		metricNewrelicmysqlQueryCount:                             newMetricNewrelicmysqlQueryCount(mbc.Metrics.NewrelicmysqlQueryCount),
		metricNewrelicmysqlReplicationExecMasterLogPos:            newMetricNewrelicmysqlReplicationExecMasterLogPos(mbc.Metrics.NewrelicmysqlReplicationExecMasterLogPos),
		metricNewrelicmysqlReplicationLastIoErrno:                 newMetricNewrelicmysqlReplicationLastIoErrno(mbc.Metrics.NewrelicmysqlReplicationLastIoErrno),
		metricNewrelicmysqlReplicationLastSQLErrno:                newMetricNewrelicmysqlReplicationLastSQLErrno(mbc.Metrics.NewrelicmysqlReplicationLastSQLErrno),
		metricNewrelicmysqlReplicationReadMasterLogPos:            newMetricNewrelicmysqlReplicationReadMasterLogPos(mbc.Metrics.NewrelicmysqlReplicationReadMasterLogPos),
		metricNewrelicmysqlReplicationRelayLogSpace:               newMetricNewrelicmysqlReplicationRelayLogSpace(mbc.Metrics.NewrelicmysqlReplicationRelayLogSpace),
		metricNewrelicmysqlReplicationSecondsBehindMaster:         newMetricNewrelicmysqlReplicationSecondsBehindMaster(mbc.Metrics.NewrelicmysqlReplicationSecondsBehindMaster),
		metricNewrelicmysqlReplicationSlaveIoRunning:              newMetricNewrelicmysqlReplicationSlaveIoRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveIoRunning),
		metricNewrelicmysqlReplicationSlaveRunning:                newMetricNewrelicmysqlReplicationSlaveRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveRunning),
		metricNewrelicmysqlReplicationSlaveSQLRunning:             newMetricNewrelicmysqlReplicationSlaveSQLRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveSQLRunning),
		metricNewrelicmysqlUptime:                                 newMetricNewrelicmysqlUptime(mbc.Metrics.NewrelicmysqlUptime),
		resourceAttributeIncludeFilter:                            make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:                            make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["newrelicmysql.instance.endpoint"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsInclude)
	}
	if mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["newrelicmysql.instance.endpoint"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricNewrelicmysqlBinlogCacheDiskUse.emit(ils.Metrics())
	mb.metricNewrelicmysqlBinlogCacheUse.emit(ils.Metrics())
	mb.metricNewrelicmysqlCommands.emit(ils.Metrics())
	mb.metricNewrelicmysqlConnectionCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlDbHandlerRollback.emit(ils.Metrics())
	mb.metricNewrelicmysqlDbOpenedTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepCertDepsDistance.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepClusterSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepFlowControlPaused.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepFlowControlPausedNs.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepFlowControlRecv.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepFlowControlSent.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalCertFailures.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalRecvQueue.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalSendQueue.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepLocalState.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepReceived.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepReceivedBytes.emit(ils.Metrics())
	mb.metricNewrelicmysqlGaleraWsrepReplicatedBytes.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbActiveTransactions.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbAdaptiveHashHashSearches.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbAdaptiveHashPagesAdded.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbAvailableUndoLogs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolBytesData.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolBytesDirty.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolDirty.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolFree.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesData.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesFlushed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesFree.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMisc.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesOld.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolPagesTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReadAhead.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReadAheadRnd.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReadRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolUsed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolUtilization.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolWaitFree.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolWriteRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbCheckpointAge.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbCurrentRowLocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbCurrentTransactions.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataPendingFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataPendingReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataPendingWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataRead.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataWritten.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDblwrPagesWritten.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDblwrWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbHashIndexCellsTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbHashIndexCellsUsed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbHistoryListLength.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufFreeList.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufMergedDeleteMarks.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufMergedDeletes.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufMergedInserts.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufMerges.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufSegmentSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbIbufSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLockStructs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLockedTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLockedTransactions.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLogWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLogWriteRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLogWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLsnCurrent.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLsnFlushed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLsnLastCheckpoint.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMasterThreadActiveLoops.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMasterThreadIdleLoops.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemAdaptiveHash.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemAdditionalPool.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemDictionary.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemFileSystem.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemLockSystem.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemPageHash.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemRecoverySystem.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemThreadHash.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMemTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexOsWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexSpinRounds.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexSpinWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbNumOpenFiles.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsFileFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsFileReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsFileWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsLogFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsLogPendingFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsLogPendingWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsLogWritten.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPageSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPagesCreated.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPagesRead.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPagesWritten.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingAioLogIos.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingAioSyncIos.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingBufferPoolFlushes.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingCheckpointWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingIbufAioReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingLogFlushes.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingLogWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingNormalAioReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPendingNormalAioWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPurgeTrxID.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbPurgeUndoNo.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbQueriesInside.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbQueriesQueued.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbReadViews.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRedoLogEnabled.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockCurrentWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockTime.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockTimeAvg.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockTimeMax.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowsDeleted.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowsInserted.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowsRead.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowsUpdated.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbSLockOsWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbSLockSpinRounds.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbSLockSpinWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbSemaphoreWaitTime.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbSemaphoreWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbTablesInUse.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbTruncatedStatusWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbUndoTablespacesActive.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbUndoTablespacesExplicit.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbUndoTablespacesImplicit.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbUndoTablespacesTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbXLockOsWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbXLockSpinRounds.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbXLockSpinWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUnflushed.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUsed.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyReadRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyWriteRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetAbortedClients.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetAbortedConnects.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxConnectionsAvailable.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxUsedConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceBytesReceived.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceBytesSent.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpDiskTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpFiles.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerCommit.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerDelete.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerPrepare.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadFirst.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadKey.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadNext.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadPrev.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadRnd.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadRndNext.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerRollback.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerUpdate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerWrite.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceKeyCacheUtilization.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceMaxPreparedStmtCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenFiles.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenedTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformancePerformanceSchemaDigestLost.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformancePreparedStmtCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheFreeBlocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheFreeMemory.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheHits.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheInserts.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheLowmemPrunes.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheNotCached.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheQueriesInCache.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheTotalBlocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQuestions.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectFullJoin.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectFullRangeJoin.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectRange.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectRangeCheck.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectScan.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSlowQueries.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortMergePasses.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortRange.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortRows.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortScan.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksImmediate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksImmediateRate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksWaited.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableOpenCache.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadCacheSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsCached.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsConnected.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsCreated.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlQueryCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationExecMasterLogPos.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationLastIoErrno.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationLastSQLErrno.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationReadMasterLogPos.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationRelayLogSpace.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSecondsBehindMaster.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveIoRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveSQLRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlUptime.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordNewrelicmysqlBinlogCacheDiskUseDataPoint adds a data point to newrelicmysql.binlog.cache_disk_use metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlBinlogCacheDiskUseDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlBinlogCacheDiskUse.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlBinlogCacheUseDataPoint adds a data point to newrelicmysql.binlog.cache_use metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlBinlogCacheUseDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlBinlogCacheUse.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlCommandsDataPoint adds a data point to newrelicmysql.commands metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlCommandsDataPoint(ts pcommon.Timestamp, inputVal string, commandAttributeValue AttributeCommand) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlCommands, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlCommands.recordDataPoint(mb.startTime, ts, val, commandAttributeValue.String())
	return nil
}

// RecordNewrelicmysqlConnectionCountDataPoint adds a data point to newrelicmysql.connection.count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlConnectionCountDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlConnectionCount, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlConnectionCount.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// RecordNewrelicmysqlDbHandlerRollbackDataPoint adds a data point to newrelicmysql.db.handler_rollback metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlDbHandlerRollbackDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlDbHandlerRollback.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlDbOpenedTablesDataPoint adds a data point to newrelicmysql.db.opened_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlDbOpenedTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlDbOpenedTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepCertDepsDistanceDataPoint adds a data point to newrelicmysql.galera.wsrep_cert_deps_distance metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepCertDepsDistanceDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlGaleraWsrepCertDepsDistance.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepClusterSizeDataPoint adds a data point to newrelicmysql.galera.wsrep_cluster_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepClusterSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepClusterSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepFlowControlPausedDataPoint adds a data point to newrelicmysql.galera.wsrep_flow_control_paused metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepFlowControlPausedDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlGaleraWsrepFlowControlPaused.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepFlowControlPausedNsDataPoint adds a data point to newrelicmysql.galera.wsrep_flow_control_paused_ns metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepFlowControlPausedNsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepFlowControlPausedNs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepFlowControlRecvDataPoint adds a data point to newrelicmysql.galera.wsrep_flow_control_recv metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepFlowControlRecvDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepFlowControlRecv.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepFlowControlSentDataPoint adds a data point to newrelicmysql.galera.wsrep_flow_control_sent metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepFlowControlSentDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepFlowControlSent.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalCertFailuresDataPoint adds a data point to newrelicmysql.galera.wsrep_local_cert_failures metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalCertFailuresDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalCertFailures.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalRecvQueueDataPoint adds a data point to newrelicmysql.galera.wsrep_local_recv_queue metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalRecvQueueDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalRecvQueue.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalRecvQueueAvgDataPoint adds a data point to newrelicmysql.galera.wsrep_local_recv_queue_avg metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalRecvQueueAvgDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalRecvQueueAvg.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalSendQueueDataPoint adds a data point to newrelicmysql.galera.wsrep_local_send_queue metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalSendQueueDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalSendQueue.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalSendQueueAvgDataPoint adds a data point to newrelicmysql.galera.wsrep_local_send_queue_avg metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalSendQueueAvgDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalSendQueueAvg.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepLocalStateDataPoint adds a data point to newrelicmysql.galera.wsrep_local_state metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepLocalStateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepLocalState.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepReceivedDataPoint adds a data point to newrelicmysql.galera.wsrep_received metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepReceivedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepReceived.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepReceivedBytesDataPoint adds a data point to newrelicmysql.galera.wsrep_received_bytes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepReceivedBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepReceivedBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlGaleraWsrepReplicatedBytesDataPoint adds a data point to newrelicmysql.galera.wsrep_replicated_bytes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlGaleraWsrepReplicatedBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlGaleraWsrepReplicatedBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbActiveTransactionsDataPoint adds a data point to newrelicmysql.innodb.active_transactions metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbActiveTransactionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbActiveTransactions.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbAdaptiveHashHashSearchesDataPoint adds a data point to newrelicmysql.innodb.adaptive_hash_hash_searches metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbAdaptiveHashHashSearchesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbAdaptiveHashHashSearches.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbAdaptiveHashNonHashSearchesDataPoint adds a data point to newrelicmysql.innodb.adaptive_hash_non_hash_searches metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbAdaptiveHashNonHashSearchesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbAdaptiveHashNonHashSearches.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbAdaptiveHashPagesAddedDataPoint adds a data point to newrelicmysql.innodb.adaptive_hash_pages_added metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbAdaptiveHashPagesAddedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbAdaptiveHashPagesAdded.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbAdaptiveHashPagesRemovedDataPoint adds a data point to newrelicmysql.innodb.adaptive_hash_pages_removed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbAdaptiveHashPagesRemovedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbAdaptiveHashPagesRemoved.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbAvailableUndoLogsDataPoint adds a data point to newrelicmysql.innodb.available_undo_logs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbAvailableUndoLogsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbAvailableUndoLogs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolBytesDataDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_bytes_data metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolBytesDataDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolBytesData.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolBytesDirtyDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_bytes_dirty metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolBytesDirtyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolBytesDirty.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolDirtyDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_dirty metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolDirtyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolDirty.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolFreeDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_free metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesDataDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_data metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesDataDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesData.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesFlushedDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_flushed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesFlushedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesFlushed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesFreeDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_free metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesLruFlushedDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_lru_flushed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesLruFlushedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesLruFlushed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesMadeNotYoungDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_made_not_young metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesMadeNotYoungDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMadeNotYoung.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesMadeYoungDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_made_young metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesMadeYoungDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMadeYoung.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesMiscDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_misc metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesMiscDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesMisc.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesOldDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_old metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesOldDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesOld.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolPagesTotalDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_pages_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolPagesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolPagesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadAheadDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_read_ahead metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadAheadDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReadAhead.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadAheadEvictedDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_read_ahead_evicted metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadAheadEvictedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReadAheadEvicted.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadAheadRndDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_read_ahead_rnd metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadAheadRndDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReadAheadRnd.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadRequestsDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_read_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReadRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadsDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolTotalDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolUsedDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_used metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolUtilizationDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_utilization metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolUtilizationDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlInnodbBufferPoolUtilization.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolWaitFreeDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_wait_free metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolWaitFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolWaitFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolWriteRequestsDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_write_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolWriteRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolWriteRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbCheckpointAgeDataPoint adds a data point to newrelicmysql.innodb.checkpoint_age metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbCheckpointAgeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbCheckpointAge.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbCurrentRowLocksDataPoint adds a data point to newrelicmysql.innodb.current_row_locks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbCurrentRowLocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbCurrentRowLocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbCurrentTransactionsDataPoint adds a data point to newrelicmysql.innodb.current_transactions metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbCurrentTransactionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbCurrentTransactions.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataFsyncsDataPoint adds a data point to newrelicmysql.innodb.data_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataPendingFsyncsDataPoint adds a data point to newrelicmysql.innodb.data_pending_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataPendingFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataPendingFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataPendingReadsDataPoint adds a data point to newrelicmysql.innodb.data_pending_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataPendingReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataPendingReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataPendingWritesDataPoint adds a data point to newrelicmysql.innodb.data_pending_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataPendingWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataPendingWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataReadDataPoint adds a data point to newrelicmysql.innodb.data_read metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataReadDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataRead.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataReadsDataPoint adds a data point to newrelicmysql.innodb.data_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataWritesDataPoint adds a data point to newrelicmysql.innodb.data_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataWrittenDataPoint adds a data point to newrelicmysql.innodb.data_written metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataWrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataWritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDblwrPagesWrittenDataPoint adds a data point to newrelicmysql.innodb.dblwr_pages_written metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDblwrPagesWrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDblwrPagesWritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDblwrWritesDataPoint adds a data point to newrelicmysql.innodb.dblwr_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDblwrWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDblwrWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbHashIndexCellsTotalDataPoint adds a data point to newrelicmysql.innodb.hash_index_cells_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbHashIndexCellsTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbHashIndexCellsTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbHashIndexCellsUsedDataPoint adds a data point to newrelicmysql.innodb.hash_index_cells_used metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbHashIndexCellsUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbHashIndexCellsUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbHistoryListLengthDataPoint adds a data point to newrelicmysql.innodb.history_list_length metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbHistoryListLengthDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbHistoryListLength.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufFreeListDataPoint adds a data point to newrelicmysql.innodb.ibuf_free_list metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufFreeListDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufFreeList.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufMergedDeleteMarksDataPoint adds a data point to newrelicmysql.innodb.ibuf_merged_delete_marks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufMergedDeleteMarksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufMergedDeleteMarks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufMergedDeletesDataPoint adds a data point to newrelicmysql.innodb.ibuf_merged_deletes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufMergedDeletesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufMergedDeletes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufMergedInsertsDataPoint adds a data point to newrelicmysql.innodb.ibuf_merged_inserts metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufMergedInsertsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufMergedInserts.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufMergesDataPoint adds a data point to newrelicmysql.innodb.ibuf_merges metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufMergesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufMerges.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufSegmentSizeDataPoint adds a data point to newrelicmysql.innodb.ibuf_segment_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufSegmentSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufSegmentSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbIbufSizeDataPoint adds a data point to newrelicmysql.innodb.ibuf_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbIbufSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbIbufSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLockStructsDataPoint adds a data point to newrelicmysql.innodb.lock_structs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLockStructsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLockStructs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLockedTablesDataPoint adds a data point to newrelicmysql.innodb.locked_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLockedTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLockedTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLockedTransactionsDataPoint adds a data point to newrelicmysql.innodb.locked_transactions metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLockedTransactionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLockedTransactions.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLogWaitsDataPoint adds a data point to newrelicmysql.innodb.log_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLogWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLogWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLogWriteRequestsDataPoint adds a data point to newrelicmysql.innodb.log_write_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLogWriteRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLogWriteRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLogWritesDataPoint adds a data point to newrelicmysql.innodb.log_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLogWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLogWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLsnCurrentDataPoint adds a data point to newrelicmysql.innodb.lsn_current metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLsnCurrentDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLsnCurrent.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLsnFlushedDataPoint adds a data point to newrelicmysql.innodb.lsn_flushed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLsnFlushedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLsnFlushed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLsnLastCheckpointDataPoint adds a data point to newrelicmysql.innodb.lsn_last_checkpoint metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLsnLastCheckpointDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLsnLastCheckpoint.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMasterThreadActiveLoopsDataPoint adds a data point to newrelicmysql.innodb.master_thread_active_loops metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMasterThreadActiveLoopsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMasterThreadActiveLoops.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMasterThreadIdleLoopsDataPoint adds a data point to newrelicmysql.innodb.master_thread_idle_loops metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMasterThreadIdleLoopsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMasterThreadIdleLoops.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemAdaptiveHashDataPoint adds a data point to newrelicmysql.innodb.mem_adaptive_hash metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemAdaptiveHashDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemAdaptiveHash.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemAdditionalPoolDataPoint adds a data point to newrelicmysql.innodb.mem_additional_pool metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemAdditionalPoolDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemAdditionalPool.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemDictionaryDataPoint adds a data point to newrelicmysql.innodb.mem_dictionary metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemDictionaryDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemDictionary.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemFileSystemDataPoint adds a data point to newrelicmysql.innodb.mem_file_system metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemFileSystemDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemFileSystem.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemLockSystemDataPoint adds a data point to newrelicmysql.innodb.mem_lock_system metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemLockSystemDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemLockSystem.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemPageHashDataPoint adds a data point to newrelicmysql.innodb.mem_page_hash metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemPageHashDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemPageHash.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemRecoverySystemDataPoint adds a data point to newrelicmysql.innodb.mem_recovery_system metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemRecoverySystemDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemRecoverySystem.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemThreadHashDataPoint adds a data point to newrelicmysql.innodb.mem_thread_hash metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemThreadHashDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemThreadHash.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMemTotalDataPoint adds a data point to newrelicmysql.innodb.mem_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMemTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMemTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexOsWaitsDataPoint adds a data point to newrelicmysql.innodb.mutex_os_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexOsWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexOsWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexSpinRoundsDataPoint adds a data point to newrelicmysql.innodb.mutex_spin_rounds metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexSpinRoundsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexSpinRounds.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexSpinWaitsDataPoint adds a data point to newrelicmysql.innodb.mutex_spin_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexSpinWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexSpinWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbNumOpenFilesDataPoint adds a data point to newrelicmysql.innodb.num_open_files metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbNumOpenFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbNumOpenFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsFileFsyncsDataPoint adds a data point to newrelicmysql.innodb.os_file_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsFileFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsFileFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsFileReadsDataPoint adds a data point to newrelicmysql.innodb.os_file_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsFileReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsFileReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsFileWritesDataPoint adds a data point to newrelicmysql.innodb.os_file_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsFileWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsFileWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsLogFsyncsDataPoint adds a data point to newrelicmysql.innodb.os_log_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsLogFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsLogFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsLogPendingFsyncsDataPoint adds a data point to newrelicmysql.innodb.os_log_pending_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsLogPendingFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsLogPendingFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsLogPendingWritesDataPoint adds a data point to newrelicmysql.innodb.os_log_pending_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsLogPendingWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsLogPendingWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsLogWrittenDataPoint adds a data point to newrelicmysql.innodb.os_log_written metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsLogWrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsLogWritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPageSizeDataPoint adds a data point to newrelicmysql.innodb.page_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPageSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPageSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPagesCreatedDataPoint adds a data point to newrelicmysql.innodb.pages_created metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPagesCreatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPagesCreated.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPagesReadDataPoint adds a data point to newrelicmysql.innodb.pages_read metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPagesReadDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPagesRead.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPagesWrittenDataPoint adds a data point to newrelicmysql.innodb.pages_written metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPagesWrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPagesWritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingAioLogIosDataPoint adds a data point to newrelicmysql.innodb.pending_aio_log_ios metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingAioLogIosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingAioLogIos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingAioSyncIosDataPoint adds a data point to newrelicmysql.innodb.pending_aio_sync_ios metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingAioSyncIosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingAioSyncIos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingBufferPoolFlushesDataPoint adds a data point to newrelicmysql.innodb.pending_buffer_pool_flushes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingBufferPoolFlushesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingBufferPoolFlushes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingCheckpointWritesDataPoint adds a data point to newrelicmysql.innodb.pending_checkpoint_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingCheckpointWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingCheckpointWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingIbufAioReadsDataPoint adds a data point to newrelicmysql.innodb.pending_ibuf_aio_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingIbufAioReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingIbufAioReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingLogFlushesDataPoint adds a data point to newrelicmysql.innodb.pending_log_flushes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingLogFlushesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingLogFlushes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingLogWritesDataPoint adds a data point to newrelicmysql.innodb.pending_log_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingLogWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingLogWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingNormalAioReadsDataPoint adds a data point to newrelicmysql.innodb.pending_normal_aio_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingNormalAioReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingNormalAioReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPendingNormalAioWritesDataPoint adds a data point to newrelicmysql.innodb.pending_normal_aio_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPendingNormalAioWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPendingNormalAioWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPurgeTrxIDDataPoint adds a data point to newrelicmysql.innodb.purge_trx_id metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPurgeTrxIDDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPurgeTrxID.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbPurgeUndoNoDataPoint adds a data point to newrelicmysql.innodb.purge_undo_no metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbPurgeUndoNoDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbPurgeUndoNo.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbQueriesInsideDataPoint adds a data point to newrelicmysql.innodb.queries_inside metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbQueriesInsideDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbQueriesInside.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbQueriesQueuedDataPoint adds a data point to newrelicmysql.innodb.queries_queued metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbQueriesQueuedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbQueriesQueued.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbReadViewsDataPoint adds a data point to newrelicmysql.innodb.read_views metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbReadViewsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbReadViews.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRedoLogEnabledDataPoint adds a data point to newrelicmysql.innodb.redo_log_enabled metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRedoLogEnabledDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRedoLogEnabled.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockCurrentWaitsDataPoint adds a data point to newrelicmysql.innodb.row_lock_current_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockCurrentWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockCurrentWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockTimeDataPoint adds a data point to newrelicmysql.innodb.row_lock_time metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockTimeAvgDataPoint adds a data point to newrelicmysql.innodb.row_lock_time_avg metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockTimeAvgDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockTimeAvg.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockTimeMaxDataPoint adds a data point to newrelicmysql.innodb.row_lock_time_max metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockTimeMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockTimeMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockWaitsDataPoint adds a data point to newrelicmysql.innodb.row_lock_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowsDeletedDataPoint adds a data point to newrelicmysql.innodb.rows_deleted metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowsDeletedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowsDeleted.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowsInsertedDataPoint adds a data point to newrelicmysql.innodb.rows_inserted metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowsInsertedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowsInserted.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowsReadDataPoint adds a data point to newrelicmysql.innodb.rows_read metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowsReadDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowsRead.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowsUpdatedDataPoint adds a data point to newrelicmysql.innodb.rows_updated metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowsUpdatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowsUpdated.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbSLockOsWaitsDataPoint adds a data point to newrelicmysql.innodb.s_lock_os_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbSLockOsWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbSLockOsWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbSLockSpinRoundsDataPoint adds a data point to newrelicmysql.innodb.s_lock_spin_rounds metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbSLockSpinRoundsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbSLockSpinRounds.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbSLockSpinWaitsDataPoint adds a data point to newrelicmysql.innodb.s_lock_spin_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbSLockSpinWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbSLockSpinWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbSemaphoreWaitTimeDataPoint adds a data point to newrelicmysql.innodb.semaphore_wait_time metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbSemaphoreWaitTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbSemaphoreWaitTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbSemaphoreWaitsDataPoint adds a data point to newrelicmysql.innodb.semaphore_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbSemaphoreWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbSemaphoreWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbTablesInUseDataPoint adds a data point to newrelicmysql.innodb.tables_in_use metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbTablesInUseDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbTablesInUse.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbTruncatedStatusWritesDataPoint adds a data point to newrelicmysql.innodb.truncated_status_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbTruncatedStatusWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbTruncatedStatusWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbUndoTablespacesActiveDataPoint adds a data point to newrelicmysql.innodb.undo_tablespaces_active metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbUndoTablespacesActiveDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbUndoTablespacesActive.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbUndoTablespacesExplicitDataPoint adds a data point to newrelicmysql.innodb.undo_tablespaces_explicit metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbUndoTablespacesExplicitDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbUndoTablespacesExplicit.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbUndoTablespacesImplicitDataPoint adds a data point to newrelicmysql.innodb.undo_tablespaces_implicit metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbUndoTablespacesImplicitDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbUndoTablespacesImplicit.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbUndoTablespacesTotalDataPoint adds a data point to newrelicmysql.innodb.undo_tablespaces_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbUndoTablespacesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbUndoTablespacesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbXLockOsWaitsDataPoint adds a data point to newrelicmysql.innodb.x_lock_os_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbXLockOsWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbXLockOsWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbXLockSpinRoundsDataPoint adds a data point to newrelicmysql.innodb.x_lock_spin_rounds metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbXLockSpinRoundsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbXLockSpinRounds.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbXLockSpinWaitsDataPoint adds a data point to newrelicmysql.innodb.x_lock_spin_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbXLockSpinWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbXLockSpinWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferBytesUnflushedDataPoint adds a data point to newrelicmysql.myisam.key_buffer_bytes_unflushed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferBytesUnflushedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUnflushed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferBytesUsedDataPoint adds a data point to newrelicmysql.myisam.key_buffer_bytes_used metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferBytesUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferSizeDataPoint adds a data point to newrelicmysql.myisam.key_buffer_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyReadRequestsDataPoint adds a data point to newrelicmysql.myisam.key_read_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyReadRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyReadRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyReadsDataPoint adds a data point to newrelicmysql.myisam.key_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyWriteRequestsDataPoint adds a data point to newrelicmysql.myisam.key_write_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyWriteRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyWriteRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyWritesDataPoint adds a data point to newrelicmysql.myisam.key_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetAbortedClientsDataPoint adds a data point to newrelicmysql.net.aborted_clients metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetAbortedClientsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetAbortedClients.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetAbortedConnectsDataPoint adds a data point to newrelicmysql.net.aborted_connects metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetAbortedConnectsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetAbortedConnects.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetConnectionsDataPoint adds a data point to newrelicmysql.net.connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxConnectionsDataPoint adds a data point to newrelicmysql.net.max_connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxConnectionsAvailableDataPoint adds a data point to newrelicmysql.net.max_connections_available metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxConnectionsAvailableDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxConnectionsAvailable.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxUsedConnectionsDataPoint adds a data point to newrelicmysql.net.max_used_connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxUsedConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxUsedConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceBytesReceivedDataPoint adds a data point to newrelicmysql.performance.bytes_received metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceBytesReceivedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceBytesReceived.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceBytesSentDataPoint adds a data point to newrelicmysql.performance.bytes_sent metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceBytesSentDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceBytesSent.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpDiskTablesDataPoint adds a data point to newrelicmysql.performance.created_tmp_disk_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpDiskTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpDiskTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpFilesDataPoint adds a data point to newrelicmysql.performance.created_tmp_files metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpTablesDataPoint adds a data point to newrelicmysql.performance.created_tmp_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerCommitDataPoint adds a data point to newrelicmysql.performance.handler_commit metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerCommitDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerCommit.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerDeleteDataPoint adds a data point to newrelicmysql.performance.handler_delete metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerDeleteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerDelete.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerPrepareDataPoint adds a data point to newrelicmysql.performance.handler_prepare metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerPrepareDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerPrepare.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadFirstDataPoint adds a data point to newrelicmysql.performance.handler_read_first metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadFirstDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadFirst.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadKeyDataPoint adds a data point to newrelicmysql.performance.handler_read_key metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadKeyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadKey.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadNextDataPoint adds a data point to newrelicmysql.performance.handler_read_next metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadNextDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadNext.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadPrevDataPoint adds a data point to newrelicmysql.performance.handler_read_prev metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadPrevDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadPrev.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadRndDataPoint adds a data point to newrelicmysql.performance.handler_read_rnd metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadRndDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadRnd.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadRndNextDataPoint adds a data point to newrelicmysql.performance.handler_read_rnd_next metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadRndNextDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadRndNext.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerRollbackDataPoint adds a data point to newrelicmysql.performance.handler_rollback metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerRollbackDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerRollback.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerUpdateDataPoint adds a data point to newrelicmysql.performance.handler_update metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerUpdateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerUpdate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerWriteDataPoint adds a data point to newrelicmysql.performance.handler_write metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerWriteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerWrite.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceKeyCacheUtilizationDataPoint adds a data point to newrelicmysql.performance.key_cache_utilization metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceKeyCacheUtilizationDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlPerformanceKeyCacheUtilization.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceMaxPreparedStmtCountDataPoint adds a data point to newrelicmysql.performance.max_prepared_stmt_count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceMaxPreparedStmtCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceMaxPreparedStmtCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenFilesDataPoint adds a data point to newrelicmysql.performance.open_files metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenTablesDataPoint adds a data point to newrelicmysql.performance.open_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenedTablesDataPoint adds a data point to newrelicmysql.performance.opened_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenedTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenedTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformancePerformanceSchemaDigestLostDataPoint adds a data point to newrelicmysql.performance.performance_schema_digest_lost metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformancePerformanceSchemaDigestLostDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformancePerformanceSchemaDigestLost.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformancePreparedStmtCountDataPoint adds a data point to newrelicmysql.performance.prepared_stmt_count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformancePreparedStmtCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformancePreparedStmtCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheFreeBlocksDataPoint adds a data point to newrelicmysql.performance.qcache_free_blocks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheFreeBlocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheFreeBlocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheFreeMemoryDataPoint adds a data point to newrelicmysql.performance.qcache_free_memory metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheFreeMemoryDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheFreeMemory.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheHitsDataPoint adds a data point to newrelicmysql.performance.qcache_hits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheHitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheHits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheInsertsDataPoint adds a data point to newrelicmysql.performance.qcache_inserts metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheInsertsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheInserts.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheLowmemPrunesDataPoint adds a data point to newrelicmysql.performance.qcache_lowmem_prunes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheLowmemPrunesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheLowmemPrunes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheNotCachedDataPoint adds a data point to newrelicmysql.performance.qcache_not_cached metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheNotCachedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheNotCached.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheQueriesInCacheDataPoint adds a data point to newrelicmysql.performance.qcache_queries_in_cache metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheQueriesInCacheDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheQueriesInCache.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheSizeDataPoint adds a data point to newrelicmysql.performance.qcache_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheTotalBlocksDataPoint adds a data point to newrelicmysql.performance.qcache_total_blocks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheTotalBlocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheTotalBlocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQuestionsDataPoint adds a data point to newrelicmysql.performance.questions metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQuestionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQuestions.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectFullJoinDataPoint adds a data point to newrelicmysql.performance.select_full_join metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectFullJoinDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectFullJoin.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectFullRangeJoinDataPoint adds a data point to newrelicmysql.performance.select_full_range_join metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectFullRangeJoinDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectFullRangeJoin.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectRangeDataPoint adds a data point to newrelicmysql.performance.select_range metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectRangeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectRange.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectRangeCheckDataPoint adds a data point to newrelicmysql.performance.select_range_check metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectRangeCheckDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectRangeCheck.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectScanDataPoint adds a data point to newrelicmysql.performance.select_scan metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectScanDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectScan.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSlowQueriesDataPoint adds a data point to newrelicmysql.performance.slow_queries metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSlowQueriesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSlowQueries.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortMergePassesDataPoint adds a data point to newrelicmysql.performance.sort_merge_passes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortMergePassesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortMergePasses.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortRangeDataPoint adds a data point to newrelicmysql.performance.sort_range metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortRangeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortRange.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortRowsDataPoint adds a data point to newrelicmysql.performance.sort_rows metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortRowsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortRows.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortScanDataPoint adds a data point to newrelicmysql.performance.sort_scan metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortScanDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortScan.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksImmediateDataPoint adds a data point to newrelicmysql.performance.table_locks_immediate metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksImmediateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksImmediate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksImmediateRateDataPoint adds a data point to newrelicmysql.performance.table_locks_immediate.rate metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksImmediateRateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksImmediateRate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksWaitedDataPoint adds a data point to newrelicmysql.performance.table_locks_waited metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksWaitedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksWaited.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableOpenCacheDataPoint adds a data point to newrelicmysql.performance.table_open_cache metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableOpenCacheDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableOpenCache.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadCacheSizeDataPoint adds a data point to newrelicmysql.performance.thread_cache_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadCacheSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadCacheSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsCachedDataPoint adds a data point to newrelicmysql.performance.threads_cached metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsCachedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsCached.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsConnectedDataPoint adds a data point to newrelicmysql.performance.threads_connected metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsConnectedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsConnected.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsCreatedDataPoint adds a data point to newrelicmysql.performance.threads_created metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsCreatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsCreated.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsRunningDataPoint adds a data point to newrelicmysql.performance.threads_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlQueryCountDataPoint adds a data point to newrelicmysql.query.count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlQueryCountDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlQueryCount, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlQueryCount.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// RecordNewrelicmysqlReplicationExecMasterLogPosDataPoint adds a data point to newrelicmysql.replication.exec_master_log_pos metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationExecMasterLogPosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationExecMasterLogPos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationLastIoErrnoDataPoint adds a data point to newrelicmysql.replication.last_io_errno metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationLastIoErrnoDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationLastIoErrno.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationLastSQLErrnoDataPoint adds a data point to newrelicmysql.replication.last_sql_errno metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationLastSQLErrnoDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationLastSQLErrno.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationReadMasterLogPosDataPoint adds a data point to newrelicmysql.replication.read_master_log_pos metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationReadMasterLogPosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationReadMasterLogPos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationRelayLogSpaceDataPoint adds a data point to newrelicmysql.replication.relay_log_space metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationRelayLogSpaceDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationRelayLogSpace.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSecondsBehindMasterDataPoint adds a data point to newrelicmysql.replication.seconds_behind_master metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSecondsBehindMasterDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSecondsBehindMaster.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveIoRunningDataPoint adds a data point to newrelicmysql.replication.slave_io_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveIoRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveIoRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveRunningDataPoint adds a data point to newrelicmysql.replication.slave_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveSQLRunningDataPoint adds a data point to newrelicmysql.replication.slave_sql_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveSQLRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveSQLRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlUptimeDataPoint adds a data point to newrelicmysql.uptime metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlUptimeDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlUptime, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlUptime.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
