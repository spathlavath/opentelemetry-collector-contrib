// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

var MetricsInfo = metricsInfo{
	PgbouncerStatsAvgBytesIn: metricInfo{
		Name: "pgbouncer.stats.avg_bytes_in",
	},
	PgbouncerStatsAvgBytesOut: metricInfo{
		Name: "pgbouncer.stats.avg_bytes_out",
	},
	PgbouncerStatsAvgRequestsPerSecond: metricInfo{
		Name: "pgbouncer.stats.avg_requests_per_second",
	},
	PgbouncerStatsAvgServerAssignmentCount: metricInfo{
		Name: "pgbouncer.stats.avg_server_assignment_count",
	},
	PgbouncerStatsAvgTransactionCount: metricInfo{
		Name: "pgbouncer.stats.avg_transaction_count",
	},
	PgbouncerStatsAvgTransactionDurationMilliseconds: metricInfo{
		Name: "pgbouncer.stats.avg_transaction_duration_milliseconds",
	},
	PgbouncerStatsBytesInPerSecond: metricInfo{
		Name: "pgbouncer.stats.bytes_in_per_second",
	},
	PgbouncerStatsBytesOutPerSecond: metricInfo{
		Name: "pgbouncer.stats.bytes_out_per_second",
	},
	PgbouncerStatsQueriesPerSecond: metricInfo{
		Name: "pgbouncer.stats.queries_per_second",
	},
	PgbouncerStatsRequestsPerSecond: metricInfo{
		Name: "pgbouncer.stats.requests_per_second",
	},
	PgbouncerStatsTotalServerAssignmentCount: metricInfo{
		Name: "pgbouncer.stats.total_server_assignment_count",
	},
	PgbouncerStatsTransactionsPerSecond: metricInfo{
		Name: "pgbouncer.stats.transactions_per_second",
	},
	PostgresqlActiveWaitingQueries: metricInfo{
		Name: "postgresql.active_waiting_queries",
	},
	PostgresqlActivityBackendXidAge: metricInfo{
		Name: "postgresql.activity.backend_xid_age",
	},
	PostgresqlActivityBackendXminAge: metricInfo{
		Name: "postgresql.activity.backend_xmin_age",
	},
	PostgresqlActivityWaitEvent: metricInfo{
		Name: "postgresql.activity.wait_event",
	},
	PostgresqlActivityXactStartAge: metricInfo{
		Name: "postgresql.activity.xact_start_age",
	},
	PostgresqlAnalyzeChildTablesDone: metricInfo{
		Name: "postgresql.analyze.child_tables_done",
	},
	PostgresqlAnalyzeChildTablesTotal: metricInfo{
		Name: "postgresql.analyze.child_tables_total",
	},
	PostgresqlAnalyzeExtStatsComputed: metricInfo{
		Name: "postgresql.analyze.ext_stats_computed",
	},
	PostgresqlAnalyzeExtStatsTotal: metricInfo{
		Name: "postgresql.analyze.ext_stats_total",
	},
	PostgresqlAnalyzeSampleBlksScanned: metricInfo{
		Name: "postgresql.analyze.sample_blks_scanned",
	},
	PostgresqlAnalyzeSampleBlksTotal: metricInfo{
		Name: "postgresql.analyze.sample_blks_total",
	},
	PostgresqlAnalyzed: metricInfo{
		Name: "postgresql.analyzed",
	},
	PostgresqlArchiverArchivedCount: metricInfo{
		Name: "postgresql.archiver.archived_count",
	},
	PostgresqlArchiverFailedCount: metricInfo{
		Name: "postgresql.archiver.failed_count",
	},
	PostgresqlAutoanalyzed: metricInfo{
		Name: "postgresql.autoanalyzed",
	},
	PostgresqlAutovacuumed: metricInfo{
		Name: "postgresql.autovacuumed",
	},
	PostgresqlBeforeXidWraparound: metricInfo{
		Name: "postgresql.before_xid_wraparound",
	},
	PostgresqlBgwriterBuffersAlloc: metricInfo{
		Name: "postgresql.bgwriter.buffers_alloc",
	},
	PostgresqlBgwriterBuffersBackend: metricInfo{
		Name: "postgresql.bgwriter.buffers_backend",
	},
	PostgresqlBgwriterBuffersBackendFsync: metricInfo{
		Name: "postgresql.bgwriter.buffers_backend_fsync",
	},
	PostgresqlBgwriterBuffersCheckpoint: metricInfo{
		Name: "postgresql.bgwriter.buffers_checkpoint",
	},
	PostgresqlBgwriterBuffersClean: metricInfo{
		Name: "postgresql.bgwriter.buffers_clean",
	},
	PostgresqlBgwriterCheckpointsRequested: metricInfo{
		Name: "postgresql.bgwriter.checkpoints_requested",
	},
	PostgresqlBgwriterCheckpointsTimed: metricInfo{
		Name: "postgresql.bgwriter.checkpoints_timed",
	},
	PostgresqlBgwriterMaxwrittenClean: metricInfo{
		Name: "postgresql.bgwriter.maxwritten_clean",
	},
	PostgresqlBgwriterSyncTime: metricInfo{
		Name: "postgresql.bgwriter.sync_time",
	},
	PostgresqlBgwriterWriteTime: metricInfo{
		Name: "postgresql.bgwriter.write_time",
	},
	PostgresqlBlkReadTime: metricInfo{
		Name: "postgresql.blk_read_time",
	},
	PostgresqlBlkWriteTime: metricInfo{
		Name: "postgresql.blk_write_time",
	},
	PostgresqlBufferHit: metricInfo{
		Name: "postgresql.buffer_hit",
	},
	PostgresqlBuffercacheDirtyBuffers: metricInfo{
		Name: "postgresql.buffercache.dirty_buffers",
	},
	PostgresqlBuffercachePinningBackends: metricInfo{
		Name: "postgresql.buffercache.pinning_backends",
	},
	PostgresqlBuffercacheUnusedBuffers: metricInfo{
		Name: "postgresql.buffercache.unused_buffers",
	},
	PostgresqlBuffercacheUsageCount: metricInfo{
		Name: "postgresql.buffercache.usage_count",
	},
	PostgresqlBuffercacheUsedBuffers: metricInfo{
		Name: "postgresql.buffercache.used_buffers",
	},
	PostgresqlChecksumsEnabled: metricInfo{
		Name: "postgresql.checksums.enabled",
	},
	PostgresqlChecksumsFailures: metricInfo{
		Name: "postgresql.checksums.failures",
	},
	PostgresqlClusterVacuumHeapBlksScanned: metricInfo{
		Name: "postgresql.cluster_vacuum.heap_blks_scanned",
	},
	PostgresqlClusterVacuumHeapBlksTotal: metricInfo{
		Name: "postgresql.cluster_vacuum.heap_blks_total",
	},
	PostgresqlClusterVacuumHeapTuplesScanned: metricInfo{
		Name: "postgresql.cluster_vacuum.heap_tuples_scanned",
	},
	PostgresqlClusterVacuumHeapTuplesWritten: metricInfo{
		Name: "postgresql.cluster_vacuum.heap_tuples_written",
	},
	PostgresqlCommits: metricInfo{
		Name: "postgresql.commits",
	},
	PostgresqlConflicts: metricInfo{
		Name: "postgresql.conflicts",
	},
	PostgresqlConflictsBufferpin: metricInfo{
		Name: "postgresql.conflicts.bufferpin",
	},
	PostgresqlConflictsDeadlock: metricInfo{
		Name: "postgresql.conflicts.deadlock",
	},
	PostgresqlConflictsLock: metricInfo{
		Name: "postgresql.conflicts.lock",
	},
	PostgresqlConflictsSnapshot: metricInfo{
		Name: "postgresql.conflicts.snapshot",
	},
	PostgresqlConflictsTablespace: metricInfo{
		Name: "postgresql.conflicts.tablespace",
	},
	PostgresqlConnections: metricInfo{
		Name: "postgresql.connections",
	},
	PostgresqlControlCheckpointDelay: metricInfo{
		Name: "postgresql.control.checkpoint_delay",
	},
	PostgresqlControlCheckpointDelayBytes: metricInfo{
		Name: "postgresql.control.checkpoint_delay_bytes",
	},
	PostgresqlControlRedoDelayBytes: metricInfo{
		Name: "postgresql.control.redo_delay_bytes",
	},
	PostgresqlControlTimelineID: metricInfo{
		Name: "postgresql.control.timeline_id",
	},
	PostgresqlCreateIndexBlocksDone: metricInfo{
		Name: "postgresql.create_index.blocks_done",
	},
	PostgresqlCreateIndexBlocksTotal: metricInfo{
		Name: "postgresql.create_index.blocks_total",
	},
	PostgresqlCreateIndexLockersDone: metricInfo{
		Name: "postgresql.create_index.lockers_done",
	},
	PostgresqlCreateIndexLockersTotal: metricInfo{
		Name: "postgresql.create_index.lockers_total",
	},
	PostgresqlCreateIndexPartitionsDone: metricInfo{
		Name: "postgresql.create_index.partitions_done",
	},
	PostgresqlCreateIndexPartitionsTotal: metricInfo{
		Name: "postgresql.create_index.partitions_total",
	},
	PostgresqlCreateIndexTuplesDone: metricInfo{
		Name: "postgresql.create_index.tuples_done",
	},
	PostgresqlCreateIndexTuplesTotal: metricInfo{
		Name: "postgresql.create_index.tuples_total",
	},
	PostgresqlDatabaseSize: metricInfo{
		Name: "postgresql.database_size",
	},
	PostgresqlDbCount: metricInfo{
		Name: "postgresql.db.count",
	},
	PostgresqlDeadlocks: metricInfo{
		Name: "postgresql.deadlocks",
	},
	PostgresqlDiskRead: metricInfo{
		Name: "postgresql.disk_read",
	},
	PostgresqlHeapBlocksHit: metricInfo{
		Name: "postgresql.heap_blocks_hit",
	},
	PostgresqlHeapBlocksRead: metricInfo{
		Name: "postgresql.heap_blocks_read",
	},
	PostgresqlIndexBlocksHit: metricInfo{
		Name: "postgresql.index_blocks_hit",
	},
	PostgresqlIndexBlocksRead: metricInfo{
		Name: "postgresql.index_blocks_read",
	},
	PostgresqlIndexScans: metricInfo{
		Name: "postgresql.index_scans",
	},
	PostgresqlIndexSize: metricInfo{
		Name: "postgresql.index_size",
	},
	PostgresqlIndexTuplesFetched: metricInfo{
		Name: "postgresql.index_tuples_fetched",
	},
	PostgresqlIndexTuplesRead: metricInfo{
		Name: "postgresql.index_tuples_read",
	},
	PostgresqlLastAnalyzeAge: metricInfo{
		Name: "postgresql.last_analyze_age",
	},
	PostgresqlLastAutoanalyzeAge: metricInfo{
		Name: "postgresql.last_autoanalyze_age",
	},
	PostgresqlLastAutovacuumAge: metricInfo{
		Name: "postgresql.last_autovacuum_age",
	},
	PostgresqlLastVacuumAge: metricInfo{
		Name: "postgresql.last_vacuum_age",
	},
	PostgresqlMaxConnections: metricInfo{
		Name: "postgresql.max_connections",
	},
	PostgresqlPercentUsageConnections: metricInfo{
		Name: "postgresql.percent_usage_connections",
	},
	PostgresqlPgStatStatementsDealloc: metricInfo{
		Name: "postgresql.pg_stat_statements.dealloc",
	},
	PostgresqlRecoveryPrefetchBlockDistance: metricInfo{
		Name: "postgresql.recovery_prefetch.block_distance",
	},
	PostgresqlRecoveryPrefetchHit: metricInfo{
		Name: "postgresql.recovery_prefetch.hit",
	},
	PostgresqlRecoveryPrefetchIoDepth: metricInfo{
		Name: "postgresql.recovery_prefetch.io_depth",
	},
	PostgresqlRecoveryPrefetchPrefetch: metricInfo{
		Name: "postgresql.recovery_prefetch.prefetch",
	},
	PostgresqlRecoveryPrefetchSkipFpw: metricInfo{
		Name: "postgresql.recovery_prefetch.skip_fpw",
	},
	PostgresqlRecoveryPrefetchSkipInit: metricInfo{
		Name: "postgresql.recovery_prefetch.skip_init",
	},
	PostgresqlRecoveryPrefetchSkipNew: metricInfo{
		Name: "postgresql.recovery_prefetch.skip_new",
	},
	PostgresqlRecoveryPrefetchSkipRep: metricInfo{
		Name: "postgresql.recovery_prefetch.skip_rep",
	},
	PostgresqlRecoveryPrefetchWalDistance: metricInfo{
		Name: "postgresql.recovery_prefetch.wal_distance",
	},
	PostgresqlRelationAllVisible: metricInfo{
		Name: "postgresql.relation.all_visible",
	},
	PostgresqlRelationPages: metricInfo{
		Name: "postgresql.relation.pages",
	},
	PostgresqlRelationTuples: metricInfo{
		Name: "postgresql.relation.tuples",
	},
	PostgresqlRelationXmin: metricInfo{
		Name: "postgresql.relation.xmin",
	},
	PostgresqlRelationSize: metricInfo{
		Name: "postgresql.relation_size",
	},
	PostgresqlReplicationBackendXminAge: metricInfo{
		Name: "postgresql.replication.backend_xmin_age",
	},
	PostgresqlReplicationFlushLsnDelay: metricInfo{
		Name: "postgresql.replication.flush_lsn_delay",
	},
	PostgresqlReplicationReplayLsnDelay: metricInfo{
		Name: "postgresql.replication.replay_lsn_delay",
	},
	PostgresqlReplicationSentLsnDelay: metricInfo{
		Name: "postgresql.replication.sent_lsn_delay",
	},
	PostgresqlReplicationWalFlushLag: metricInfo{
		Name: "postgresql.replication.wal_flush_lag",
	},
	PostgresqlReplicationWalReplayLag: metricInfo{
		Name: "postgresql.replication.wal_replay_lag",
	},
	PostgresqlReplicationWalWriteLag: metricInfo{
		Name: "postgresql.replication.wal_write_lag",
	},
	PostgresqlReplicationWriteLsnDelay: metricInfo{
		Name: "postgresql.replication.write_lsn_delay",
	},
	PostgresqlReplicationDelay: metricInfo{
		Name: "postgresql.replication_delay",
	},
	PostgresqlReplicationDelayBytes: metricInfo{
		Name: "postgresql.replication_delay_bytes",
	},
	PostgresqlReplicationSlotCatalogXminAge: metricInfo{
		Name: "postgresql.replication_slot.catalog_xmin_age",
	},
	PostgresqlReplicationSlotConfirmedFlushDelayBytes: metricInfo{
		Name: "postgresql.replication_slot.confirmed_flush_delay_bytes",
	},
	PostgresqlReplicationSlotRestartDelayBytes: metricInfo{
		Name: "postgresql.replication_slot.restart_delay_bytes",
	},
	PostgresqlReplicationSlotSpillBytes: metricInfo{
		Name: "postgresql.replication_slot.spill_bytes",
	},
	PostgresqlReplicationSlotSpillCount: metricInfo{
		Name: "postgresql.replication_slot.spill_count",
	},
	PostgresqlReplicationSlotSpillTxns: metricInfo{
		Name: "postgresql.replication_slot.spill_txns",
	},
	PostgresqlReplicationSlotStreamBytes: metricInfo{
		Name: "postgresql.replication_slot.stream_bytes",
	},
	PostgresqlReplicationSlotStreamCount: metricInfo{
		Name: "postgresql.replication_slot.stream_count",
	},
	PostgresqlReplicationSlotStreamTxns: metricInfo{
		Name: "postgresql.replication_slot.stream_txns",
	},
	PostgresqlReplicationSlotTotalBytes: metricInfo{
		Name: "postgresql.replication_slot.total_bytes",
	},
	PostgresqlReplicationSlotTotalTxns: metricInfo{
		Name: "postgresql.replication_slot.total_txns",
	},
	PostgresqlReplicationSlotXminAge: metricInfo{
		Name: "postgresql.replication_slot.xmin_age",
	},
	PostgresqlRollbacks: metricInfo{
		Name: "postgresql.rollbacks",
	},
	PostgresqlRowsDeleted: metricInfo{
		Name: "postgresql.rows_deleted",
	},
	PostgresqlRowsFetched: metricInfo{
		Name: "postgresql.rows_fetched",
	},
	PostgresqlRowsInserted: metricInfo{
		Name: "postgresql.rows_inserted",
	},
	PostgresqlRowsReturned: metricInfo{
		Name: "postgresql.rows_returned",
	},
	PostgresqlRowsUpdated: metricInfo{
		Name: "postgresql.rows_updated",
	},
	PostgresqlRunning: metricInfo{
		Name: "postgresql.running",
	},
	PostgresqlSessionsAbandoned: metricInfo{
		Name: "postgresql.sessions.abandoned",
	},
	PostgresqlSessionsActiveTime: metricInfo{
		Name: "postgresql.sessions.active_time",
	},
	PostgresqlSessionsCount: metricInfo{
		Name: "postgresql.sessions.count",
	},
	PostgresqlSessionsFatal: metricInfo{
		Name: "postgresql.sessions.fatal",
	},
	PostgresqlSessionsIdleInTransactionTime: metricInfo{
		Name: "postgresql.sessions.idle_in_transaction_time",
	},
	PostgresqlSessionsKilled: metricInfo{
		Name: "postgresql.sessions.killed",
	},
	PostgresqlSessionsSessionTime: metricInfo{
		Name: "postgresql.sessions.session_time",
	},
	PostgresqlSlruBlksExists: metricInfo{
		Name: "postgresql.slru.blks_exists",
	},
	PostgresqlSlruBlksHit: metricInfo{
		Name: "postgresql.slru.blks_hit",
	},
	PostgresqlSlruBlksRead: metricInfo{
		Name: "postgresql.slru.blks_read",
	},
	PostgresqlSlruBlksWritten: metricInfo{
		Name: "postgresql.slru.blks_written",
	},
	PostgresqlSlruBlksZeroed: metricInfo{
		Name: "postgresql.slru.blks_zeroed",
	},
	PostgresqlSlruFlushes: metricInfo{
		Name: "postgresql.slru.flushes",
	},
	PostgresqlSlruTruncates: metricInfo{
		Name: "postgresql.slru.truncates",
	},
	PostgresqlSnapshotXipCount: metricInfo{
		Name: "postgresql.snapshot.xip_count",
	},
	PostgresqlSnapshotXmax: metricInfo{
		Name: "postgresql.snapshot.xmax",
	},
	PostgresqlSnapshotXmin: metricInfo{
		Name: "postgresql.snapshot.xmin",
	},
	PostgresqlSubscriptionApplyError: metricInfo{
		Name: "postgresql.subscription.apply_error",
	},
	PostgresqlSubscriptionLastMsgReceiptAge: metricInfo{
		Name: "postgresql.subscription.last_msg_receipt_age",
	},
	PostgresqlSubscriptionLastMsgSendAge: metricInfo{
		Name: "postgresql.subscription.last_msg_send_age",
	},
	PostgresqlSubscriptionLatestEndAge: metricInfo{
		Name: "postgresql.subscription.latest_end_age",
	},
	PostgresqlSubscriptionSyncError: metricInfo{
		Name: "postgresql.subscription.sync_error",
	},
	PostgresqlTempBytes: metricInfo{
		Name: "postgresql.temp_bytes",
	},
	PostgresqlTempFiles: metricInfo{
		Name: "postgresql.temp_files",
	},
	PostgresqlToastAutovacuumed: metricInfo{
		Name: "postgresql.toast.autovacuumed",
	},
	PostgresqlToastLastAutovacuumAge: metricInfo{
		Name: "postgresql.toast.last_autovacuum_age",
	},
	PostgresqlToastLastVacuumAge: metricInfo{
		Name: "postgresql.toast.last_vacuum_age",
	},
	PostgresqlToastVacuumed: metricInfo{
		Name: "postgresql.toast.vacuumed",
	},
	PostgresqlToastBlocksHit: metricInfo{
		Name: "postgresql.toast_blocks_hit",
	},
	PostgresqlToastBlocksRead: metricInfo{
		Name: "postgresql.toast_blocks_read",
	},
	PostgresqlToastIndexBlocksHit: metricInfo{
		Name: "postgresql.toast_index_blocks_hit",
	},
	PostgresqlToastIndexBlocksRead: metricInfo{
		Name: "postgresql.toast_index_blocks_read",
	},
	PostgresqlToastSize: metricInfo{
		Name: "postgresql.toast_size",
	},
	PostgresqlTransactionsDurationMax: metricInfo{
		Name: "postgresql.transactions.duration.max",
	},
	PostgresqlTransactionsDurationSum: metricInfo{
		Name: "postgresql.transactions.duration.sum",
	},
	PostgresqlUptime: metricInfo{
		Name: "postgresql.uptime",
	},
	PostgresqlVacuumHeapBlksScanned: metricInfo{
		Name: "postgresql.vacuum.heap_blks_scanned",
	},
	PostgresqlVacuumHeapBlksTotal: metricInfo{
		Name: "postgresql.vacuum.heap_blks_total",
	},
	PostgresqlVacuumHeapBlksVacuumed: metricInfo{
		Name: "postgresql.vacuum.heap_blks_vacuumed",
	},
	PostgresqlVacuumIndexVacuumCount: metricInfo{
		Name: "postgresql.vacuum.index_vacuum_count",
	},
	PostgresqlVacuumMaxDeadTuples: metricInfo{
		Name: "postgresql.vacuum.max_dead_tuples",
	},
	PostgresqlVacuumNumDeadTuples: metricInfo{
		Name: "postgresql.vacuum.num_dead_tuples",
	},
	PostgresqlVacuumed: metricInfo{
		Name: "postgresql.vacuumed",
	},
	PostgresqlWalBuffersFull: metricInfo{
		Name: "postgresql.wal.buffers_full",
	},
	PostgresqlWalBytes: metricInfo{
		Name: "postgresql.wal.bytes",
	},
	PostgresqlWalFpi: metricInfo{
		Name: "postgresql.wal.fpi",
	},
	PostgresqlWalRecords: metricInfo{
		Name: "postgresql.wal.records",
	},
	PostgresqlWalSync: metricInfo{
		Name: "postgresql.wal.sync",
	},
	PostgresqlWalSyncTime: metricInfo{
		Name: "postgresql.wal.sync_time",
	},
	PostgresqlWalWrite: metricInfo{
		Name: "postgresql.wal.write",
	},
	PostgresqlWalWriteTime: metricInfo{
		Name: "postgresql.wal.write_time",
	},
	PostgresqlWalFilesAge: metricInfo{
		Name: "postgresql.wal_files.age",
	},
	PostgresqlWalFilesCount: metricInfo{
		Name: "postgresql.wal_files.count",
	},
	PostgresqlWalFilesSize: metricInfo{
		Name: "postgresql.wal_files.size",
	},
	PostgresqlWalReceiverConnected: metricInfo{
		Name: "postgresql.wal_receiver.connected",
	},
	PostgresqlWalReceiverLastMsgReceiptAge: metricInfo{
		Name: "postgresql.wal_receiver.last_msg_receipt_age",
	},
	PostgresqlWalReceiverLastMsgSendAge: metricInfo{
		Name: "postgresql.wal_receiver.last_msg_send_age",
	},
	PostgresqlWalReceiverLatestEndAge: metricInfo{
		Name: "postgresql.wal_receiver.latest_end_age",
	},
	PostgresqlWalReceiverReceivedTimeline: metricInfo{
		Name: "postgresql.wal_receiver.received_timeline",
	},
}

type metricsInfo struct {
	PgbouncerStatsAvgBytesIn                          metricInfo
	PgbouncerStatsAvgBytesOut                         metricInfo
	PgbouncerStatsAvgRequestsPerSecond                metricInfo
	PgbouncerStatsAvgServerAssignmentCount            metricInfo
	PgbouncerStatsAvgTransactionCount                 metricInfo
	PgbouncerStatsAvgTransactionDurationMilliseconds  metricInfo
	PgbouncerStatsBytesInPerSecond                    metricInfo
	PgbouncerStatsBytesOutPerSecond                   metricInfo
	PgbouncerStatsQueriesPerSecond                    metricInfo
	PgbouncerStatsRequestsPerSecond                   metricInfo
	PgbouncerStatsTotalServerAssignmentCount          metricInfo
	PgbouncerStatsTransactionsPerSecond               metricInfo
	PostgresqlActiveWaitingQueries                    metricInfo
	PostgresqlActivityBackendXidAge                   metricInfo
	PostgresqlActivityBackendXminAge                  metricInfo
	PostgresqlActivityWaitEvent                       metricInfo
	PostgresqlActivityXactStartAge                    metricInfo
	PostgresqlAnalyzeChildTablesDone                  metricInfo
	PostgresqlAnalyzeChildTablesTotal                 metricInfo
	PostgresqlAnalyzeExtStatsComputed                 metricInfo
	PostgresqlAnalyzeExtStatsTotal                    metricInfo
	PostgresqlAnalyzeSampleBlksScanned                metricInfo
	PostgresqlAnalyzeSampleBlksTotal                  metricInfo
	PostgresqlAnalyzed                                metricInfo
	PostgresqlArchiverArchivedCount                   metricInfo
	PostgresqlArchiverFailedCount                     metricInfo
	PostgresqlAutoanalyzed                            metricInfo
	PostgresqlAutovacuumed                            metricInfo
	PostgresqlBeforeXidWraparound                     metricInfo
	PostgresqlBgwriterBuffersAlloc                    metricInfo
	PostgresqlBgwriterBuffersBackend                  metricInfo
	PostgresqlBgwriterBuffersBackendFsync             metricInfo
	PostgresqlBgwriterBuffersCheckpoint               metricInfo
	PostgresqlBgwriterBuffersClean                    metricInfo
	PostgresqlBgwriterCheckpointsRequested            metricInfo
	PostgresqlBgwriterCheckpointsTimed                metricInfo
	PostgresqlBgwriterMaxwrittenClean                 metricInfo
	PostgresqlBgwriterSyncTime                        metricInfo
	PostgresqlBgwriterWriteTime                       metricInfo
	PostgresqlBlkReadTime                             metricInfo
	PostgresqlBlkWriteTime                            metricInfo
	PostgresqlBufferHit                               metricInfo
	PostgresqlBuffercacheDirtyBuffers                 metricInfo
	PostgresqlBuffercachePinningBackends              metricInfo
	PostgresqlBuffercacheUnusedBuffers                metricInfo
	PostgresqlBuffercacheUsageCount                   metricInfo
	PostgresqlBuffercacheUsedBuffers                  metricInfo
	PostgresqlChecksumsEnabled                        metricInfo
	PostgresqlChecksumsFailures                       metricInfo
	PostgresqlClusterVacuumHeapBlksScanned            metricInfo
	PostgresqlClusterVacuumHeapBlksTotal              metricInfo
	PostgresqlClusterVacuumHeapTuplesScanned          metricInfo
	PostgresqlClusterVacuumHeapTuplesWritten          metricInfo
	PostgresqlCommits                                 metricInfo
	PostgresqlConflicts                               metricInfo
	PostgresqlConflictsBufferpin                      metricInfo
	PostgresqlConflictsDeadlock                       metricInfo
	PostgresqlConflictsLock                           metricInfo
	PostgresqlConflictsSnapshot                       metricInfo
	PostgresqlConflictsTablespace                     metricInfo
	PostgresqlConnections                             metricInfo
	PostgresqlControlCheckpointDelay                  metricInfo
	PostgresqlControlCheckpointDelayBytes             metricInfo
	PostgresqlControlRedoDelayBytes                   metricInfo
	PostgresqlControlTimelineID                       metricInfo
	PostgresqlCreateIndexBlocksDone                   metricInfo
	PostgresqlCreateIndexBlocksTotal                  metricInfo
	PostgresqlCreateIndexLockersDone                  metricInfo
	PostgresqlCreateIndexLockersTotal                 metricInfo
	PostgresqlCreateIndexPartitionsDone               metricInfo
	PostgresqlCreateIndexPartitionsTotal              metricInfo
	PostgresqlCreateIndexTuplesDone                   metricInfo
	PostgresqlCreateIndexTuplesTotal                  metricInfo
	PostgresqlDatabaseSize                            metricInfo
	PostgresqlDbCount                                 metricInfo
	PostgresqlDeadlocks                               metricInfo
	PostgresqlDiskRead                                metricInfo
	PostgresqlHeapBlocksHit                           metricInfo
	PostgresqlHeapBlocksRead                          metricInfo
	PostgresqlIndexBlocksHit                          metricInfo
	PostgresqlIndexBlocksRead                         metricInfo
	PostgresqlIndexScans                              metricInfo
	PostgresqlIndexSize                               metricInfo
	PostgresqlIndexTuplesFetched                      metricInfo
	PostgresqlIndexTuplesRead                         metricInfo
	PostgresqlLastAnalyzeAge                          metricInfo
	PostgresqlLastAutoanalyzeAge                      metricInfo
	PostgresqlLastAutovacuumAge                       metricInfo
	PostgresqlLastVacuumAge                           metricInfo
	PostgresqlMaxConnections                          metricInfo
	PostgresqlPercentUsageConnections                 metricInfo
	PostgresqlPgStatStatementsDealloc                 metricInfo
	PostgresqlRecoveryPrefetchBlockDistance           metricInfo
	PostgresqlRecoveryPrefetchHit                     metricInfo
	PostgresqlRecoveryPrefetchIoDepth                 metricInfo
	PostgresqlRecoveryPrefetchPrefetch                metricInfo
	PostgresqlRecoveryPrefetchSkipFpw                 metricInfo
	PostgresqlRecoveryPrefetchSkipInit                metricInfo
	PostgresqlRecoveryPrefetchSkipNew                 metricInfo
	PostgresqlRecoveryPrefetchSkipRep                 metricInfo
	PostgresqlRecoveryPrefetchWalDistance             metricInfo
	PostgresqlRelationAllVisible                      metricInfo
	PostgresqlRelationPages                           metricInfo
	PostgresqlRelationTuples                          metricInfo
	PostgresqlRelationXmin                            metricInfo
	PostgresqlRelationSize                            metricInfo
	PostgresqlReplicationBackendXminAge               metricInfo
	PostgresqlReplicationFlushLsnDelay                metricInfo
	PostgresqlReplicationReplayLsnDelay               metricInfo
	PostgresqlReplicationSentLsnDelay                 metricInfo
	PostgresqlReplicationWalFlushLag                  metricInfo
	PostgresqlReplicationWalReplayLag                 metricInfo
	PostgresqlReplicationWalWriteLag                  metricInfo
	PostgresqlReplicationWriteLsnDelay                metricInfo
	PostgresqlReplicationDelay                        metricInfo
	PostgresqlReplicationDelayBytes                   metricInfo
	PostgresqlReplicationSlotCatalogXminAge           metricInfo
	PostgresqlReplicationSlotConfirmedFlushDelayBytes metricInfo
	PostgresqlReplicationSlotRestartDelayBytes        metricInfo
	PostgresqlReplicationSlotSpillBytes               metricInfo
	PostgresqlReplicationSlotSpillCount               metricInfo
	PostgresqlReplicationSlotSpillTxns                metricInfo
	PostgresqlReplicationSlotStreamBytes              metricInfo
	PostgresqlReplicationSlotStreamCount              metricInfo
	PostgresqlReplicationSlotStreamTxns               metricInfo
	PostgresqlReplicationSlotTotalBytes               metricInfo
	PostgresqlReplicationSlotTotalTxns                metricInfo
	PostgresqlReplicationSlotXminAge                  metricInfo
	PostgresqlRollbacks                               metricInfo
	PostgresqlRowsDeleted                             metricInfo
	PostgresqlRowsFetched                             metricInfo
	PostgresqlRowsInserted                            metricInfo
	PostgresqlRowsReturned                            metricInfo
	PostgresqlRowsUpdated                             metricInfo
	PostgresqlRunning                                 metricInfo
	PostgresqlSessionsAbandoned                       metricInfo
	PostgresqlSessionsActiveTime                      metricInfo
	PostgresqlSessionsCount                           metricInfo
	PostgresqlSessionsFatal                           metricInfo
	PostgresqlSessionsIdleInTransactionTime           metricInfo
	PostgresqlSessionsKilled                          metricInfo
	PostgresqlSessionsSessionTime                     metricInfo
	PostgresqlSlruBlksExists                          metricInfo
	PostgresqlSlruBlksHit                             metricInfo
	PostgresqlSlruBlksRead                            metricInfo
	PostgresqlSlruBlksWritten                         metricInfo
	PostgresqlSlruBlksZeroed                          metricInfo
	PostgresqlSlruFlushes                             metricInfo
	PostgresqlSlruTruncates                           metricInfo
	PostgresqlSnapshotXipCount                        metricInfo
	PostgresqlSnapshotXmax                            metricInfo
	PostgresqlSnapshotXmin                            metricInfo
	PostgresqlSubscriptionApplyError                  metricInfo
	PostgresqlSubscriptionLastMsgReceiptAge           metricInfo
	PostgresqlSubscriptionLastMsgSendAge              metricInfo
	PostgresqlSubscriptionLatestEndAge                metricInfo
	PostgresqlSubscriptionSyncError                   metricInfo
	PostgresqlTempBytes                               metricInfo
	PostgresqlTempFiles                               metricInfo
	PostgresqlToastAutovacuumed                       metricInfo
	PostgresqlToastLastAutovacuumAge                  metricInfo
	PostgresqlToastLastVacuumAge                      metricInfo
	PostgresqlToastVacuumed                           metricInfo
	PostgresqlToastBlocksHit                          metricInfo
	PostgresqlToastBlocksRead                         metricInfo
	PostgresqlToastIndexBlocksHit                     metricInfo
	PostgresqlToastIndexBlocksRead                    metricInfo
	PostgresqlToastSize                               metricInfo
	PostgresqlTransactionsDurationMax                 metricInfo
	PostgresqlTransactionsDurationSum                 metricInfo
	PostgresqlUptime                                  metricInfo
	PostgresqlVacuumHeapBlksScanned                   metricInfo
	PostgresqlVacuumHeapBlksTotal                     metricInfo
	PostgresqlVacuumHeapBlksVacuumed                  metricInfo
	PostgresqlVacuumIndexVacuumCount                  metricInfo
	PostgresqlVacuumMaxDeadTuples                     metricInfo
	PostgresqlVacuumNumDeadTuples                     metricInfo
	PostgresqlVacuumed                                metricInfo
	PostgresqlWalBuffersFull                          metricInfo
	PostgresqlWalBytes                                metricInfo
	PostgresqlWalFpi                                  metricInfo
	PostgresqlWalRecords                              metricInfo
	PostgresqlWalSync                                 metricInfo
	PostgresqlWalSyncTime                             metricInfo
	PostgresqlWalWrite                                metricInfo
	PostgresqlWalWriteTime                            metricInfo
	PostgresqlWalFilesAge                             metricInfo
	PostgresqlWalFilesCount                           metricInfo
	PostgresqlWalFilesSize                            metricInfo
	PostgresqlWalReceiverConnected                    metricInfo
	PostgresqlWalReceiverLastMsgReceiptAge            metricInfo
	PostgresqlWalReceiverLastMsgSendAge               metricInfo
	PostgresqlWalReceiverLatestEndAge                 metricInfo
	PostgresqlWalReceiverReceivedTimeline             metricInfo
}

type metricInfo struct {
	Name string
}

type metricPgbouncerStatsAvgBytesIn struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_bytes_in metric with initial data.
func (m *metricPgbouncerStatsAvgBytesIn) init() {
	m.data.SetName("pgbouncer.stats.avg_bytes_in")
	m.data.SetDescription("Average bytes received per request (PgBouncer 1.8+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgBytesIn) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgBytesIn) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgBytesIn) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgBytesIn(cfg MetricConfig) metricPgbouncerStatsAvgBytesIn {
	m := metricPgbouncerStatsAvgBytesIn{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsAvgBytesOut struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_bytes_out metric with initial data.
func (m *metricPgbouncerStatsAvgBytesOut) init() {
	m.data.SetName("pgbouncer.stats.avg_bytes_out")
	m.data.SetDescription("Average bytes sent per request (PgBouncer 1.8+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgBytesOut) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgBytesOut) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgBytesOut) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgBytesOut(cfg MetricConfig) metricPgbouncerStatsAvgBytesOut {
	m := metricPgbouncerStatsAvgBytesOut{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsAvgRequestsPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_requests_per_second metric with initial data.
func (m *metricPgbouncerStatsAvgRequestsPerSecond) init() {
	m.data.SetName("pgbouncer.stats.avg_requests_per_second")
	m.data.SetDescription("Average requests per second (PgBouncer 1.8+)")
	m.data.SetUnit("{requests}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgRequestsPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgRequestsPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgRequestsPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgRequestsPerSecond(cfg MetricConfig) metricPgbouncerStatsAvgRequestsPerSecond {
	m := metricPgbouncerStatsAvgRequestsPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsAvgServerAssignmentCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_server_assignment_count metric with initial data.
func (m *metricPgbouncerStatsAvgServerAssignmentCount) init() {
	m.data.SetName("pgbouncer.stats.avg_server_assignment_count")
	m.data.SetDescription("Average number of server assignments per transaction (PgBouncer 1.8+)")
	m.data.SetUnit("{assignments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgServerAssignmentCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgServerAssignmentCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgServerAssignmentCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgServerAssignmentCount(cfg MetricConfig) metricPgbouncerStatsAvgServerAssignmentCount {
	m := metricPgbouncerStatsAvgServerAssignmentCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsAvgTransactionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_transaction_count metric with initial data.
func (m *metricPgbouncerStatsAvgTransactionCount) init() {
	m.data.SetName("pgbouncer.stats.avg_transaction_count")
	m.data.SetDescription("Average transaction count (PgBouncer 1.8+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgTransactionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgTransactionCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgTransactionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgTransactionCount(cfg MetricConfig) metricPgbouncerStatsAvgTransactionCount {
	m := metricPgbouncerStatsAvgTransactionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsAvgTransactionDurationMilliseconds struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.avg_transaction_duration_milliseconds metric with initial data.
func (m *metricPgbouncerStatsAvgTransactionDurationMilliseconds) init() {
	m.data.SetName("pgbouncer.stats.avg_transaction_duration_milliseconds")
	m.data.SetDescription("Average transaction duration in milliseconds (PgBouncer 1.8+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsAvgTransactionDurationMilliseconds) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsAvgTransactionDurationMilliseconds) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsAvgTransactionDurationMilliseconds) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsAvgTransactionDurationMilliseconds(cfg MetricConfig) metricPgbouncerStatsAvgTransactionDurationMilliseconds {
	m := metricPgbouncerStatsAvgTransactionDurationMilliseconds{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsBytesInPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.bytes_in_per_second metric with initial data.
func (m *metricPgbouncerStatsBytesInPerSecond) init() {
	m.data.SetName("pgbouncer.stats.bytes_in_per_second")
	m.data.SetDescription("Average bytes received per second from clients (PgBouncer 1.8+)")
	m.data.SetUnit("By/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsBytesInPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsBytesInPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsBytesInPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsBytesInPerSecond(cfg MetricConfig) metricPgbouncerStatsBytesInPerSecond {
	m := metricPgbouncerStatsBytesInPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsBytesOutPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.bytes_out_per_second metric with initial data.
func (m *metricPgbouncerStatsBytesOutPerSecond) init() {
	m.data.SetName("pgbouncer.stats.bytes_out_per_second")
	m.data.SetDescription("Average bytes sent per second to clients (PgBouncer 1.8+)")
	m.data.SetUnit("By/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsBytesOutPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsBytesOutPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsBytesOutPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsBytesOutPerSecond(cfg MetricConfig) metricPgbouncerStatsBytesOutPerSecond {
	m := metricPgbouncerStatsBytesOutPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsQueriesPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.queries_per_second metric with initial data.
func (m *metricPgbouncerStatsQueriesPerSecond) init() {
	m.data.SetName("pgbouncer.stats.queries_per_second")
	m.data.SetDescription("Average SQL queries per second (PgBouncer 1.8+)")
	m.data.SetUnit("{queries}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsQueriesPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsQueriesPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsQueriesPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsQueriesPerSecond(cfg MetricConfig) metricPgbouncerStatsQueriesPerSecond {
	m := metricPgbouncerStatsQueriesPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsRequestsPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.requests_per_second metric with initial data.
func (m *metricPgbouncerStatsRequestsPerSecond) init() {
	m.data.SetName("pgbouncer.stats.requests_per_second")
	m.data.SetDescription("Average client requests per second (PgBouncer 1.8+)")
	m.data.SetUnit("{requests}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsRequestsPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsRequestsPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsRequestsPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsRequestsPerSecond(cfg MetricConfig) metricPgbouncerStatsRequestsPerSecond {
	m := metricPgbouncerStatsRequestsPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsTotalServerAssignmentCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.total_server_assignment_count metric with initial data.
func (m *metricPgbouncerStatsTotalServerAssignmentCount) init() {
	m.data.SetName("pgbouncer.stats.total_server_assignment_count")
	m.data.SetDescription("Total number of server assignments since PgBouncer start (PgBouncer 1.8+)")
	m.data.SetUnit("{assignments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsTotalServerAssignmentCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsTotalServerAssignmentCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsTotalServerAssignmentCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsTotalServerAssignmentCount(cfg MetricConfig) metricPgbouncerStatsTotalServerAssignmentCount {
	m := metricPgbouncerStatsTotalServerAssignmentCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPgbouncerStatsTransactionsPerSecond struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills pgbouncer.stats.transactions_per_second metric with initial data.
func (m *metricPgbouncerStatsTransactionsPerSecond) init() {
	m.data.SetName("pgbouncer.stats.transactions_per_second")
	m.data.SetDescription("Average transactions per second (PgBouncer 1.8+)")
	m.data.SetUnit("{transactions}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPgbouncerStatsTransactionsPerSecond) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPgbouncerStatsTransactionsPerSecond) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPgbouncerStatsTransactionsPerSecond) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPgbouncerStatsTransactionsPerSecond(cfg MetricConfig) metricPgbouncerStatsTransactionsPerSecond {
	m := metricPgbouncerStatsTransactionsPerSecond{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlActiveWaitingQueries struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.active_waiting_queries metric with initial data.
func (m *metricPostgresqlActiveWaitingQueries) init() {
	m.data.SetName("postgresql.active_waiting_queries")
	m.data.SetDescription("Number of active queries currently waiting on locks or other resources (PostgreSQL 9.6+)")
	m.data.SetUnit("{queries}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlActiveWaitingQueries) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlActiveWaitingQueries) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlActiveWaitingQueries) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlActiveWaitingQueries(cfg MetricConfig) metricPostgresqlActiveWaitingQueries {
	m := metricPostgresqlActiveWaitingQueries{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlActivityBackendXidAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.activity.backend_xid_age metric with initial data.
func (m *metricPostgresqlActivityBackendXidAge) init() {
	m.data.SetName("postgresql.activity.backend_xid_age")
	m.data.SetDescription("Maximum age of backend transaction IDs currently in use (PostgreSQL 9.6+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlActivityBackendXidAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlActivityBackendXidAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlActivityBackendXidAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlActivityBackendXidAge(cfg MetricConfig) metricPostgresqlActivityBackendXidAge {
	m := metricPostgresqlActivityBackendXidAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlActivityBackendXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.activity.backend_xmin_age metric with initial data.
func (m *metricPostgresqlActivityBackendXminAge) init() {
	m.data.SetName("postgresql.activity.backend_xmin_age")
	m.data.SetDescription("Maximum age of backend xmin values (oldest transaction visible to any backend) (PostgreSQL 9.6+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlActivityBackendXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlActivityBackendXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlActivityBackendXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlActivityBackendXminAge(cfg MetricConfig) metricPostgresqlActivityBackendXminAge {
	m := metricPostgresqlActivityBackendXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlActivityWaitEvent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.activity.wait_event metric with initial data.
func (m *metricPostgresqlActivityWaitEvent) init() {
	m.data.SetName("postgresql.activity.wait_event")
	m.data.SetDescription("Count of backends grouped by wait event type for performance analysis (PostgreSQL 9.6+)")
	m.data.SetUnit("{backends}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlActivityWaitEvent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string, waitEventAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
	dp.Attributes().PutStr("wait_event", waitEventAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlActivityWaitEvent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlActivityWaitEvent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlActivityWaitEvent(cfg MetricConfig) metricPostgresqlActivityWaitEvent {
	m := metricPostgresqlActivityWaitEvent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlActivityXactStartAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.activity.xact_start_age metric with initial data.
func (m *metricPostgresqlActivityXactStartAge) init() {
	m.data.SetName("postgresql.activity.xact_start_age")
	m.data.SetDescription("Maximum age in seconds of the oldest transaction start time across all backends (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlActivityXactStartAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlActivityXactStartAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlActivityXactStartAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlActivityXactStartAge(cfg MetricConfig) metricPostgresqlActivityXactStartAge {
	m := metricPostgresqlActivityXactStartAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeChildTablesDone struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.child_tables_done metric with initial data.
func (m *metricPostgresqlAnalyzeChildTablesDone) init() {
	m.data.SetName("postgresql.analyze.child_tables_done")
	m.data.SetDescription("Number of child tables processed during ANALYZE operation (PostgreSQL 13+)")
	m.data.SetUnit("{tables}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeChildTablesDone) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeChildTablesDone) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeChildTablesDone) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeChildTablesDone(cfg MetricConfig) metricPostgresqlAnalyzeChildTablesDone {
	m := metricPostgresqlAnalyzeChildTablesDone{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeChildTablesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.child_tables_total metric with initial data.
func (m *metricPostgresqlAnalyzeChildTablesTotal) init() {
	m.data.SetName("postgresql.analyze.child_tables_total")
	m.data.SetDescription("Total number of child tables to be analyzed (PostgreSQL 13+)")
	m.data.SetUnit("{tables}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeChildTablesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeChildTablesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeChildTablesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeChildTablesTotal(cfg MetricConfig) metricPostgresqlAnalyzeChildTablesTotal {
	m := metricPostgresqlAnalyzeChildTablesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeExtStatsComputed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.ext_stats_computed metric with initial data.
func (m *metricPostgresqlAnalyzeExtStatsComputed) init() {
	m.data.SetName("postgresql.analyze.ext_stats_computed")
	m.data.SetDescription("Number of extended statistics computed during ANALYZE operation (PostgreSQL 13+)")
	m.data.SetUnit("{statistics}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeExtStatsComputed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeExtStatsComputed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeExtStatsComputed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeExtStatsComputed(cfg MetricConfig) metricPostgresqlAnalyzeExtStatsComputed {
	m := metricPostgresqlAnalyzeExtStatsComputed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeExtStatsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.ext_stats_total metric with initial data.
func (m *metricPostgresqlAnalyzeExtStatsTotal) init() {
	m.data.SetName("postgresql.analyze.ext_stats_total")
	m.data.SetDescription("Total number of extended statistics to be computed (PostgreSQL 13+)")
	m.data.SetUnit("{statistics}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeExtStatsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeExtStatsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeExtStatsTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeExtStatsTotal(cfg MetricConfig) metricPostgresqlAnalyzeExtStatsTotal {
	m := metricPostgresqlAnalyzeExtStatsTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeSampleBlksScanned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.sample_blks_scanned metric with initial data.
func (m *metricPostgresqlAnalyzeSampleBlksScanned) init() {
	m.data.SetName("postgresql.analyze.sample_blks_scanned")
	m.data.SetDescription("Number of sample blocks scanned during ANALYZE operation (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeSampleBlksScanned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeSampleBlksScanned) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeSampleBlksScanned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeSampleBlksScanned(cfg MetricConfig) metricPostgresqlAnalyzeSampleBlksScanned {
	m := metricPostgresqlAnalyzeSampleBlksScanned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzeSampleBlksTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyze.sample_blks_total metric with initial data.
func (m *metricPostgresqlAnalyzeSampleBlksTotal) init() {
	m.data.SetName("postgresql.analyze.sample_blks_total")
	m.data.SetDescription("Total number of sample blocks to be scanned (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzeSampleBlksTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzeSampleBlksTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzeSampleBlksTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzeSampleBlksTotal(cfg MetricConfig) metricPostgresqlAnalyzeSampleBlksTotal {
	m := metricPostgresqlAnalyzeSampleBlksTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAnalyzed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.analyzed metric with initial data.
func (m *metricPostgresqlAnalyzed) init() {
	m.data.SetName("postgresql.analyzed")
	m.data.SetDescription("Number of times this table has been manually analyzed (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAnalyzed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAnalyzed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAnalyzed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAnalyzed(cfg MetricConfig) metricPostgresqlAnalyzed {
	m := metricPostgresqlAnalyzed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlArchiverArchivedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.archiver.archived_count metric with initial data.
func (m *metricPostgresqlArchiverArchivedCount) init() {
	m.data.SetName("postgresql.archiver.archived_count")
	m.data.SetDescription("Number of WAL files successfully archived (PostgreSQL 9.6+)")
	m.data.SetUnit("{files}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlArchiverArchivedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlArchiverArchivedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlArchiverArchivedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlArchiverArchivedCount(cfg MetricConfig) metricPostgresqlArchiverArchivedCount {
	m := metricPostgresqlArchiverArchivedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlArchiverFailedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.archiver.failed_count metric with initial data.
func (m *metricPostgresqlArchiverFailedCount) init() {
	m.data.SetName("postgresql.archiver.failed_count")
	m.data.SetDescription("Number of failed attempts to archive WAL files (PostgreSQL 9.6+)")
	m.data.SetUnit("{files}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlArchiverFailedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlArchiverFailedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlArchiverFailedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlArchiverFailedCount(cfg MetricConfig) metricPostgresqlArchiverFailedCount {
	m := metricPostgresqlArchiverFailedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAutoanalyzed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.autoanalyzed metric with initial data.
func (m *metricPostgresqlAutoanalyzed) init() {
	m.data.SetName("postgresql.autoanalyzed")
	m.data.SetDescription("Number of times this table has been analyzed by autoanalyze (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAutoanalyzed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAutoanalyzed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAutoanalyzed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAutoanalyzed(cfg MetricConfig) metricPostgresqlAutoanalyzed {
	m := metricPostgresqlAutoanalyzed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlAutovacuumed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.autovacuumed metric with initial data.
func (m *metricPostgresqlAutovacuumed) init() {
	m.data.SetName("postgresql.autovacuumed")
	m.data.SetDescription("Number of times this table has been vacuumed by autovacuum (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlAutovacuumed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlAutovacuumed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlAutovacuumed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlAutovacuumed(cfg MetricConfig) metricPostgresqlAutovacuumed {
	m := metricPostgresqlAutovacuumed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBeforeXidWraparound struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.before_xid_wraparound metric with initial data.
func (m *metricPostgresqlBeforeXidWraparound) init() {
	m.data.SetName("postgresql.before_xid_wraparound")
	m.data.SetDescription("Number of transactions before XID wraparound occurs")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBeforeXidWraparound) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBeforeXidWraparound) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBeforeXidWraparound) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBeforeXidWraparound(cfg MetricConfig) metricPostgresqlBeforeXidWraparound {
	m := metricPostgresqlBeforeXidWraparound{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersAlloc struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers_alloc metric with initial data.
func (m *metricPostgresqlBgwriterBuffersAlloc) init() {
	m.data.SetName("postgresql.bgwriter.buffers_alloc")
	m.data.SetDescription("Number of buffers allocated (PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersAlloc) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersAlloc) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersAlloc) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersAlloc(cfg MetricConfig) metricPostgresqlBgwriterBuffersAlloc {
	m := metricPostgresqlBgwriterBuffersAlloc{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersBackend struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers_backend metric with initial data.
func (m *metricPostgresqlBgwriterBuffersBackend) init() {
	m.data.SetName("postgresql.bgwriter.buffers_backend")
	m.data.SetDescription("Number of buffers written directly by a backend (PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersBackend) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersBackend) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersBackend) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersBackend(cfg MetricConfig) metricPostgresqlBgwriterBuffersBackend {
	m := metricPostgresqlBgwriterBuffersBackend{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersBackendFsync struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers_backend_fsync metric with initial data.
func (m *metricPostgresqlBgwriterBuffersBackendFsync) init() {
	m.data.SetName("postgresql.bgwriter.buffers_backend_fsync")
	m.data.SetDescription("Number of times a backend had to execute its own fsync call (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersBackendFsync) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersBackendFsync) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersBackendFsync) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersBackendFsync(cfg MetricConfig) metricPostgresqlBgwriterBuffersBackendFsync {
	m := metricPostgresqlBgwriterBuffersBackendFsync{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersCheckpoint struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers_checkpoint metric with initial data.
func (m *metricPostgresqlBgwriterBuffersCheckpoint) init() {
	m.data.SetName("postgresql.bgwriter.buffers_checkpoint")
	m.data.SetDescription("Number of buffers written during checkpoints (PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersCheckpoint) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersCheckpoint) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersCheckpoint) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersCheckpoint(cfg MetricConfig) metricPostgresqlBgwriterBuffersCheckpoint {
	m := metricPostgresqlBgwriterBuffersCheckpoint{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersClean struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers_clean metric with initial data.
func (m *metricPostgresqlBgwriterBuffersClean) init() {
	m.data.SetName("postgresql.bgwriter.buffers_clean")
	m.data.SetDescription("Number of buffers written by the background writer (PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersClean) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersClean) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersClean) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersClean(cfg MetricConfig) metricPostgresqlBgwriterBuffersClean {
	m := metricPostgresqlBgwriterBuffersClean{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterCheckpointsRequested struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.checkpoints_requested metric with initial data.
func (m *metricPostgresqlBgwriterCheckpointsRequested) init() {
	m.data.SetName("postgresql.bgwriter.checkpoints_requested")
	m.data.SetDescription("Number of requested checkpoints that have been performed (PostgreSQL 9.6+)")
	m.data.SetUnit("{checkpoints}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterCheckpointsRequested) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterCheckpointsRequested) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterCheckpointsRequested) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterCheckpointsRequested(cfg MetricConfig) metricPostgresqlBgwriterCheckpointsRequested {
	m := metricPostgresqlBgwriterCheckpointsRequested{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterCheckpointsTimed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.checkpoints_timed metric with initial data.
func (m *metricPostgresqlBgwriterCheckpointsTimed) init() {
	m.data.SetName("postgresql.bgwriter.checkpoints_timed")
	m.data.SetDescription("Number of scheduled checkpoints that have been performed (PostgreSQL 9.6+)")
	m.data.SetUnit("{checkpoints}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterCheckpointsTimed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterCheckpointsTimed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterCheckpointsTimed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterCheckpointsTimed(cfg MetricConfig) metricPostgresqlBgwriterCheckpointsTimed {
	m := metricPostgresqlBgwriterCheckpointsTimed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterMaxwrittenClean struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.maxwritten_clean metric with initial data.
func (m *metricPostgresqlBgwriterMaxwrittenClean) init() {
	m.data.SetName("postgresql.bgwriter.maxwritten_clean")
	m.data.SetDescription("Number of times the background writer stopped a cleaning scan because it had written too many buffers (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterMaxwrittenClean) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterMaxwrittenClean) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterMaxwrittenClean) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterMaxwrittenClean(cfg MetricConfig) metricPostgresqlBgwriterMaxwrittenClean {
	m := metricPostgresqlBgwriterMaxwrittenClean{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterSyncTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.sync_time metric with initial data.
func (m *metricPostgresqlBgwriterSyncTime) init() {
	m.data.SetName("postgresql.bgwriter.sync_time")
	m.data.SetDescription("Total time spent syncing checkpoint data to disk in milliseconds (PostgreSQL 9.6+)")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterSyncTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterSyncTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterSyncTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterSyncTime(cfg MetricConfig) metricPostgresqlBgwriterSyncTime {
	m := metricPostgresqlBgwriterSyncTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.write_time metric with initial data.
func (m *metricPostgresqlBgwriterWriteTime) init() {
	m.data.SetName("postgresql.bgwriter.write_time")
	m.data.SetDescription("Total time spent writing checkpoint data to disk in milliseconds (PostgreSQL 9.6+)")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterWriteTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterWriteTime(cfg MetricConfig) metricPostgresqlBgwriterWriteTime {
	m := metricPostgresqlBgwriterWriteTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlkReadTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blk_read_time metric with initial data.
func (m *metricPostgresqlBlkReadTime) init() {
	m.data.SetName("postgresql.blk_read_time")
	m.data.SetDescription("Time spent reading data file blocks (milliseconds)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlkReadTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlkReadTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlkReadTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlkReadTime(cfg MetricConfig) metricPostgresqlBlkReadTime {
	m := metricPostgresqlBlkReadTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlkWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blk_write_time metric with initial data.
func (m *metricPostgresqlBlkWriteTime) init() {
	m.data.SetName("postgresql.blk_write_time")
	m.data.SetDescription("Time spent writing data file blocks (milliseconds)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlkWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlkWriteTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlkWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlkWriteTime(cfg MetricConfig) metricPostgresqlBlkWriteTime {
	m := metricPostgresqlBlkWriteTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBufferHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffer_hit metric with initial data.
func (m *metricPostgresqlBufferHit) init() {
	m.data.SetName("postgresql.buffer_hit")
	m.data.SetDescription("Number of times disk blocks were found in the buffer cache")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBufferHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBufferHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBufferHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBufferHit(cfg MetricConfig) metricPostgresqlBufferHit {
	m := metricPostgresqlBufferHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBuffercacheDirtyBuffers struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffercache.dirty_buffers metric with initial data.
func (m *metricPostgresqlBuffercacheDirtyBuffers) init() {
	m.data.SetName("postgresql.buffercache.dirty_buffers")
	m.data.SetDescription("Number of dirty buffers in the shared buffer cache (requires pg_buffercache extension, PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBuffercacheDirtyBuffers) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBuffercacheDirtyBuffers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBuffercacheDirtyBuffers) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBuffercacheDirtyBuffers(cfg MetricConfig) metricPostgresqlBuffercacheDirtyBuffers {
	m := metricPostgresqlBuffercacheDirtyBuffers{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBuffercachePinningBackends struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffercache.pinning_backends metric with initial data.
func (m *metricPostgresqlBuffercachePinningBackends) init() {
	m.data.SetName("postgresql.buffercache.pinning_backends")
	m.data.SetDescription("Number of backends pinning buffers in the shared buffer cache (requires pg_buffercache extension, PostgreSQL 9.6+)")
	m.data.SetUnit("{backends}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBuffercachePinningBackends) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBuffercachePinningBackends) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBuffercachePinningBackends) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBuffercachePinningBackends(cfg MetricConfig) metricPostgresqlBuffercachePinningBackends {
	m := metricPostgresqlBuffercachePinningBackends{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBuffercacheUnusedBuffers struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffercache.unused_buffers metric with initial data.
func (m *metricPostgresqlBuffercacheUnusedBuffers) init() {
	m.data.SetName("postgresql.buffercache.unused_buffers")
	m.data.SetDescription("Number of unused buffers in the shared buffer cache (requires pg_buffercache extension, PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBuffercacheUnusedBuffers) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBuffercacheUnusedBuffers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBuffercacheUnusedBuffers) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBuffercacheUnusedBuffers(cfg MetricConfig) metricPostgresqlBuffercacheUnusedBuffers {
	m := metricPostgresqlBuffercacheUnusedBuffers{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBuffercacheUsageCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffercache.usage_count metric with initial data.
func (m *metricPostgresqlBuffercacheUsageCount) init() {
	m.data.SetName("postgresql.buffercache.usage_count")
	m.data.SetDescription("Sum of usage counts for buffers in the shared buffer cache (requires pg_buffercache extension, PostgreSQL 9.6+)")
	m.data.SetUnit("{count}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBuffercacheUsageCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBuffercacheUsageCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBuffercacheUsageCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBuffercacheUsageCount(cfg MetricConfig) metricPostgresqlBuffercacheUsageCount {
	m := metricPostgresqlBuffercacheUsageCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBuffercacheUsedBuffers struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffercache.used_buffers metric with initial data.
func (m *metricPostgresqlBuffercacheUsedBuffers) init() {
	m.data.SetName("postgresql.buffercache.used_buffers")
	m.data.SetDescription("Number of used buffers in the shared buffer cache (requires pg_buffercache extension, PostgreSQL 9.6+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBuffercacheUsedBuffers) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBuffercacheUsedBuffers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBuffercacheUsedBuffers) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBuffercacheUsedBuffers(cfg MetricConfig) metricPostgresqlBuffercacheUsedBuffers {
	m := metricPostgresqlBuffercacheUsedBuffers{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlChecksumsEnabled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.checksums.enabled metric with initial data.
func (m *metricPostgresqlChecksumsEnabled) init() {
	m.data.SetName("postgresql.checksums.enabled")
	m.data.SetDescription("Whether data checksums are enabled for this PostgreSQL cluster (PostgreSQL 12+)")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlChecksumsEnabled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlChecksumsEnabled) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlChecksumsEnabled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlChecksumsEnabled(cfg MetricConfig) metricPostgresqlChecksumsEnabled {
	m := metricPostgresqlChecksumsEnabled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlChecksumsFailures struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.checksums.failures metric with initial data.
func (m *metricPostgresqlChecksumsFailures) init() {
	m.data.SetName("postgresql.checksums.failures")
	m.data.SetDescription("Number of data page checksum failures detected (PostgreSQL 12+)")
	m.data.SetUnit("{failures}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlChecksumsFailures) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlChecksumsFailures) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlChecksumsFailures) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlChecksumsFailures(cfg MetricConfig) metricPostgresqlChecksumsFailures {
	m := metricPostgresqlChecksumsFailures{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlClusterVacuumHeapBlksScanned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.cluster_vacuum.heap_blks_scanned metric with initial data.
func (m *metricPostgresqlClusterVacuumHeapBlksScanned) init() {
	m.data.SetName("postgresql.cluster_vacuum.heap_blks_scanned")
	m.data.SetDescription("Number of heap blocks scanned during CLUSTER/VACUUM FULL operation (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlClusterVacuumHeapBlksScanned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlClusterVacuumHeapBlksScanned) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlClusterVacuumHeapBlksScanned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlClusterVacuumHeapBlksScanned(cfg MetricConfig) metricPostgresqlClusterVacuumHeapBlksScanned {
	m := metricPostgresqlClusterVacuumHeapBlksScanned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlClusterVacuumHeapBlksTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.cluster_vacuum.heap_blks_total metric with initial data.
func (m *metricPostgresqlClusterVacuumHeapBlksTotal) init() {
	m.data.SetName("postgresql.cluster_vacuum.heap_blks_total")
	m.data.SetDescription("Total number of heap blocks to be scanned during CLUSTER/VACUUM FULL (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlClusterVacuumHeapBlksTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlClusterVacuumHeapBlksTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlClusterVacuumHeapBlksTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlClusterVacuumHeapBlksTotal(cfg MetricConfig) metricPostgresqlClusterVacuumHeapBlksTotal {
	m := metricPostgresqlClusterVacuumHeapBlksTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlClusterVacuumHeapTuplesScanned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.cluster_vacuum.heap_tuples_scanned metric with initial data.
func (m *metricPostgresqlClusterVacuumHeapTuplesScanned) init() {
	m.data.SetName("postgresql.cluster_vacuum.heap_tuples_scanned")
	m.data.SetDescription("Number of heap tuples scanned during CLUSTER/VACUUM FULL operation (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlClusterVacuumHeapTuplesScanned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlClusterVacuumHeapTuplesScanned) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlClusterVacuumHeapTuplesScanned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlClusterVacuumHeapTuplesScanned(cfg MetricConfig) metricPostgresqlClusterVacuumHeapTuplesScanned {
	m := metricPostgresqlClusterVacuumHeapTuplesScanned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlClusterVacuumHeapTuplesWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.cluster_vacuum.heap_tuples_written metric with initial data.
func (m *metricPostgresqlClusterVacuumHeapTuplesWritten) init() {
	m.data.SetName("postgresql.cluster_vacuum.heap_tuples_written")
	m.data.SetDescription("Number of heap tuples written during CLUSTER/VACUUM FULL operation (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlClusterVacuumHeapTuplesWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlClusterVacuumHeapTuplesWritten) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlClusterVacuumHeapTuplesWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlClusterVacuumHeapTuplesWritten(cfg MetricConfig) metricPostgresqlClusterVacuumHeapTuplesWritten {
	m := metricPostgresqlClusterVacuumHeapTuplesWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCommits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.commits metric with initial data.
func (m *metricPostgresqlCommits) init() {
	m.data.SetName("postgresql.commits")
	m.data.SetDescription("Number of transactions that have been committed")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCommits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCommits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCommits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCommits(cfg MetricConfig) metricPostgresqlCommits {
	m := metricPostgresqlCommits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflicts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts metric with initial data.
func (m *metricPostgresqlConflicts) init() {
	m.data.SetName("postgresql.conflicts")
	m.data.SetDescription("Number of queries canceled due to conflicts with recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflicts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflicts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflicts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflicts(cfg MetricConfig) metricPostgresqlConflicts {
	m := metricPostgresqlConflicts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsBufferpin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.bufferpin metric with initial data.
func (m *metricPostgresqlConflictsBufferpin) init() {
	m.data.SetName("postgresql.conflicts.bufferpin")
	m.data.SetDescription("Queries canceled due to pinned buffers during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsBufferpin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsBufferpin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsBufferpin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsBufferpin(cfg MetricConfig) metricPostgresqlConflictsBufferpin {
	m := metricPostgresqlConflictsBufferpin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsDeadlock struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.deadlock metric with initial data.
func (m *metricPostgresqlConflictsDeadlock) init() {
	m.data.SetName("postgresql.conflicts.deadlock")
	m.data.SetDescription("Queries canceled due to deadlocks during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsDeadlock) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsDeadlock) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsDeadlock) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsDeadlock(cfg MetricConfig) metricPostgresqlConflictsDeadlock {
	m := metricPostgresqlConflictsDeadlock{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsLock struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.lock metric with initial data.
func (m *metricPostgresqlConflictsLock) init() {
	m.data.SetName("postgresql.conflicts.lock")
	m.data.SetDescription("Queries canceled due to lock timeouts during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsLock) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsLock) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsLock) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsLock(cfg MetricConfig) metricPostgresqlConflictsLock {
	m := metricPostgresqlConflictsLock{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsSnapshot struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.snapshot metric with initial data.
func (m *metricPostgresqlConflictsSnapshot) init() {
	m.data.SetName("postgresql.conflicts.snapshot")
	m.data.SetDescription("Queries canceled due to old snapshots during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsSnapshot) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsSnapshot) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsSnapshot) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsSnapshot(cfg MetricConfig) metricPostgresqlConflictsSnapshot {
	m := metricPostgresqlConflictsSnapshot{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsTablespace struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.tablespace metric with initial data.
func (m *metricPostgresqlConflictsTablespace) init() {
	m.data.SetName("postgresql.conflicts.tablespace")
	m.data.SetDescription("Queries canceled due to dropped tablespaces during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsTablespace) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsTablespace) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsTablespace) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsTablespace(cfg MetricConfig) metricPostgresqlConflictsTablespace {
	m := metricPostgresqlConflictsTablespace{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connections metric with initial data.
func (m *metricPostgresqlConnections) init() {
	m.data.SetName("postgresql.connections")
	m.data.SetDescription("Number of active connections (backends) to the database")
	m.data.SetUnit("{connections}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnections(cfg MetricConfig) metricPostgresqlConnections {
	m := metricPostgresqlConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlControlCheckpointDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.control.checkpoint_delay metric with initial data.
func (m *metricPostgresqlControlCheckpointDelay) init() {
	m.data.SetName("postgresql.control.checkpoint_delay")
	m.data.SetDescription("Time elapsed since the last checkpoint in seconds (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlControlCheckpointDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlControlCheckpointDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlControlCheckpointDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlControlCheckpointDelay(cfg MetricConfig) metricPostgresqlControlCheckpointDelay {
	m := metricPostgresqlControlCheckpointDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlControlCheckpointDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.control.checkpoint_delay_bytes metric with initial data.
func (m *metricPostgresqlControlCheckpointDelayBytes) init() {
	m.data.SetName("postgresql.control.checkpoint_delay_bytes")
	m.data.SetDescription("WAL distance from the last checkpoint in bytes (PostgreSQL 10+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlControlCheckpointDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlControlCheckpointDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlControlCheckpointDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlControlCheckpointDelayBytes(cfg MetricConfig) metricPostgresqlControlCheckpointDelayBytes {
	m := metricPostgresqlControlCheckpointDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlControlRedoDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.control.redo_delay_bytes metric with initial data.
func (m *metricPostgresqlControlRedoDelayBytes) init() {
	m.data.SetName("postgresql.control.redo_delay_bytes")
	m.data.SetDescription("WAL distance from the redo location in bytes (PostgreSQL 10+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlControlRedoDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlControlRedoDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlControlRedoDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlControlRedoDelayBytes(cfg MetricConfig) metricPostgresqlControlRedoDelayBytes {
	m := metricPostgresqlControlRedoDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlControlTimelineID struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.control.timeline_id metric with initial data.
func (m *metricPostgresqlControlTimelineID) init() {
	m.data.SetName("postgresql.control.timeline_id")
	m.data.SetDescription("Current timeline ID (PostgreSQL 10+)")
	m.data.SetUnit("{timeline}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlControlTimelineID) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlControlTimelineID) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlControlTimelineID) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlControlTimelineID(cfg MetricConfig) metricPostgresqlControlTimelineID {
	m := metricPostgresqlControlTimelineID{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexBlocksDone struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.blocks_done metric with initial data.
func (m *metricPostgresqlCreateIndexBlocksDone) init() {
	m.data.SetName("postgresql.create_index.blocks_done")
	m.data.SetDescription("Number of blocks processed during CREATE INDEX operation (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexBlocksDone) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexBlocksDone) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexBlocksDone) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexBlocksDone(cfg MetricConfig) metricPostgresqlCreateIndexBlocksDone {
	m := metricPostgresqlCreateIndexBlocksDone{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexBlocksTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.blocks_total metric with initial data.
func (m *metricPostgresqlCreateIndexBlocksTotal) init() {
	m.data.SetName("postgresql.create_index.blocks_total")
	m.data.SetDescription("Total number of blocks to be processed during CREATE INDEX (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexBlocksTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexBlocksTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexBlocksTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexBlocksTotal(cfg MetricConfig) metricPostgresqlCreateIndexBlocksTotal {
	m := metricPostgresqlCreateIndexBlocksTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexLockersDone struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.lockers_done metric with initial data.
func (m *metricPostgresqlCreateIndexLockersDone) init() {
	m.data.SetName("postgresql.create_index.lockers_done")
	m.data.SetDescription("Number of lockers processed during CREATE INDEX operation (PostgreSQL 12+)")
	m.data.SetUnit("{lockers}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexLockersDone) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexLockersDone) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexLockersDone) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexLockersDone(cfg MetricConfig) metricPostgresqlCreateIndexLockersDone {
	m := metricPostgresqlCreateIndexLockersDone{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexLockersTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.lockers_total metric with initial data.
func (m *metricPostgresqlCreateIndexLockersTotal) init() {
	m.data.SetName("postgresql.create_index.lockers_total")
	m.data.SetDescription("Total number of lockers to be processed during CREATE INDEX (PostgreSQL 12+)")
	m.data.SetUnit("{lockers}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexLockersTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexLockersTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexLockersTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexLockersTotal(cfg MetricConfig) metricPostgresqlCreateIndexLockersTotal {
	m := metricPostgresqlCreateIndexLockersTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexPartitionsDone struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.partitions_done metric with initial data.
func (m *metricPostgresqlCreateIndexPartitionsDone) init() {
	m.data.SetName("postgresql.create_index.partitions_done")
	m.data.SetDescription("Number of partitions processed during CREATE INDEX operation (PostgreSQL 12+)")
	m.data.SetUnit("{partitions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexPartitionsDone) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexPartitionsDone) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexPartitionsDone) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexPartitionsDone(cfg MetricConfig) metricPostgresqlCreateIndexPartitionsDone {
	m := metricPostgresqlCreateIndexPartitionsDone{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexPartitionsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.partitions_total metric with initial data.
func (m *metricPostgresqlCreateIndexPartitionsTotal) init() {
	m.data.SetName("postgresql.create_index.partitions_total")
	m.data.SetDescription("Total number of partitions to be processed during CREATE INDEX (PostgreSQL 12+)")
	m.data.SetUnit("{partitions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexPartitionsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexPartitionsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexPartitionsTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexPartitionsTotal(cfg MetricConfig) metricPostgresqlCreateIndexPartitionsTotal {
	m := metricPostgresqlCreateIndexPartitionsTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexTuplesDone struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.tuples_done metric with initial data.
func (m *metricPostgresqlCreateIndexTuplesDone) init() {
	m.data.SetName("postgresql.create_index.tuples_done")
	m.data.SetDescription("Number of tuples indexed during CREATE INDEX operation (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexTuplesDone) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexTuplesDone) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexTuplesDone) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexTuplesDone(cfg MetricConfig) metricPostgresqlCreateIndexTuplesDone {
	m := metricPostgresqlCreateIndexTuplesDone{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCreateIndexTuplesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.create_index.tuples_total metric with initial data.
func (m *metricPostgresqlCreateIndexTuplesTotal) init() {
	m.data.SetName("postgresql.create_index.tuples_total")
	m.data.SetDescription("Total number of tuples to be indexed during CREATE INDEX (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCreateIndexTuplesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCreateIndexTuplesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCreateIndexTuplesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCreateIndexTuplesTotal(cfg MetricConfig) metricPostgresqlCreateIndexTuplesTotal {
	m := metricPostgresqlCreateIndexTuplesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database_size metric with initial data.
func (m *metricPostgresqlDatabaseSize) init() {
	m.data.SetName("postgresql.database_size")
	m.data.SetDescription("Size of the database in bytes")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDatabaseSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseSize(cfg MetricConfig) metricPostgresqlDatabaseSize {
	m := metricPostgresqlDatabaseSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDbCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.db.count metric with initial data.
func (m *metricPostgresqlDbCount) init() {
	m.data.SetName("postgresql.db.count")
	m.data.SetDescription("Number of databases that allow connections (PostgreSQL 9.6+)")
	m.data.SetUnit("{databases}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDbCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDbCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDbCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDbCount(cfg MetricConfig) metricPostgresqlDbCount {
	m := metricPostgresqlDbCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDeadlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.deadlocks metric with initial data.
func (m *metricPostgresqlDeadlocks) init() {
	m.data.SetName("postgresql.deadlocks")
	m.data.SetDescription("Number of deadlocks detected")
	m.data.SetUnit("{deadlocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDeadlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDeadlocks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDeadlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDeadlocks(cfg MetricConfig) metricPostgresqlDeadlocks {
	m := metricPostgresqlDeadlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDiskRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.disk_read metric with initial data.
func (m *metricPostgresqlDiskRead) init() {
	m.data.SetName("postgresql.disk_read")
	m.data.SetDescription("Number of disk blocks read")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDiskRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDiskRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDiskRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDiskRead(cfg MetricConfig) metricPostgresqlDiskRead {
	m := metricPostgresqlDiskRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlHeapBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.heap_blocks_hit metric with initial data.
func (m *metricPostgresqlHeapBlocksHit) init() {
	m.data.SetName("postgresql.heap_blocks_hit")
	m.data.SetDescription("Number of heap blocks read from buffer cache for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlHeapBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlHeapBlocksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlHeapBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlHeapBlocksHit(cfg MetricConfig) metricPostgresqlHeapBlocksHit {
	m := metricPostgresqlHeapBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlHeapBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.heap_blocks_read metric with initial data.
func (m *metricPostgresqlHeapBlocksRead) init() {
	m.data.SetName("postgresql.heap_blocks_read")
	m.data.SetDescription("Number of heap blocks read from disk for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlHeapBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlHeapBlocksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlHeapBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlHeapBlocksRead(cfg MetricConfig) metricPostgresqlHeapBlocksRead {
	m := metricPostgresqlHeapBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_blocks_hit metric with initial data.
func (m *metricPostgresqlIndexBlocksHit) init() {
	m.data.SetName("postgresql.index_blocks_hit")
	m.data.SetDescription("Number of index blocks read from buffer cache for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexBlocksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexBlocksHit(cfg MetricConfig) metricPostgresqlIndexBlocksHit {
	m := metricPostgresqlIndexBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_blocks_read metric with initial data.
func (m *metricPostgresqlIndexBlocksRead) init() {
	m.data.SetName("postgresql.index_blocks_read")
	m.data.SetDescription("Number of index blocks read from disk for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexBlocksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexBlocksRead(cfg MetricConfig) metricPostgresqlIndexBlocksRead {
	m := metricPostgresqlIndexBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_scans metric with initial data.
func (m *metricPostgresqlIndexScans) init() {
	m.data.SetName("postgresql.index_scans")
	m.data.SetDescription("Number of index scans initiated on this index (PostgreSQL 9.6+)")
	m.data.SetUnit("{scans}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexScans(cfg MetricConfig) metricPostgresqlIndexScans {
	m := metricPostgresqlIndexScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_size metric with initial data.
func (m *metricPostgresqlIndexSize) init() {
	m.data.SetName("postgresql.index_size")
	m.data.SetDescription("Size of this index in bytes (PostgreSQL 9.6+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexSize(cfg MetricConfig) metricPostgresqlIndexSize {
	m := metricPostgresqlIndexSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexTuplesFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_tuples_fetched metric with initial data.
func (m *metricPostgresqlIndexTuplesFetched) init() {
	m.data.SetName("postgresql.index_tuples_fetched")
	m.data.SetDescription("Number of live table rows fetched by simple index scans using this index (PostgreSQL 9.6+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexTuplesFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexTuplesFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexTuplesFetched) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexTuplesFetched(cfg MetricConfig) metricPostgresqlIndexTuplesFetched {
	m := metricPostgresqlIndexTuplesFetched{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexTuplesRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index_tuples_read metric with initial data.
func (m *metricPostgresqlIndexTuplesRead) init() {
	m.data.SetName("postgresql.index_tuples_read")
	m.data.SetDescription("Number of index entries returned by scans on this index (PostgreSQL 9.6+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexTuplesRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
	dp.Attributes().PutStr("index_name", indexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexTuplesRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexTuplesRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexTuplesRead(cfg MetricConfig) metricPostgresqlIndexTuplesRead {
	m := metricPostgresqlIndexTuplesRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlLastAnalyzeAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.last_analyze_age metric with initial data.
func (m *metricPostgresqlLastAnalyzeAge) init() {
	m.data.SetName("postgresql.last_analyze_age")
	m.data.SetDescription("Seconds since last manual ANALYZE on this table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlLastAnalyzeAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlLastAnalyzeAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlLastAnalyzeAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlLastAnalyzeAge(cfg MetricConfig) metricPostgresqlLastAnalyzeAge {
	m := metricPostgresqlLastAnalyzeAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlLastAutoanalyzeAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.last_autoanalyze_age metric with initial data.
func (m *metricPostgresqlLastAutoanalyzeAge) init() {
	m.data.SetName("postgresql.last_autoanalyze_age")
	m.data.SetDescription("Seconds since last autoanalyze on this table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlLastAutoanalyzeAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlLastAutoanalyzeAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlLastAutoanalyzeAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlLastAutoanalyzeAge(cfg MetricConfig) metricPostgresqlLastAutoanalyzeAge {
	m := metricPostgresqlLastAutoanalyzeAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlLastAutovacuumAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.last_autovacuum_age metric with initial data.
func (m *metricPostgresqlLastAutovacuumAge) init() {
	m.data.SetName("postgresql.last_autovacuum_age")
	m.data.SetDescription("Seconds since last autovacuum on this table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlLastAutovacuumAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlLastAutovacuumAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlLastAutovacuumAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlLastAutovacuumAge(cfg MetricConfig) metricPostgresqlLastAutovacuumAge {
	m := metricPostgresqlLastAutovacuumAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlLastVacuumAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.last_vacuum_age metric with initial data.
func (m *metricPostgresqlLastVacuumAge) init() {
	m.data.SetName("postgresql.last_vacuum_age")
	m.data.SetDescription("Seconds since last manual VACUUM on this table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlLastVacuumAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlLastVacuumAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlLastVacuumAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlLastVacuumAge(cfg MetricConfig) metricPostgresqlLastVacuumAge {
	m := metricPostgresqlLastVacuumAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlMaxConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.max_connections metric with initial data.
func (m *metricPostgresqlMaxConnections) init() {
	m.data.SetName("postgresql.max_connections")
	m.data.SetDescription("Maximum number of concurrent connections allowed to the server (PostgreSQL 9.6+)")
	m.data.SetUnit("{connections}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlMaxConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlMaxConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlMaxConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlMaxConnections(cfg MetricConfig) metricPostgresqlMaxConnections {
	m := metricPostgresqlMaxConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlPercentUsageConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.percent_usage_connections metric with initial data.
func (m *metricPostgresqlPercentUsageConnections) init() {
	m.data.SetName("postgresql.percent_usage_connections")
	m.data.SetDescription("Percentage of max_connections currently in use (PostgreSQL 9.6+)")
	m.data.SetUnit("%")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlPercentUsageConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlPercentUsageConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlPercentUsageConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlPercentUsageConnections(cfg MetricConfig) metricPostgresqlPercentUsageConnections {
	m := metricPostgresqlPercentUsageConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlPgStatStatementsDealloc struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.pg_stat_statements.dealloc metric with initial data.
func (m *metricPostgresqlPgStatStatementsDealloc) init() {
	m.data.SetName("postgresql.pg_stat_statements.dealloc")
	m.data.SetDescription("Number of times pg_stat_statements has deallocated least-used statements (requires pg_stat_statements extension, PostgreSQL 13+)")
	m.data.SetUnit("{deallocations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlPgStatStatementsDealloc) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlPgStatStatementsDealloc) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlPgStatStatementsDealloc) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlPgStatStatementsDealloc(cfg MetricConfig) metricPostgresqlPgStatStatementsDealloc {
	m := metricPostgresqlPgStatStatementsDealloc{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchBlockDistance struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.block_distance metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchBlockDistance) init() {
	m.data.SetName("postgresql.recovery_prefetch.block_distance")
	m.data.SetDescription("Number of blocks between the current replay position and the prefetch position (PostgreSQL 15+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchBlockDistance) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchBlockDistance) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchBlockDistance) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchBlockDistance(cfg MetricConfig) metricPostgresqlRecoveryPrefetchBlockDistance {
	m := metricPostgresqlRecoveryPrefetchBlockDistance{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.hit metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchHit) init() {
	m.data.SetName("postgresql.recovery_prefetch.hit")
	m.data.SetDescription("Number of prefetch requests that hit the buffer cache (PostgreSQL 15+)")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchHit(cfg MetricConfig) metricPostgresqlRecoveryPrefetchHit {
	m := metricPostgresqlRecoveryPrefetchHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchIoDepth struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.io_depth metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchIoDepth) init() {
	m.data.SetName("postgresql.recovery_prefetch.io_depth")
	m.data.SetDescription("Number of I/O operations in progress (PostgreSQL 15+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchIoDepth) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchIoDepth) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchIoDepth) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchIoDepth(cfg MetricConfig) metricPostgresqlRecoveryPrefetchIoDepth {
	m := metricPostgresqlRecoveryPrefetchIoDepth{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchPrefetch struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.prefetch metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchPrefetch) init() {
	m.data.SetName("postgresql.recovery_prefetch.prefetch")
	m.data.SetDescription("Number of blocks prefetched during recovery (PostgreSQL 15+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchPrefetch) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchPrefetch) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchPrefetch) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchPrefetch(cfg MetricConfig) metricPostgresqlRecoveryPrefetchPrefetch {
	m := metricPostgresqlRecoveryPrefetchPrefetch{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchSkipFpw struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.skip_fpw metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchSkipFpw) init() {
	m.data.SetName("postgresql.recovery_prefetch.skip_fpw")
	m.data.SetDescription("Number of prefetch requests skipped because a full page write was found (PostgreSQL 15+)")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchSkipFpw) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchSkipFpw) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchSkipFpw) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchSkipFpw(cfg MetricConfig) metricPostgresqlRecoveryPrefetchSkipFpw {
	m := metricPostgresqlRecoveryPrefetchSkipFpw{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchSkipInit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.skip_init metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchSkipInit) init() {
	m.data.SetName("postgresql.recovery_prefetch.skip_init")
	m.data.SetDescription("Number of prefetch requests skipped because the relation was being initialized (PostgreSQL 15+)")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchSkipInit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchSkipInit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchSkipInit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchSkipInit(cfg MetricConfig) metricPostgresqlRecoveryPrefetchSkipInit {
	m := metricPostgresqlRecoveryPrefetchSkipInit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchSkipNew struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.skip_new metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchSkipNew) init() {
	m.data.SetName("postgresql.recovery_prefetch.skip_new")
	m.data.SetDescription("Number of prefetch requests skipped because the relation did not exist yet (PostgreSQL 15+)")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchSkipNew) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchSkipNew) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchSkipNew) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchSkipNew(cfg MetricConfig) metricPostgresqlRecoveryPrefetchSkipNew {
	m := metricPostgresqlRecoveryPrefetchSkipNew{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchSkipRep struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.skip_rep metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchSkipRep) init() {
	m.data.SetName("postgresql.recovery_prefetch.skip_rep")
	m.data.SetDescription("Number of prefetch requests skipped because they were already in progress (PostgreSQL 15+)")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchSkipRep) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchSkipRep) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchSkipRep) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchSkipRep(cfg MetricConfig) metricPostgresqlRecoveryPrefetchSkipRep {
	m := metricPostgresqlRecoveryPrefetchSkipRep{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRecoveryPrefetchWalDistance struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.recovery_prefetch.wal_distance metric with initial data.
func (m *metricPostgresqlRecoveryPrefetchWalDistance) init() {
	m.data.SetName("postgresql.recovery_prefetch.wal_distance")
	m.data.SetDescription("Distance in WAL bytes between the current replay position and the prefetch position (PostgreSQL 15+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRecoveryPrefetchWalDistance) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRecoveryPrefetchWalDistance) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRecoveryPrefetchWalDistance) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRecoveryPrefetchWalDistance(cfg MetricConfig) metricPostgresqlRecoveryPrefetchWalDistance {
	m := metricPostgresqlRecoveryPrefetchWalDistance{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRelationAllVisible struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.relation.all_visible metric with initial data.
func (m *metricPostgresqlRelationAllVisible) init() {
	m.data.SetName("postgresql.relation.all_visible")
	m.data.SetDescription("Number of pages marked all-visible in the visibility map (PostgreSQL 9.6+)")
	m.data.SetUnit("{pages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRelationAllVisible) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRelationAllVisible) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRelationAllVisible) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRelationAllVisible(cfg MetricConfig) metricPostgresqlRelationAllVisible {
	m := metricPostgresqlRelationAllVisible{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRelationPages struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.relation.pages metric with initial data.
func (m *metricPostgresqlRelationPages) init() {
	m.data.SetName("postgresql.relation.pages")
	m.data.SetDescription("Number of disk pages in the relation (PostgreSQL 9.6+)")
	m.data.SetUnit("{pages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRelationPages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRelationPages) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRelationPages) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRelationPages(cfg MetricConfig) metricPostgresqlRelationPages {
	m := metricPostgresqlRelationPages{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRelationTuples struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.relation.tuples metric with initial data.
func (m *metricPostgresqlRelationTuples) init() {
	m.data.SetName("postgresql.relation.tuples")
	m.data.SetDescription("Estimated number of live rows in the relation (PostgreSQL 9.6+)")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRelationTuples) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRelationTuples) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRelationTuples) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRelationTuples(cfg MetricConfig) metricPostgresqlRelationTuples {
	m := metricPostgresqlRelationTuples{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRelationXmin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.relation.xmin metric with initial data.
func (m *metricPostgresqlRelationXmin) init() {
	m.data.SetName("postgresql.relation.xmin")
	m.data.SetDescription("Age of the oldest unfrozen transaction ID in the relation (PostgreSQL 9.6+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRelationXmin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRelationXmin) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRelationXmin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRelationXmin(cfg MetricConfig) metricPostgresqlRelationXmin {
	m := metricPostgresqlRelationXmin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRelationSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.relation_size metric with initial data.
func (m *metricPostgresqlRelationSize) init() {
	m.data.SetName("postgresql.relation_size")
	m.data.SetDescription("Size of the table data (heap) in bytes (PostgreSQL 9.6+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRelationSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRelationSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRelationSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRelationSize(cfg MetricConfig) metricPostgresqlRelationSize {
	m := metricPostgresqlRelationSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationBackendXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.backend_xmin_age metric with initial data.
func (m *metricPostgresqlReplicationBackendXminAge) init() {
	m.data.SetName("postgresql.replication.backend_xmin_age")
	m.data.SetDescription("Age of the oldest transaction on the standby server that is holding back vacuum")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationBackendXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationBackendXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationBackendXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationBackendXminAge(cfg MetricConfig) metricPostgresqlReplicationBackendXminAge {
	m := metricPostgresqlReplicationBackendXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationFlushLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.flush_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationFlushLsnDelay) init() {
	m.data.SetName("postgresql.replication.flush_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL flushed but not yet applied on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationFlushLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationFlushLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationFlushLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationFlushLsnDelay(cfg MetricConfig) metricPostgresqlReplicationFlushLsnDelay {
	m := metricPostgresqlReplicationFlushLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationReplayLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.replay_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationReplayLsnDelay) init() {
	m.data.SetName("postgresql.replication.replay_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL not yet replayed on standby (total replication lag in bytes)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationReplayLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationReplayLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationReplayLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationReplayLsnDelay(cfg MetricConfig) metricPostgresqlReplicationReplayLsnDelay {
	m := metricPostgresqlReplicationReplayLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSentLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.sent_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationSentLsnDelay) init() {
	m.data.SetName("postgresql.replication.sent_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL sent but not yet written to disk on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSentLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSentLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSentLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSentLsnDelay(cfg MetricConfig) metricPostgresqlReplicationSentLsnDelay {
	m := metricPostgresqlReplicationSentLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalFlushLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_flush_lag metric with initial data.
func (m *metricPostgresqlReplicationWalFlushLag) init() {
	m.data.SetName("postgresql.replication.wal_flush_lag")
	m.data.SetDescription("Time elapsed between WAL flush on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalFlushLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalFlushLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalFlushLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalFlushLag(cfg MetricConfig) metricPostgresqlReplicationWalFlushLag {
	m := metricPostgresqlReplicationWalFlushLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalReplayLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_replay_lag metric with initial data.
func (m *metricPostgresqlReplicationWalReplayLag) init() {
	m.data.SetName("postgresql.replication.wal_replay_lag")
	m.data.SetDescription("Time elapsed between WAL replay on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalReplayLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalReplayLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalReplayLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalReplayLag(cfg MetricConfig) metricPostgresqlReplicationWalReplayLag {
	m := metricPostgresqlReplicationWalReplayLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalWriteLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_write_lag metric with initial data.
func (m *metricPostgresqlReplicationWalWriteLag) init() {
	m.data.SetName("postgresql.replication.wal_write_lag")
	m.data.SetDescription("Time elapsed between WAL write on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalWriteLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalWriteLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalWriteLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalWriteLag(cfg MetricConfig) metricPostgresqlReplicationWalWriteLag {
	m := metricPostgresqlReplicationWalWriteLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWriteLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.write_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationWriteLsnDelay) init() {
	m.data.SetName("postgresql.replication.write_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL written but not yet flushed on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWriteLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWriteLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWriteLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWriteLsnDelay(cfg MetricConfig) metricPostgresqlReplicationWriteLsnDelay {
	m := metricPostgresqlReplicationWriteLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_delay metric with initial data.
func (m *metricPostgresqlReplicationDelay) init() {
	m.data.SetName("postgresql.replication_delay")
	m.data.SetDescription("Time lag between primary and standby (standby-side metric, PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationDelay(cfg MetricConfig) metricPostgresqlReplicationDelay {
	m := metricPostgresqlReplicationDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_delay_bytes metric with initial data.
func (m *metricPostgresqlReplicationDelayBytes) init() {
	m.data.SetName("postgresql.replication_delay_bytes")
	m.data.SetDescription("Byte lag between WAL received and replayed on standby (standby-side metric, PostgreSQL 9.6+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationDelayBytes(cfg MetricConfig) metricPostgresqlReplicationDelayBytes {
	m := metricPostgresqlReplicationDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotCatalogXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.catalog_xmin_age metric with initial data.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) init() {
	m.data.SetName("postgresql.replication_slot.catalog_xmin_age")
	m.data.SetDescription("Age of oldest transaction affecting system catalogs that this slot needs to keep (PostgreSQL 9.4+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotCatalogXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotCatalogXminAge(cfg MetricConfig) metricPostgresqlReplicationSlotCatalogXminAge {
	m := metricPostgresqlReplicationSlotCatalogXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotConfirmedFlushDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.confirmed_flush_delay_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) init() {
	m.data.SetName("postgresql.replication_slot.confirmed_flush_delay_bytes")
	m.data.SetDescription("Number of bytes between current WAL position and confirmed_flush_lsn (logical slots only, PostgreSQL 9.4+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotConfirmedFlushDelayBytes(cfg MetricConfig) metricPostgresqlReplicationSlotConfirmedFlushDelayBytes {
	m := metricPostgresqlReplicationSlotConfirmedFlushDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotRestartDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.restart_delay_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) init() {
	m.data.SetName("postgresql.replication_slot.restart_delay_bytes")
	m.data.SetDescription("Number of bytes of WAL between current position and slot's restart_lsn (PostgreSQL 9.4+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotRestartDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotRestartDelayBytes(cfg MetricConfig) metricPostgresqlReplicationSlotRestartDelayBytes {
	m := metricPostgresqlReplicationSlotRestartDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillBytes) init() {
	m.data.SetName("postgresql.replication_slot.spill_bytes")
	m.data.SetDescription("Total amount of data spilled to disk for logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillBytes(cfg MetricConfig) metricPostgresqlReplicationSlotSpillBytes {
	m := metricPostgresqlReplicationSlotSpillBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_count metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillCount) init() {
	m.data.SetName("postgresql.replication_slot.spill_count")
	m.data.SetDescription("Number of times transactions were spilled to disk during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{spills}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillCount(cfg MetricConfig) metricPostgresqlReplicationSlotSpillCount {
	m := metricPostgresqlReplicationSlotSpillCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillTxns) init() {
	m.data.SetName("postgresql.replication_slot.spill_txns")
	m.data.SetDescription("Number of transactions spilled to disk during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillTxns(cfg MetricConfig) metricPostgresqlReplicationSlotSpillTxns {
	m := metricPostgresqlReplicationSlotSpillTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamBytes) init() {
	m.data.SetName("postgresql.replication_slot.stream_bytes")
	m.data.SetDescription("Total amount of data streamed for in-progress transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamBytes(cfg MetricConfig) metricPostgresqlReplicationSlotStreamBytes {
	m := metricPostgresqlReplicationSlotStreamBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_count metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamCount) init() {
	m.data.SetName("postgresql.replication_slot.stream_count")
	m.data.SetDescription("Number of times in-progress transactions were streamed during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{streams}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamCount(cfg MetricConfig) metricPostgresqlReplicationSlotStreamCount {
	m := metricPostgresqlReplicationSlotStreamCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamTxns) init() {
	m.data.SetName("postgresql.replication_slot.stream_txns")
	m.data.SetDescription("Number of in-progress transactions streamed during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamTxns(cfg MetricConfig) metricPostgresqlReplicationSlotStreamTxns {
	m := metricPostgresqlReplicationSlotStreamTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotTotalBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.total_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotTotalBytes) init() {
	m.data.SetName("postgresql.replication_slot.total_bytes")
	m.data.SetDescription("Total amount of data decoded for transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotTotalBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotTotalBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotTotalBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotTotalBytes(cfg MetricConfig) metricPostgresqlReplicationSlotTotalBytes {
	m := metricPostgresqlReplicationSlotTotalBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotTotalTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.total_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotTotalTxns) init() {
	m.data.SetName("postgresql.replication_slot.total_txns")
	m.data.SetDescription("Total number of decoded transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotTotalTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotTotalTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotTotalTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotTotalTxns(cfg MetricConfig) metricPostgresqlReplicationSlotTotalTxns {
	m := metricPostgresqlReplicationSlotTotalTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.xmin_age metric with initial data.
func (m *metricPostgresqlReplicationSlotXminAge) init() {
	m.data.SetName("postgresql.replication_slot.xmin_age")
	m.data.SetDescription("Age of oldest transaction that this replication slot needs to keep (PostgreSQL 9.4+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotXminAge(cfg MetricConfig) metricPostgresqlReplicationSlotXminAge {
	m := metricPostgresqlReplicationSlotXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRollbacks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rollbacks metric with initial data.
func (m *metricPostgresqlRollbacks) init() {
	m.data.SetName("postgresql.rollbacks")
	m.data.SetDescription("Number of transactions that have been rolled back")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRollbacks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRollbacks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRollbacks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRollbacks(cfg MetricConfig) metricPostgresqlRollbacks {
	m := metricPostgresqlRollbacks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsDeleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_deleted metric with initial data.
func (m *metricPostgresqlRowsDeleted) init() {
	m.data.SetName("postgresql.rows_deleted")
	m.data.SetDescription("Number of rows deleted")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsDeleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsDeleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsDeleted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsDeleted(cfg MetricConfig) metricPostgresqlRowsDeleted {
	m := metricPostgresqlRowsDeleted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_fetched metric with initial data.
func (m *metricPostgresqlRowsFetched) init() {
	m.data.SetName("postgresql.rows_fetched")
	m.data.SetDescription("Number of rows fetched by queries")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsFetched) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsFetched(cfg MetricConfig) metricPostgresqlRowsFetched {
	m := metricPostgresqlRowsFetched{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsInserted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_inserted metric with initial data.
func (m *metricPostgresqlRowsInserted) init() {
	m.data.SetName("postgresql.rows_inserted")
	m.data.SetDescription("Number of rows inserted")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsInserted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsInserted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsInserted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsInserted(cfg MetricConfig) metricPostgresqlRowsInserted {
	m := metricPostgresqlRowsInserted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsReturned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_returned metric with initial data.
func (m *metricPostgresqlRowsReturned) init() {
	m.data.SetName("postgresql.rows_returned")
	m.data.SetDescription("Number of rows returned by queries")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsReturned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsReturned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsReturned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsReturned(cfg MetricConfig) metricPostgresqlRowsReturned {
	m := metricPostgresqlRowsReturned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsUpdated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_updated metric with initial data.
func (m *metricPostgresqlRowsUpdated) init() {
	m.data.SetName("postgresql.rows_updated")
	m.data.SetDescription("Number of rows updated")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsUpdated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsUpdated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsUpdated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsUpdated(cfg MetricConfig) metricPostgresqlRowsUpdated {
	m := metricPostgresqlRowsUpdated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.running metric with initial data.
func (m *metricPostgresqlRunning) init() {
	m.data.SetName("postgresql.running")
	m.data.SetDescription("PostgreSQL server running status health check (PostgreSQL 9.6+)")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRunning(cfg MetricConfig) metricPostgresqlRunning {
	m := metricPostgresqlRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsAbandoned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.abandoned metric with initial data.
func (m *metricPostgresqlSessionsAbandoned) init() {
	m.data.SetName("postgresql.sessions.abandoned")
	m.data.SetDescription("Number of sessions abandoned due to client disconnection (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsAbandoned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsAbandoned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsAbandoned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsAbandoned(cfg MetricConfig) metricPostgresqlSessionsAbandoned {
	m := metricPostgresqlSessionsAbandoned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsActiveTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.active_time metric with initial data.
func (m *metricPostgresqlSessionsActiveTime) init() {
	m.data.SetName("postgresql.sessions.active_time")
	m.data.SetDescription("Time spent executing queries in this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsActiveTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsActiveTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsActiveTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsActiveTime(cfg MetricConfig) metricPostgresqlSessionsActiveTime {
	m := metricPostgresqlSessionsActiveTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.count metric with initial data.
func (m *metricPostgresqlSessionsCount) init() {
	m.data.SetName("postgresql.sessions.count")
	m.data.SetDescription("Number of sessions established to this database (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsCount(cfg MetricConfig) metricPostgresqlSessionsCount {
	m := metricPostgresqlSessionsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsFatal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.fatal metric with initial data.
func (m *metricPostgresqlSessionsFatal) init() {
	m.data.SetName("postgresql.sessions.fatal")
	m.data.SetDescription("Number of sessions terminated by fatal errors (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsFatal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsFatal) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsFatal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsFatal(cfg MetricConfig) metricPostgresqlSessionsFatal {
	m := metricPostgresqlSessionsFatal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsIdleInTransactionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.idle_in_transaction_time metric with initial data.
func (m *metricPostgresqlSessionsIdleInTransactionTime) init() {
	m.data.SetName("postgresql.sessions.idle_in_transaction_time")
	m.data.SetDescription("Time spent idle in transactions in this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsIdleInTransactionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsIdleInTransactionTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsIdleInTransactionTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsIdleInTransactionTime(cfg MetricConfig) metricPostgresqlSessionsIdleInTransactionTime {
	m := metricPostgresqlSessionsIdleInTransactionTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsKilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.killed metric with initial data.
func (m *metricPostgresqlSessionsKilled) init() {
	m.data.SetName("postgresql.sessions.killed")
	m.data.SetDescription("Number of sessions terminated by operator intervention (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsKilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsKilled) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsKilled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsKilled(cfg MetricConfig) metricPostgresqlSessionsKilled {
	m := metricPostgresqlSessionsKilled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsSessionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.session_time metric with initial data.
func (m *metricPostgresqlSessionsSessionTime) init() {
	m.data.SetName("postgresql.sessions.session_time")
	m.data.SetDescription("Time spent in sessions for this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsSessionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsSessionTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsSessionTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsSessionTime(cfg MetricConfig) metricPostgresqlSessionsSessionTime {
	m := metricPostgresqlSessionsSessionTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruBlksExists struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.blks_exists metric with initial data.
func (m *metricPostgresqlSlruBlksExists) init() {
	m.data.SetName("postgresql.slru.blks_exists")
	m.data.SetDescription("Number of blocks checked for existence for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruBlksExists) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruBlksExists) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruBlksExists) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruBlksExists(cfg MetricConfig) metricPostgresqlSlruBlksExists {
	m := metricPostgresqlSlruBlksExists{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruBlksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.blks_hit metric with initial data.
func (m *metricPostgresqlSlruBlksHit) init() {
	m.data.SetName("postgresql.slru.blks_hit")
	m.data.SetDescription("Number of times disk blocks were found already in the SLRU cache (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruBlksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruBlksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruBlksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruBlksHit(cfg MetricConfig) metricPostgresqlSlruBlksHit {
	m := metricPostgresqlSlruBlksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruBlksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.blks_read metric with initial data.
func (m *metricPostgresqlSlruBlksRead) init() {
	m.data.SetName("postgresql.slru.blks_read")
	m.data.SetDescription("Number of disk blocks read for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruBlksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruBlksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruBlksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruBlksRead(cfg MetricConfig) metricPostgresqlSlruBlksRead {
	m := metricPostgresqlSlruBlksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruBlksWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.blks_written metric with initial data.
func (m *metricPostgresqlSlruBlksWritten) init() {
	m.data.SetName("postgresql.slru.blks_written")
	m.data.SetDescription("Number of disk blocks written for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruBlksWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruBlksWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruBlksWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruBlksWritten(cfg MetricConfig) metricPostgresqlSlruBlksWritten {
	m := metricPostgresqlSlruBlksWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruBlksZeroed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.blks_zeroed metric with initial data.
func (m *metricPostgresqlSlruBlksZeroed) init() {
	m.data.SetName("postgresql.slru.blks_zeroed")
	m.data.SetDescription("Number of blocks zeroed during initializations for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruBlksZeroed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruBlksZeroed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruBlksZeroed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruBlksZeroed(cfg MetricConfig) metricPostgresqlSlruBlksZeroed {
	m := metricPostgresqlSlruBlksZeroed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruFlushes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.flushes metric with initial data.
func (m *metricPostgresqlSlruFlushes) init() {
	m.data.SetName("postgresql.slru.flushes")
	m.data.SetDescription("Number of flushes of dirty data for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{flushes}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruFlushes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruFlushes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruFlushes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruFlushes(cfg MetricConfig) metricPostgresqlSlruFlushes {
	m := metricPostgresqlSlruFlushes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSlruTruncates struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.slru.truncates metric with initial data.
func (m *metricPostgresqlSlruTruncates) init() {
	m.data.SetName("postgresql.slru.truncates")
	m.data.SetDescription("Number of truncates for this SLRU (PostgreSQL 13+)")
	m.data.SetUnit("{truncates}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSlruTruncates) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("slru_name", slruNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSlruTruncates) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSlruTruncates) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSlruTruncates(cfg MetricConfig) metricPostgresqlSlruTruncates {
	m := metricPostgresqlSlruTruncates{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSnapshotXipCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.snapshot.xip_count metric with initial data.
func (m *metricPostgresqlSnapshotXipCount) init() {
	m.data.SetName("postgresql.snapshot.xip_count")
	m.data.SetDescription("Number of in-progress transactions in the current snapshot (PostgreSQL 13+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSnapshotXipCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSnapshotXipCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSnapshotXipCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSnapshotXipCount(cfg MetricConfig) metricPostgresqlSnapshotXipCount {
	m := metricPostgresqlSnapshotXipCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSnapshotXmax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.snapshot.xmax metric with initial data.
func (m *metricPostgresqlSnapshotXmax) init() {
	m.data.SetName("postgresql.snapshot.xmax")
	m.data.SetDescription("First as-yet-unassigned transaction ID in the current snapshot (PostgreSQL 13+)")
	m.data.SetUnit("{xid}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSnapshotXmax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSnapshotXmax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSnapshotXmax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSnapshotXmax(cfg MetricConfig) metricPostgresqlSnapshotXmax {
	m := metricPostgresqlSnapshotXmax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSnapshotXmin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.snapshot.xmin metric with initial data.
func (m *metricPostgresqlSnapshotXmin) init() {
	m.data.SetName("postgresql.snapshot.xmin")
	m.data.SetDescription("Earliest transaction ID still active in the current snapshot (PostgreSQL 13+)")
	m.data.SetUnit("{xid}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSnapshotXmin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSnapshotXmin) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSnapshotXmin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSnapshotXmin(cfg MetricConfig) metricPostgresqlSnapshotXmin {
	m := metricPostgresqlSnapshotXmin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSubscriptionApplyError struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.subscription.apply_error metric with initial data.
func (m *metricPostgresqlSubscriptionApplyError) init() {
	m.data.SetName("postgresql.subscription.apply_error")
	m.data.SetDescription("Number of errors encountered while applying logical replication changes (PostgreSQL 15+)")
	m.data.SetUnit("{errors}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSubscriptionApplyError) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("subscription_name", subscriptionNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSubscriptionApplyError) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSubscriptionApplyError) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSubscriptionApplyError(cfg MetricConfig) metricPostgresqlSubscriptionApplyError {
	m := metricPostgresqlSubscriptionApplyError{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSubscriptionLastMsgReceiptAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.subscription.last_msg_receipt_age metric with initial data.
func (m *metricPostgresqlSubscriptionLastMsgReceiptAge) init() {
	m.data.SetName("postgresql.subscription.last_msg_receipt_age")
	m.data.SetDescription("Time elapsed since last message received from publisher in logical replication (PostgreSQL 15+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSubscriptionLastMsgReceiptAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("subscription_name", subscriptionNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSubscriptionLastMsgReceiptAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSubscriptionLastMsgReceiptAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSubscriptionLastMsgReceiptAge(cfg MetricConfig) metricPostgresqlSubscriptionLastMsgReceiptAge {
	m := metricPostgresqlSubscriptionLastMsgReceiptAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSubscriptionLastMsgSendAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.subscription.last_msg_send_age metric with initial data.
func (m *metricPostgresqlSubscriptionLastMsgSendAge) init() {
	m.data.SetName("postgresql.subscription.last_msg_send_age")
	m.data.SetDescription("Time elapsed since last message sent from publisher in logical replication (PostgreSQL 15+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSubscriptionLastMsgSendAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("subscription_name", subscriptionNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSubscriptionLastMsgSendAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSubscriptionLastMsgSendAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSubscriptionLastMsgSendAge(cfg MetricConfig) metricPostgresqlSubscriptionLastMsgSendAge {
	m := metricPostgresqlSubscriptionLastMsgSendAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSubscriptionLatestEndAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.subscription.latest_end_age metric with initial data.
func (m *metricPostgresqlSubscriptionLatestEndAge) init() {
	m.data.SetName("postgresql.subscription.latest_end_age")
	m.data.SetDescription("Time elapsed since latest WAL location reported to publisher in logical replication (PostgreSQL 15+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSubscriptionLatestEndAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("subscription_name", subscriptionNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSubscriptionLatestEndAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSubscriptionLatestEndAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSubscriptionLatestEndAge(cfg MetricConfig) metricPostgresqlSubscriptionLatestEndAge {
	m := metricPostgresqlSubscriptionLatestEndAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSubscriptionSyncError struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.subscription.sync_error metric with initial data.
func (m *metricPostgresqlSubscriptionSyncError) init() {
	m.data.SetName("postgresql.subscription.sync_error")
	m.data.SetDescription("Number of errors encountered during initial sync in logical replication (PostgreSQL 15+)")
	m.data.SetUnit("{errors}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSubscriptionSyncError) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("subscription_name", subscriptionNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSubscriptionSyncError) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSubscriptionSyncError) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSubscriptionSyncError(cfg MetricConfig) metricPostgresqlSubscriptionSyncError {
	m := metricPostgresqlSubscriptionSyncError{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTempBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.temp_bytes metric with initial data.
func (m *metricPostgresqlTempBytes) init() {
	m.data.SetName("postgresql.temp_bytes")
	m.data.SetDescription("Total amount of data written to temporary files")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTempBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTempBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTempBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTempBytes(cfg MetricConfig) metricPostgresqlTempBytes {
	m := metricPostgresqlTempBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTempFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.temp_files metric with initial data.
func (m *metricPostgresqlTempFiles) init() {
	m.data.SetName("postgresql.temp_files")
	m.data.SetDescription("Number of temporary files created")
	m.data.SetUnit("{files}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTempFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTempFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTempFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTempFiles(cfg MetricConfig) metricPostgresqlTempFiles {
	m := metricPostgresqlTempFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastAutovacuumed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast.autovacuumed metric with initial data.
func (m *metricPostgresqlToastAutovacuumed) init() {
	m.data.SetName("postgresql.toast.autovacuumed")
	m.data.SetDescription("Number of times this TOAST table has been vacuumed by autovacuum (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastAutovacuumed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastAutovacuumed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastAutovacuumed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastAutovacuumed(cfg MetricConfig) metricPostgresqlToastAutovacuumed {
	m := metricPostgresqlToastAutovacuumed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastLastAutovacuumAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast.last_autovacuum_age metric with initial data.
func (m *metricPostgresqlToastLastAutovacuumAge) init() {
	m.data.SetName("postgresql.toast.last_autovacuum_age")
	m.data.SetDescription("Seconds since last autovacuum on this TOAST table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastLastAutovacuumAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastLastAutovacuumAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastLastAutovacuumAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastLastAutovacuumAge(cfg MetricConfig) metricPostgresqlToastLastAutovacuumAge {
	m := metricPostgresqlToastLastAutovacuumAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastLastVacuumAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast.last_vacuum_age metric with initial data.
func (m *metricPostgresqlToastLastVacuumAge) init() {
	m.data.SetName("postgresql.toast.last_vacuum_age")
	m.data.SetDescription("Seconds since last manual VACUUM on this TOAST table (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastLastVacuumAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastLastVacuumAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastLastVacuumAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastLastVacuumAge(cfg MetricConfig) metricPostgresqlToastLastVacuumAge {
	m := metricPostgresqlToastLastVacuumAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastVacuumed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast.vacuumed metric with initial data.
func (m *metricPostgresqlToastVacuumed) init() {
	m.data.SetName("postgresql.toast.vacuumed")
	m.data.SetDescription("Number of times this TOAST table has been manually vacuumed (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastVacuumed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastVacuumed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastVacuumed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastVacuumed(cfg MetricConfig) metricPostgresqlToastVacuumed {
	m := metricPostgresqlToastVacuumed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast_blocks_hit metric with initial data.
func (m *metricPostgresqlToastBlocksHit) init() {
	m.data.SetName("postgresql.toast_blocks_hit")
	m.data.SetDescription("Number of TOAST blocks read from buffer cache for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastBlocksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastBlocksHit(cfg MetricConfig) metricPostgresqlToastBlocksHit {
	m := metricPostgresqlToastBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast_blocks_read metric with initial data.
func (m *metricPostgresqlToastBlocksRead) init() {
	m.data.SetName("postgresql.toast_blocks_read")
	m.data.SetDescription("Number of TOAST blocks read from disk for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastBlocksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastBlocksRead(cfg MetricConfig) metricPostgresqlToastBlocksRead {
	m := metricPostgresqlToastBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastIndexBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast_index_blocks_hit metric with initial data.
func (m *metricPostgresqlToastIndexBlocksHit) init() {
	m.data.SetName("postgresql.toast_index_blocks_hit")
	m.data.SetDescription("Number of TOAST index blocks read from buffer cache for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastIndexBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastIndexBlocksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastIndexBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastIndexBlocksHit(cfg MetricConfig) metricPostgresqlToastIndexBlocksHit {
	m := metricPostgresqlToastIndexBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastIndexBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast_index_blocks_read metric with initial data.
func (m *metricPostgresqlToastIndexBlocksRead) init() {
	m.data.SetName("postgresql.toast_index_blocks_read")
	m.data.SetDescription("Number of TOAST index blocks read from disk for this table (PostgreSQL 9.6+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastIndexBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastIndexBlocksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastIndexBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastIndexBlocksRead(cfg MetricConfig) metricPostgresqlToastIndexBlocksRead {
	m := metricPostgresqlToastIndexBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlToastSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.toast_size metric with initial data.
func (m *metricPostgresqlToastSize) init() {
	m.data.SetName("postgresql.toast_size")
	m.data.SetDescription("Size of the TOAST data for this table in bytes (PostgreSQL 9.6+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlToastSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlToastSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlToastSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlToastSize(cfg MetricConfig) metricPostgresqlToastSize {
	m := metricPostgresqlToastSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTransactionsDurationMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.transactions.duration.max metric with initial data.
func (m *metricPostgresqlTransactionsDurationMax) init() {
	m.data.SetName("postgresql.transactions.duration.max")
	m.data.SetDescription("Maximum transaction duration in seconds across all active backends (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTransactionsDurationMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTransactionsDurationMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTransactionsDurationMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTransactionsDurationMax(cfg MetricConfig) metricPostgresqlTransactionsDurationMax {
	m := metricPostgresqlTransactionsDurationMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTransactionsDurationSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.transactions.duration.sum metric with initial data.
func (m *metricPostgresqlTransactionsDurationSum) init() {
	m.data.SetName("postgresql.transactions.duration.sum")
	m.data.SetDescription("Sum of transaction durations in seconds across all active backends (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTransactionsDurationSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("backend_type", backendTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTransactionsDurationSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTransactionsDurationSum) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTransactionsDurationSum(cfg MetricConfig) metricPostgresqlTransactionsDurationSum {
	m := metricPostgresqlTransactionsDurationSum{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlUptime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.uptime metric with initial data.
func (m *metricPostgresqlUptime) init() {
	m.data.SetName("postgresql.uptime")
	m.data.SetDescription("PostgreSQL server uptime in seconds (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlUptime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlUptime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlUptime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlUptime(cfg MetricConfig) metricPostgresqlUptime {
	m := metricPostgresqlUptime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumHeapBlksScanned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.heap_blks_scanned metric with initial data.
func (m *metricPostgresqlVacuumHeapBlksScanned) init() {
	m.data.SetName("postgresql.vacuum.heap_blks_scanned")
	m.data.SetDescription("Number of heap blocks scanned during VACUUM operation (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumHeapBlksScanned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumHeapBlksScanned) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumHeapBlksScanned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumHeapBlksScanned(cfg MetricConfig) metricPostgresqlVacuumHeapBlksScanned {
	m := metricPostgresqlVacuumHeapBlksScanned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumHeapBlksTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.heap_blks_total metric with initial data.
func (m *metricPostgresqlVacuumHeapBlksTotal) init() {
	m.data.SetName("postgresql.vacuum.heap_blks_total")
	m.data.SetDescription("Total number of heap blocks to be scanned during VACUUM (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumHeapBlksTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumHeapBlksTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumHeapBlksTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumHeapBlksTotal(cfg MetricConfig) metricPostgresqlVacuumHeapBlksTotal {
	m := metricPostgresqlVacuumHeapBlksTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumHeapBlksVacuumed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.heap_blks_vacuumed metric with initial data.
func (m *metricPostgresqlVacuumHeapBlksVacuumed) init() {
	m.data.SetName("postgresql.vacuum.heap_blks_vacuumed")
	m.data.SetDescription("Number of heap blocks vacuumed during VACUUM operation (PostgreSQL 12+)")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumHeapBlksVacuumed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumHeapBlksVacuumed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumHeapBlksVacuumed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumHeapBlksVacuumed(cfg MetricConfig) metricPostgresqlVacuumHeapBlksVacuumed {
	m := metricPostgresqlVacuumHeapBlksVacuumed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumIndexVacuumCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.index_vacuum_count metric with initial data.
func (m *metricPostgresqlVacuumIndexVacuumCount) init() {
	m.data.SetName("postgresql.vacuum.index_vacuum_count")
	m.data.SetDescription("Number of completed index vacuum cycles during VACUUM operation (PostgreSQL 12+)")
	m.data.SetUnit("{cycles}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumIndexVacuumCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumIndexVacuumCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumIndexVacuumCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumIndexVacuumCount(cfg MetricConfig) metricPostgresqlVacuumIndexVacuumCount {
	m := metricPostgresqlVacuumIndexVacuumCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumMaxDeadTuples struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.max_dead_tuples metric with initial data.
func (m *metricPostgresqlVacuumMaxDeadTuples) init() {
	m.data.SetName("postgresql.vacuum.max_dead_tuples")
	m.data.SetDescription("Maximum number of dead tuples that can be stored before index vacuum is required (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumMaxDeadTuples) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumMaxDeadTuples) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumMaxDeadTuples) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumMaxDeadTuples(cfg MetricConfig) metricPostgresqlVacuumMaxDeadTuples {
	m := metricPostgresqlVacuumMaxDeadTuples{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumNumDeadTuples struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuum.num_dead_tuples metric with initial data.
func (m *metricPostgresqlVacuumNumDeadTuples) init() {
	m.data.SetName("postgresql.vacuum.num_dead_tuples")
	m.data.SetDescription("Current number of dead tuples collected during VACUUM operation (PostgreSQL 12+)")
	m.data.SetUnit("{tuples}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumNumDeadTuples) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumNumDeadTuples) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumNumDeadTuples) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumNumDeadTuples(cfg MetricConfig) metricPostgresqlVacuumNumDeadTuples {
	m := metricPostgresqlVacuumNumDeadTuples{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlVacuumed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.vacuumed metric with initial data.
func (m *metricPostgresqlVacuumed) init() {
	m.data.SetName("postgresql.vacuumed")
	m.data.SetDescription("Number of times this table has been manually vacuumed (PostgreSQL 9.6+)")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlVacuumed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("table_name", tableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlVacuumed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlVacuumed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlVacuumed(cfg MetricConfig) metricPostgresqlVacuumed {
	m := metricPostgresqlVacuumed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalBuffersFull struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.buffers_full metric with initial data.
func (m *metricPostgresqlWalBuffersFull) init() {
	m.data.SetName("postgresql.wal.buffers_full")
	m.data.SetDescription("Number of times WAL data was written because WAL buffers became full (PostgreSQL 14+)")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalBuffersFull) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalBuffersFull) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalBuffersFull) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalBuffersFull(cfg MetricConfig) metricPostgresqlWalBuffersFull {
	m := metricPostgresqlWalBuffersFull{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.bytes metric with initial data.
func (m *metricPostgresqlWalBytes) init() {
	m.data.SetName("postgresql.wal.bytes")
	m.data.SetDescription("Total amount of WAL bytes generated (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalBytes(cfg MetricConfig) metricPostgresqlWalBytes {
	m := metricPostgresqlWalBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalFpi struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.fpi metric with initial data.
func (m *metricPostgresqlWalFpi) init() {
	m.data.SetName("postgresql.wal.fpi")
	m.data.SetDescription("Total number of WAL full page images generated (PostgreSQL 14+)")
	m.data.SetUnit("{images}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalFpi) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalFpi) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalFpi) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalFpi(cfg MetricConfig) metricPostgresqlWalFpi {
	m := metricPostgresqlWalFpi{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.records metric with initial data.
func (m *metricPostgresqlWalRecords) init() {
	m.data.SetName("postgresql.wal.records")
	m.data.SetDescription("Total number of WAL records generated (PostgreSQL 14+)")
	m.data.SetUnit("{records}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalRecords) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalRecords) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalRecords(cfg MetricConfig) metricPostgresqlWalRecords {
	m := metricPostgresqlWalRecords{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalSync struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.sync metric with initial data.
func (m *metricPostgresqlWalSync) init() {
	m.data.SetName("postgresql.wal.sync")
	m.data.SetDescription("Number of times WAL files were synced to disk (PostgreSQL 14-17)")
	m.data.SetUnit("{syncs}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalSync) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalSync) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalSync) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalSync(cfg MetricConfig) metricPostgresqlWalSync {
	m := metricPostgresqlWalSync{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalSyncTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.sync_time metric with initial data.
func (m *metricPostgresqlWalSyncTime) init() {
	m.data.SetName("postgresql.wal.sync_time")
	m.data.SetDescription("Total time spent syncing WAL files to disk (PostgreSQL 14-17)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalSyncTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalSyncTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalSyncTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalSyncTime(cfg MetricConfig) metricPostgresqlWalSyncTime {
	m := metricPostgresqlWalSyncTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalWrite struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.write metric with initial data.
func (m *metricPostgresqlWalWrite) init() {
	m.data.SetName("postgresql.wal.write")
	m.data.SetDescription("Number of times WAL buffers were written to disk (PostgreSQL 14-17)")
	m.data.SetUnit("{writes}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalWrite) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalWrite) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalWrite) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalWrite(cfg MetricConfig) metricPostgresqlWalWrite {
	m := metricPostgresqlWalWrite{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.write_time metric with initial data.
func (m *metricPostgresqlWalWriteTime) init() {
	m.data.SetName("postgresql.wal.write_time")
	m.data.SetDescription("Total time spent writing WAL buffers to disk (PostgreSQL 14-17)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalWriteTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalWriteTime(cfg MetricConfig) metricPostgresqlWalWriteTime {
	m := metricPostgresqlWalWriteTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalFilesAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_files.age metric with initial data.
func (m *metricPostgresqlWalFilesAge) init() {
	m.data.SetName("postgresql.wal_files.age")
	m.data.SetDescription("Age of the oldest WAL file in seconds (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalFilesAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalFilesAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalFilesAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalFilesAge(cfg MetricConfig) metricPostgresqlWalFilesAge {
	m := metricPostgresqlWalFilesAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalFilesCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_files.count metric with initial data.
func (m *metricPostgresqlWalFilesCount) init() {
	m.data.SetName("postgresql.wal_files.count")
	m.data.SetDescription("Total number of WAL files in the pg_wal directory (PostgreSQL 10+)")
	m.data.SetUnit("{files}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalFilesCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalFilesCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalFilesCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalFilesCount(cfg MetricConfig) metricPostgresqlWalFilesCount {
	m := metricPostgresqlWalFilesCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalFilesSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_files.size metric with initial data.
func (m *metricPostgresqlWalFilesSize) init() {
	m.data.SetName("postgresql.wal_files.size")
	m.data.SetDescription("Total size of all WAL files in bytes (PostgreSQL 10+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalFilesSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalFilesSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalFilesSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalFilesSize(cfg MetricConfig) metricPostgresqlWalFilesSize {
	m := metricPostgresqlWalFilesSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalReceiverConnected struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_receiver.connected metric with initial data.
func (m *metricPostgresqlWalReceiverConnected) init() {
	m.data.SetName("postgresql.wal_receiver.connected")
	m.data.SetDescription("Whether WAL receiver is connected (1 if streaming, 0 otherwise, PostgreSQL 9.6+)")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalReceiverConnected) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalReceiverConnected) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalReceiverConnected) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalReceiverConnected(cfg MetricConfig) metricPostgresqlWalReceiverConnected {
	m := metricPostgresqlWalReceiverConnected{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalReceiverLastMsgReceiptAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_receiver.last_msg_receipt_age metric with initial data.
func (m *metricPostgresqlWalReceiverLastMsgReceiptAge) init() {
	m.data.SetName("postgresql.wal_receiver.last_msg_receipt_age")
	m.data.SetDescription("Time elapsed since last message received by WAL receiver from primary (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalReceiverLastMsgReceiptAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalReceiverLastMsgReceiptAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalReceiverLastMsgReceiptAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalReceiverLastMsgReceiptAge(cfg MetricConfig) metricPostgresqlWalReceiverLastMsgReceiptAge {
	m := metricPostgresqlWalReceiverLastMsgReceiptAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalReceiverLastMsgSendAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_receiver.last_msg_send_age metric with initial data.
func (m *metricPostgresqlWalReceiverLastMsgSendAge) init() {
	m.data.SetName("postgresql.wal_receiver.last_msg_send_age")
	m.data.SetDescription("Time elapsed since last message sent from primary to WAL receiver (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalReceiverLastMsgSendAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalReceiverLastMsgSendAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalReceiverLastMsgSendAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalReceiverLastMsgSendAge(cfg MetricConfig) metricPostgresqlWalReceiverLastMsgSendAge {
	m := metricPostgresqlWalReceiverLastMsgSendAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalReceiverLatestEndAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_receiver.latest_end_age metric with initial data.
func (m *metricPostgresqlWalReceiverLatestEndAge) init() {
	m.data.SetName("postgresql.wal_receiver.latest_end_age")
	m.data.SetDescription("Time elapsed since last WAL location reported back to primary by WAL receiver (PostgreSQL 9.6+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalReceiverLatestEndAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalReceiverLatestEndAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalReceiverLatestEndAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalReceiverLatestEndAge(cfg MetricConfig) metricPostgresqlWalReceiverLatestEndAge {
	m := metricPostgresqlWalReceiverLatestEndAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalReceiverReceivedTimeline struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal_receiver.received_timeline metric with initial data.
func (m *metricPostgresqlWalReceiverReceivedTimeline) init() {
	m.data.SetName("postgresql.wal_receiver.received_timeline")
	m.data.SetDescription("Timeline number of last WAL file received and synced to disk by WAL receiver (PostgreSQL 9.6+)")
	m.data.SetUnit("{timeline}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalReceiverReceivedTimeline) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalReceiverReceivedTimeline) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalReceiverReceivedTimeline) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalReceiverReceivedTimeline(cfg MetricConfig) metricPostgresqlWalReceiverReceivedTimeline {
	m := metricPostgresqlWalReceiverReceivedTimeline{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                                  MetricsBuilderConfig // config of the metrics builder.
	startTime                                               pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                         int                  // maximum observed number of metrics per resource.
	metricsBuffer                                           pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                               component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter                          map[string]filter.Filter
	resourceAttributeExcludeFilter                          map[string]filter.Filter
	metricPgbouncerStatsAvgBytesIn                          metricPgbouncerStatsAvgBytesIn
	metricPgbouncerStatsAvgBytesOut                         metricPgbouncerStatsAvgBytesOut
	metricPgbouncerStatsAvgRequestsPerSecond                metricPgbouncerStatsAvgRequestsPerSecond
	metricPgbouncerStatsAvgServerAssignmentCount            metricPgbouncerStatsAvgServerAssignmentCount
	metricPgbouncerStatsAvgTransactionCount                 metricPgbouncerStatsAvgTransactionCount
	metricPgbouncerStatsAvgTransactionDurationMilliseconds  metricPgbouncerStatsAvgTransactionDurationMilliseconds
	metricPgbouncerStatsBytesInPerSecond                    metricPgbouncerStatsBytesInPerSecond
	metricPgbouncerStatsBytesOutPerSecond                   metricPgbouncerStatsBytesOutPerSecond
	metricPgbouncerStatsQueriesPerSecond                    metricPgbouncerStatsQueriesPerSecond
	metricPgbouncerStatsRequestsPerSecond                   metricPgbouncerStatsRequestsPerSecond
	metricPgbouncerStatsTotalServerAssignmentCount          metricPgbouncerStatsTotalServerAssignmentCount
	metricPgbouncerStatsTransactionsPerSecond               metricPgbouncerStatsTransactionsPerSecond
	metricPostgresqlActiveWaitingQueries                    metricPostgresqlActiveWaitingQueries
	metricPostgresqlActivityBackendXidAge                   metricPostgresqlActivityBackendXidAge
	metricPostgresqlActivityBackendXminAge                  metricPostgresqlActivityBackendXminAge
	metricPostgresqlActivityWaitEvent                       metricPostgresqlActivityWaitEvent
	metricPostgresqlActivityXactStartAge                    metricPostgresqlActivityXactStartAge
	metricPostgresqlAnalyzeChildTablesDone                  metricPostgresqlAnalyzeChildTablesDone
	metricPostgresqlAnalyzeChildTablesTotal                 metricPostgresqlAnalyzeChildTablesTotal
	metricPostgresqlAnalyzeExtStatsComputed                 metricPostgresqlAnalyzeExtStatsComputed
	metricPostgresqlAnalyzeExtStatsTotal                    metricPostgresqlAnalyzeExtStatsTotal
	metricPostgresqlAnalyzeSampleBlksScanned                metricPostgresqlAnalyzeSampleBlksScanned
	metricPostgresqlAnalyzeSampleBlksTotal                  metricPostgresqlAnalyzeSampleBlksTotal
	metricPostgresqlAnalyzed                                metricPostgresqlAnalyzed
	metricPostgresqlArchiverArchivedCount                   metricPostgresqlArchiverArchivedCount
	metricPostgresqlArchiverFailedCount                     metricPostgresqlArchiverFailedCount
	metricPostgresqlAutoanalyzed                            metricPostgresqlAutoanalyzed
	metricPostgresqlAutovacuumed                            metricPostgresqlAutovacuumed
	metricPostgresqlBeforeXidWraparound                     metricPostgresqlBeforeXidWraparound
	metricPostgresqlBgwriterBuffersAlloc                    metricPostgresqlBgwriterBuffersAlloc
	metricPostgresqlBgwriterBuffersBackend                  metricPostgresqlBgwriterBuffersBackend
	metricPostgresqlBgwriterBuffersBackendFsync             metricPostgresqlBgwriterBuffersBackendFsync
	metricPostgresqlBgwriterBuffersCheckpoint               metricPostgresqlBgwriterBuffersCheckpoint
	metricPostgresqlBgwriterBuffersClean                    metricPostgresqlBgwriterBuffersClean
	metricPostgresqlBgwriterCheckpointsRequested            metricPostgresqlBgwriterCheckpointsRequested
	metricPostgresqlBgwriterCheckpointsTimed                metricPostgresqlBgwriterCheckpointsTimed
	metricPostgresqlBgwriterMaxwrittenClean                 metricPostgresqlBgwriterMaxwrittenClean
	metricPostgresqlBgwriterSyncTime                        metricPostgresqlBgwriterSyncTime
	metricPostgresqlBgwriterWriteTime                       metricPostgresqlBgwriterWriteTime
	metricPostgresqlBlkReadTime                             metricPostgresqlBlkReadTime
	metricPostgresqlBlkWriteTime                            metricPostgresqlBlkWriteTime
	metricPostgresqlBufferHit                               metricPostgresqlBufferHit
	metricPostgresqlBuffercacheDirtyBuffers                 metricPostgresqlBuffercacheDirtyBuffers
	metricPostgresqlBuffercachePinningBackends              metricPostgresqlBuffercachePinningBackends
	metricPostgresqlBuffercacheUnusedBuffers                metricPostgresqlBuffercacheUnusedBuffers
	metricPostgresqlBuffercacheUsageCount                   metricPostgresqlBuffercacheUsageCount
	metricPostgresqlBuffercacheUsedBuffers                  metricPostgresqlBuffercacheUsedBuffers
	metricPostgresqlChecksumsEnabled                        metricPostgresqlChecksumsEnabled
	metricPostgresqlChecksumsFailures                       metricPostgresqlChecksumsFailures
	metricPostgresqlClusterVacuumHeapBlksScanned            metricPostgresqlClusterVacuumHeapBlksScanned
	metricPostgresqlClusterVacuumHeapBlksTotal              metricPostgresqlClusterVacuumHeapBlksTotal
	metricPostgresqlClusterVacuumHeapTuplesScanned          metricPostgresqlClusterVacuumHeapTuplesScanned
	metricPostgresqlClusterVacuumHeapTuplesWritten          metricPostgresqlClusterVacuumHeapTuplesWritten
	metricPostgresqlCommits                                 metricPostgresqlCommits
	metricPostgresqlConflicts                               metricPostgresqlConflicts
	metricPostgresqlConflictsBufferpin                      metricPostgresqlConflictsBufferpin
	metricPostgresqlConflictsDeadlock                       metricPostgresqlConflictsDeadlock
	metricPostgresqlConflictsLock                           metricPostgresqlConflictsLock
	metricPostgresqlConflictsSnapshot                       metricPostgresqlConflictsSnapshot
	metricPostgresqlConflictsTablespace                     metricPostgresqlConflictsTablespace
	metricPostgresqlConnections                             metricPostgresqlConnections
	metricPostgresqlControlCheckpointDelay                  metricPostgresqlControlCheckpointDelay
	metricPostgresqlControlCheckpointDelayBytes             metricPostgresqlControlCheckpointDelayBytes
	metricPostgresqlControlRedoDelayBytes                   metricPostgresqlControlRedoDelayBytes
	metricPostgresqlControlTimelineID                       metricPostgresqlControlTimelineID
	metricPostgresqlCreateIndexBlocksDone                   metricPostgresqlCreateIndexBlocksDone
	metricPostgresqlCreateIndexBlocksTotal                  metricPostgresqlCreateIndexBlocksTotal
	metricPostgresqlCreateIndexLockersDone                  metricPostgresqlCreateIndexLockersDone
	metricPostgresqlCreateIndexLockersTotal                 metricPostgresqlCreateIndexLockersTotal
	metricPostgresqlCreateIndexPartitionsDone               metricPostgresqlCreateIndexPartitionsDone
	metricPostgresqlCreateIndexPartitionsTotal              metricPostgresqlCreateIndexPartitionsTotal
	metricPostgresqlCreateIndexTuplesDone                   metricPostgresqlCreateIndexTuplesDone
	metricPostgresqlCreateIndexTuplesTotal                  metricPostgresqlCreateIndexTuplesTotal
	metricPostgresqlDatabaseSize                            metricPostgresqlDatabaseSize
	metricPostgresqlDbCount                                 metricPostgresqlDbCount
	metricPostgresqlDeadlocks                               metricPostgresqlDeadlocks
	metricPostgresqlDiskRead                                metricPostgresqlDiskRead
	metricPostgresqlHeapBlocksHit                           metricPostgresqlHeapBlocksHit
	metricPostgresqlHeapBlocksRead                          metricPostgresqlHeapBlocksRead
	metricPostgresqlIndexBlocksHit                          metricPostgresqlIndexBlocksHit
	metricPostgresqlIndexBlocksRead                         metricPostgresqlIndexBlocksRead
	metricPostgresqlIndexScans                              metricPostgresqlIndexScans
	metricPostgresqlIndexSize                               metricPostgresqlIndexSize
	metricPostgresqlIndexTuplesFetched                      metricPostgresqlIndexTuplesFetched
	metricPostgresqlIndexTuplesRead                         metricPostgresqlIndexTuplesRead
	metricPostgresqlLastAnalyzeAge                          metricPostgresqlLastAnalyzeAge
	metricPostgresqlLastAutoanalyzeAge                      metricPostgresqlLastAutoanalyzeAge
	metricPostgresqlLastAutovacuumAge                       metricPostgresqlLastAutovacuumAge
	metricPostgresqlLastVacuumAge                           metricPostgresqlLastVacuumAge
	metricPostgresqlMaxConnections                          metricPostgresqlMaxConnections
	metricPostgresqlPercentUsageConnections                 metricPostgresqlPercentUsageConnections
	metricPostgresqlPgStatStatementsDealloc                 metricPostgresqlPgStatStatementsDealloc
	metricPostgresqlRecoveryPrefetchBlockDistance           metricPostgresqlRecoveryPrefetchBlockDistance
	metricPostgresqlRecoveryPrefetchHit                     metricPostgresqlRecoveryPrefetchHit
	metricPostgresqlRecoveryPrefetchIoDepth                 metricPostgresqlRecoveryPrefetchIoDepth
	metricPostgresqlRecoveryPrefetchPrefetch                metricPostgresqlRecoveryPrefetchPrefetch
	metricPostgresqlRecoveryPrefetchSkipFpw                 metricPostgresqlRecoveryPrefetchSkipFpw
	metricPostgresqlRecoveryPrefetchSkipInit                metricPostgresqlRecoveryPrefetchSkipInit
	metricPostgresqlRecoveryPrefetchSkipNew                 metricPostgresqlRecoveryPrefetchSkipNew
	metricPostgresqlRecoveryPrefetchSkipRep                 metricPostgresqlRecoveryPrefetchSkipRep
	metricPostgresqlRecoveryPrefetchWalDistance             metricPostgresqlRecoveryPrefetchWalDistance
	metricPostgresqlRelationAllVisible                      metricPostgresqlRelationAllVisible
	metricPostgresqlRelationPages                           metricPostgresqlRelationPages
	metricPostgresqlRelationTuples                          metricPostgresqlRelationTuples
	metricPostgresqlRelationXmin                            metricPostgresqlRelationXmin
	metricPostgresqlRelationSize                            metricPostgresqlRelationSize
	metricPostgresqlReplicationBackendXminAge               metricPostgresqlReplicationBackendXminAge
	metricPostgresqlReplicationFlushLsnDelay                metricPostgresqlReplicationFlushLsnDelay
	metricPostgresqlReplicationReplayLsnDelay               metricPostgresqlReplicationReplayLsnDelay
	metricPostgresqlReplicationSentLsnDelay                 metricPostgresqlReplicationSentLsnDelay
	metricPostgresqlReplicationWalFlushLag                  metricPostgresqlReplicationWalFlushLag
	metricPostgresqlReplicationWalReplayLag                 metricPostgresqlReplicationWalReplayLag
	metricPostgresqlReplicationWalWriteLag                  metricPostgresqlReplicationWalWriteLag
	metricPostgresqlReplicationWriteLsnDelay                metricPostgresqlReplicationWriteLsnDelay
	metricPostgresqlReplicationDelay                        metricPostgresqlReplicationDelay
	metricPostgresqlReplicationDelayBytes                   metricPostgresqlReplicationDelayBytes
	metricPostgresqlReplicationSlotCatalogXminAge           metricPostgresqlReplicationSlotCatalogXminAge
	metricPostgresqlReplicationSlotConfirmedFlushDelayBytes metricPostgresqlReplicationSlotConfirmedFlushDelayBytes
	metricPostgresqlReplicationSlotRestartDelayBytes        metricPostgresqlReplicationSlotRestartDelayBytes
	metricPostgresqlReplicationSlotSpillBytes               metricPostgresqlReplicationSlotSpillBytes
	metricPostgresqlReplicationSlotSpillCount               metricPostgresqlReplicationSlotSpillCount
	metricPostgresqlReplicationSlotSpillTxns                metricPostgresqlReplicationSlotSpillTxns
	metricPostgresqlReplicationSlotStreamBytes              metricPostgresqlReplicationSlotStreamBytes
	metricPostgresqlReplicationSlotStreamCount              metricPostgresqlReplicationSlotStreamCount
	metricPostgresqlReplicationSlotStreamTxns               metricPostgresqlReplicationSlotStreamTxns
	metricPostgresqlReplicationSlotTotalBytes               metricPostgresqlReplicationSlotTotalBytes
	metricPostgresqlReplicationSlotTotalTxns                metricPostgresqlReplicationSlotTotalTxns
	metricPostgresqlReplicationSlotXminAge                  metricPostgresqlReplicationSlotXminAge
	metricPostgresqlRollbacks                               metricPostgresqlRollbacks
	metricPostgresqlRowsDeleted                             metricPostgresqlRowsDeleted
	metricPostgresqlRowsFetched                             metricPostgresqlRowsFetched
	metricPostgresqlRowsInserted                            metricPostgresqlRowsInserted
	metricPostgresqlRowsReturned                            metricPostgresqlRowsReturned
	metricPostgresqlRowsUpdated                             metricPostgresqlRowsUpdated
	metricPostgresqlRunning                                 metricPostgresqlRunning
	metricPostgresqlSessionsAbandoned                       metricPostgresqlSessionsAbandoned
	metricPostgresqlSessionsActiveTime                      metricPostgresqlSessionsActiveTime
	metricPostgresqlSessionsCount                           metricPostgresqlSessionsCount
	metricPostgresqlSessionsFatal                           metricPostgresqlSessionsFatal
	metricPostgresqlSessionsIdleInTransactionTime           metricPostgresqlSessionsIdleInTransactionTime
	metricPostgresqlSessionsKilled                          metricPostgresqlSessionsKilled
	metricPostgresqlSessionsSessionTime                     metricPostgresqlSessionsSessionTime
	metricPostgresqlSlruBlksExists                          metricPostgresqlSlruBlksExists
	metricPostgresqlSlruBlksHit                             metricPostgresqlSlruBlksHit
	metricPostgresqlSlruBlksRead                            metricPostgresqlSlruBlksRead
	metricPostgresqlSlruBlksWritten                         metricPostgresqlSlruBlksWritten
	metricPostgresqlSlruBlksZeroed                          metricPostgresqlSlruBlksZeroed
	metricPostgresqlSlruFlushes                             metricPostgresqlSlruFlushes
	metricPostgresqlSlruTruncates                           metricPostgresqlSlruTruncates
	metricPostgresqlSnapshotXipCount                        metricPostgresqlSnapshotXipCount
	metricPostgresqlSnapshotXmax                            metricPostgresqlSnapshotXmax
	metricPostgresqlSnapshotXmin                            metricPostgresqlSnapshotXmin
	metricPostgresqlSubscriptionApplyError                  metricPostgresqlSubscriptionApplyError
	metricPostgresqlSubscriptionLastMsgReceiptAge           metricPostgresqlSubscriptionLastMsgReceiptAge
	metricPostgresqlSubscriptionLastMsgSendAge              metricPostgresqlSubscriptionLastMsgSendAge
	metricPostgresqlSubscriptionLatestEndAge                metricPostgresqlSubscriptionLatestEndAge
	metricPostgresqlSubscriptionSyncError                   metricPostgresqlSubscriptionSyncError
	metricPostgresqlTempBytes                               metricPostgresqlTempBytes
	metricPostgresqlTempFiles                               metricPostgresqlTempFiles
	metricPostgresqlToastAutovacuumed                       metricPostgresqlToastAutovacuumed
	metricPostgresqlToastLastAutovacuumAge                  metricPostgresqlToastLastAutovacuumAge
	metricPostgresqlToastLastVacuumAge                      metricPostgresqlToastLastVacuumAge
	metricPostgresqlToastVacuumed                           metricPostgresqlToastVacuumed
	metricPostgresqlToastBlocksHit                          metricPostgresqlToastBlocksHit
	metricPostgresqlToastBlocksRead                         metricPostgresqlToastBlocksRead
	metricPostgresqlToastIndexBlocksHit                     metricPostgresqlToastIndexBlocksHit
	metricPostgresqlToastIndexBlocksRead                    metricPostgresqlToastIndexBlocksRead
	metricPostgresqlToastSize                               metricPostgresqlToastSize
	metricPostgresqlTransactionsDurationMax                 metricPostgresqlTransactionsDurationMax
	metricPostgresqlTransactionsDurationSum                 metricPostgresqlTransactionsDurationSum
	metricPostgresqlUptime                                  metricPostgresqlUptime
	metricPostgresqlVacuumHeapBlksScanned                   metricPostgresqlVacuumHeapBlksScanned
	metricPostgresqlVacuumHeapBlksTotal                     metricPostgresqlVacuumHeapBlksTotal
	metricPostgresqlVacuumHeapBlksVacuumed                  metricPostgresqlVacuumHeapBlksVacuumed
	metricPostgresqlVacuumIndexVacuumCount                  metricPostgresqlVacuumIndexVacuumCount
	metricPostgresqlVacuumMaxDeadTuples                     metricPostgresqlVacuumMaxDeadTuples
	metricPostgresqlVacuumNumDeadTuples                     metricPostgresqlVacuumNumDeadTuples
	metricPostgresqlVacuumed                                metricPostgresqlVacuumed
	metricPostgresqlWalBuffersFull                          metricPostgresqlWalBuffersFull
	metricPostgresqlWalBytes                                metricPostgresqlWalBytes
	metricPostgresqlWalFpi                                  metricPostgresqlWalFpi
	metricPostgresqlWalRecords                              metricPostgresqlWalRecords
	metricPostgresqlWalSync                                 metricPostgresqlWalSync
	metricPostgresqlWalSyncTime                             metricPostgresqlWalSyncTime
	metricPostgresqlWalWrite                                metricPostgresqlWalWrite
	metricPostgresqlWalWriteTime                            metricPostgresqlWalWriteTime
	metricPostgresqlWalFilesAge                             metricPostgresqlWalFilesAge
	metricPostgresqlWalFilesCount                           metricPostgresqlWalFilesCount
	metricPostgresqlWalFilesSize                            metricPostgresqlWalFilesSize
	metricPostgresqlWalReceiverConnected                    metricPostgresqlWalReceiverConnected
	metricPostgresqlWalReceiverLastMsgReceiptAge            metricPostgresqlWalReceiverLastMsgReceiptAge
	metricPostgresqlWalReceiverLastMsgSendAge               metricPostgresqlWalReceiverLastMsgSendAge
	metricPostgresqlWalReceiverLatestEndAge                 metricPostgresqlWalReceiverLatestEndAge
	metricPostgresqlWalReceiverReceivedTimeline             metricPostgresqlWalReceiverReceivedTimeline
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                   mbc,
		startTime:                                pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                            pmetric.NewMetrics(),
		buildInfo:                                settings.BuildInfo,
		metricPgbouncerStatsAvgBytesIn:           newMetricPgbouncerStatsAvgBytesIn(mbc.Metrics.PgbouncerStatsAvgBytesIn),
		metricPgbouncerStatsAvgBytesOut:          newMetricPgbouncerStatsAvgBytesOut(mbc.Metrics.PgbouncerStatsAvgBytesOut),
		metricPgbouncerStatsAvgRequestsPerSecond: newMetricPgbouncerStatsAvgRequestsPerSecond(mbc.Metrics.PgbouncerStatsAvgRequestsPerSecond),
		metricPgbouncerStatsAvgServerAssignmentCount:            newMetricPgbouncerStatsAvgServerAssignmentCount(mbc.Metrics.PgbouncerStatsAvgServerAssignmentCount),
		metricPgbouncerStatsAvgTransactionCount:                 newMetricPgbouncerStatsAvgTransactionCount(mbc.Metrics.PgbouncerStatsAvgTransactionCount),
		metricPgbouncerStatsAvgTransactionDurationMilliseconds:  newMetricPgbouncerStatsAvgTransactionDurationMilliseconds(mbc.Metrics.PgbouncerStatsAvgTransactionDurationMilliseconds),
		metricPgbouncerStatsBytesInPerSecond:                    newMetricPgbouncerStatsBytesInPerSecond(mbc.Metrics.PgbouncerStatsBytesInPerSecond),
		metricPgbouncerStatsBytesOutPerSecond:                   newMetricPgbouncerStatsBytesOutPerSecond(mbc.Metrics.PgbouncerStatsBytesOutPerSecond),
		metricPgbouncerStatsQueriesPerSecond:                    newMetricPgbouncerStatsQueriesPerSecond(mbc.Metrics.PgbouncerStatsQueriesPerSecond),
		metricPgbouncerStatsRequestsPerSecond:                   newMetricPgbouncerStatsRequestsPerSecond(mbc.Metrics.PgbouncerStatsRequestsPerSecond),
		metricPgbouncerStatsTotalServerAssignmentCount:          newMetricPgbouncerStatsTotalServerAssignmentCount(mbc.Metrics.PgbouncerStatsTotalServerAssignmentCount),
		metricPgbouncerStatsTransactionsPerSecond:               newMetricPgbouncerStatsTransactionsPerSecond(mbc.Metrics.PgbouncerStatsTransactionsPerSecond),
		metricPostgresqlActiveWaitingQueries:                    newMetricPostgresqlActiveWaitingQueries(mbc.Metrics.PostgresqlActiveWaitingQueries),
		metricPostgresqlActivityBackendXidAge:                   newMetricPostgresqlActivityBackendXidAge(mbc.Metrics.PostgresqlActivityBackendXidAge),
		metricPostgresqlActivityBackendXminAge:                  newMetricPostgresqlActivityBackendXminAge(mbc.Metrics.PostgresqlActivityBackendXminAge),
		metricPostgresqlActivityWaitEvent:                       newMetricPostgresqlActivityWaitEvent(mbc.Metrics.PostgresqlActivityWaitEvent),
		metricPostgresqlActivityXactStartAge:                    newMetricPostgresqlActivityXactStartAge(mbc.Metrics.PostgresqlActivityXactStartAge),
		metricPostgresqlAnalyzeChildTablesDone:                  newMetricPostgresqlAnalyzeChildTablesDone(mbc.Metrics.PostgresqlAnalyzeChildTablesDone),
		metricPostgresqlAnalyzeChildTablesTotal:                 newMetricPostgresqlAnalyzeChildTablesTotal(mbc.Metrics.PostgresqlAnalyzeChildTablesTotal),
		metricPostgresqlAnalyzeExtStatsComputed:                 newMetricPostgresqlAnalyzeExtStatsComputed(mbc.Metrics.PostgresqlAnalyzeExtStatsComputed),
		metricPostgresqlAnalyzeExtStatsTotal:                    newMetricPostgresqlAnalyzeExtStatsTotal(mbc.Metrics.PostgresqlAnalyzeExtStatsTotal),
		metricPostgresqlAnalyzeSampleBlksScanned:                newMetricPostgresqlAnalyzeSampleBlksScanned(mbc.Metrics.PostgresqlAnalyzeSampleBlksScanned),
		metricPostgresqlAnalyzeSampleBlksTotal:                  newMetricPostgresqlAnalyzeSampleBlksTotal(mbc.Metrics.PostgresqlAnalyzeSampleBlksTotal),
		metricPostgresqlAnalyzed:                                newMetricPostgresqlAnalyzed(mbc.Metrics.PostgresqlAnalyzed),
		metricPostgresqlArchiverArchivedCount:                   newMetricPostgresqlArchiverArchivedCount(mbc.Metrics.PostgresqlArchiverArchivedCount),
		metricPostgresqlArchiverFailedCount:                     newMetricPostgresqlArchiverFailedCount(mbc.Metrics.PostgresqlArchiverFailedCount),
		metricPostgresqlAutoanalyzed:                            newMetricPostgresqlAutoanalyzed(mbc.Metrics.PostgresqlAutoanalyzed),
		metricPostgresqlAutovacuumed:                            newMetricPostgresqlAutovacuumed(mbc.Metrics.PostgresqlAutovacuumed),
		metricPostgresqlBeforeXidWraparound:                     newMetricPostgresqlBeforeXidWraparound(mbc.Metrics.PostgresqlBeforeXidWraparound),
		metricPostgresqlBgwriterBuffersAlloc:                    newMetricPostgresqlBgwriterBuffersAlloc(mbc.Metrics.PostgresqlBgwriterBuffersAlloc),
		metricPostgresqlBgwriterBuffersBackend:                  newMetricPostgresqlBgwriterBuffersBackend(mbc.Metrics.PostgresqlBgwriterBuffersBackend),
		metricPostgresqlBgwriterBuffersBackendFsync:             newMetricPostgresqlBgwriterBuffersBackendFsync(mbc.Metrics.PostgresqlBgwriterBuffersBackendFsync),
		metricPostgresqlBgwriterBuffersCheckpoint:               newMetricPostgresqlBgwriterBuffersCheckpoint(mbc.Metrics.PostgresqlBgwriterBuffersCheckpoint),
		metricPostgresqlBgwriterBuffersClean:                    newMetricPostgresqlBgwriterBuffersClean(mbc.Metrics.PostgresqlBgwriterBuffersClean),
		metricPostgresqlBgwriterCheckpointsRequested:            newMetricPostgresqlBgwriterCheckpointsRequested(mbc.Metrics.PostgresqlBgwriterCheckpointsRequested),
		metricPostgresqlBgwriterCheckpointsTimed:                newMetricPostgresqlBgwriterCheckpointsTimed(mbc.Metrics.PostgresqlBgwriterCheckpointsTimed),
		metricPostgresqlBgwriterMaxwrittenClean:                 newMetricPostgresqlBgwriterMaxwrittenClean(mbc.Metrics.PostgresqlBgwriterMaxwrittenClean),
		metricPostgresqlBgwriterSyncTime:                        newMetricPostgresqlBgwriterSyncTime(mbc.Metrics.PostgresqlBgwriterSyncTime),
		metricPostgresqlBgwriterWriteTime:                       newMetricPostgresqlBgwriterWriteTime(mbc.Metrics.PostgresqlBgwriterWriteTime),
		metricPostgresqlBlkReadTime:                             newMetricPostgresqlBlkReadTime(mbc.Metrics.PostgresqlBlkReadTime),
		metricPostgresqlBlkWriteTime:                            newMetricPostgresqlBlkWriteTime(mbc.Metrics.PostgresqlBlkWriteTime),
		metricPostgresqlBufferHit:                               newMetricPostgresqlBufferHit(mbc.Metrics.PostgresqlBufferHit),
		metricPostgresqlBuffercacheDirtyBuffers:                 newMetricPostgresqlBuffercacheDirtyBuffers(mbc.Metrics.PostgresqlBuffercacheDirtyBuffers),
		metricPostgresqlBuffercachePinningBackends:              newMetricPostgresqlBuffercachePinningBackends(mbc.Metrics.PostgresqlBuffercachePinningBackends),
		metricPostgresqlBuffercacheUnusedBuffers:                newMetricPostgresqlBuffercacheUnusedBuffers(mbc.Metrics.PostgresqlBuffercacheUnusedBuffers),
		metricPostgresqlBuffercacheUsageCount:                   newMetricPostgresqlBuffercacheUsageCount(mbc.Metrics.PostgresqlBuffercacheUsageCount),
		metricPostgresqlBuffercacheUsedBuffers:                  newMetricPostgresqlBuffercacheUsedBuffers(mbc.Metrics.PostgresqlBuffercacheUsedBuffers),
		metricPostgresqlChecksumsEnabled:                        newMetricPostgresqlChecksumsEnabled(mbc.Metrics.PostgresqlChecksumsEnabled),
		metricPostgresqlChecksumsFailures:                       newMetricPostgresqlChecksumsFailures(mbc.Metrics.PostgresqlChecksumsFailures),
		metricPostgresqlClusterVacuumHeapBlksScanned:            newMetricPostgresqlClusterVacuumHeapBlksScanned(mbc.Metrics.PostgresqlClusterVacuumHeapBlksScanned),
		metricPostgresqlClusterVacuumHeapBlksTotal:              newMetricPostgresqlClusterVacuumHeapBlksTotal(mbc.Metrics.PostgresqlClusterVacuumHeapBlksTotal),
		metricPostgresqlClusterVacuumHeapTuplesScanned:          newMetricPostgresqlClusterVacuumHeapTuplesScanned(mbc.Metrics.PostgresqlClusterVacuumHeapTuplesScanned),
		metricPostgresqlClusterVacuumHeapTuplesWritten:          newMetricPostgresqlClusterVacuumHeapTuplesWritten(mbc.Metrics.PostgresqlClusterVacuumHeapTuplesWritten),
		metricPostgresqlCommits:                                 newMetricPostgresqlCommits(mbc.Metrics.PostgresqlCommits),
		metricPostgresqlConflicts:                               newMetricPostgresqlConflicts(mbc.Metrics.PostgresqlConflicts),
		metricPostgresqlConflictsBufferpin:                      newMetricPostgresqlConflictsBufferpin(mbc.Metrics.PostgresqlConflictsBufferpin),
		metricPostgresqlConflictsDeadlock:                       newMetricPostgresqlConflictsDeadlock(mbc.Metrics.PostgresqlConflictsDeadlock),
		metricPostgresqlConflictsLock:                           newMetricPostgresqlConflictsLock(mbc.Metrics.PostgresqlConflictsLock),
		metricPostgresqlConflictsSnapshot:                       newMetricPostgresqlConflictsSnapshot(mbc.Metrics.PostgresqlConflictsSnapshot),
		metricPostgresqlConflictsTablespace:                     newMetricPostgresqlConflictsTablespace(mbc.Metrics.PostgresqlConflictsTablespace),
		metricPostgresqlConnections:                             newMetricPostgresqlConnections(mbc.Metrics.PostgresqlConnections),
		metricPostgresqlControlCheckpointDelay:                  newMetricPostgresqlControlCheckpointDelay(mbc.Metrics.PostgresqlControlCheckpointDelay),
		metricPostgresqlControlCheckpointDelayBytes:             newMetricPostgresqlControlCheckpointDelayBytes(mbc.Metrics.PostgresqlControlCheckpointDelayBytes),
		metricPostgresqlControlRedoDelayBytes:                   newMetricPostgresqlControlRedoDelayBytes(mbc.Metrics.PostgresqlControlRedoDelayBytes),
		metricPostgresqlControlTimelineID:                       newMetricPostgresqlControlTimelineID(mbc.Metrics.PostgresqlControlTimelineID),
		metricPostgresqlCreateIndexBlocksDone:                   newMetricPostgresqlCreateIndexBlocksDone(mbc.Metrics.PostgresqlCreateIndexBlocksDone),
		metricPostgresqlCreateIndexBlocksTotal:                  newMetricPostgresqlCreateIndexBlocksTotal(mbc.Metrics.PostgresqlCreateIndexBlocksTotal),
		metricPostgresqlCreateIndexLockersDone:                  newMetricPostgresqlCreateIndexLockersDone(mbc.Metrics.PostgresqlCreateIndexLockersDone),
		metricPostgresqlCreateIndexLockersTotal:                 newMetricPostgresqlCreateIndexLockersTotal(mbc.Metrics.PostgresqlCreateIndexLockersTotal),
		metricPostgresqlCreateIndexPartitionsDone:               newMetricPostgresqlCreateIndexPartitionsDone(mbc.Metrics.PostgresqlCreateIndexPartitionsDone),
		metricPostgresqlCreateIndexPartitionsTotal:              newMetricPostgresqlCreateIndexPartitionsTotal(mbc.Metrics.PostgresqlCreateIndexPartitionsTotal),
		metricPostgresqlCreateIndexTuplesDone:                   newMetricPostgresqlCreateIndexTuplesDone(mbc.Metrics.PostgresqlCreateIndexTuplesDone),
		metricPostgresqlCreateIndexTuplesTotal:                  newMetricPostgresqlCreateIndexTuplesTotal(mbc.Metrics.PostgresqlCreateIndexTuplesTotal),
		metricPostgresqlDatabaseSize:                            newMetricPostgresqlDatabaseSize(mbc.Metrics.PostgresqlDatabaseSize),
		metricPostgresqlDbCount:                                 newMetricPostgresqlDbCount(mbc.Metrics.PostgresqlDbCount),
		metricPostgresqlDeadlocks:                               newMetricPostgresqlDeadlocks(mbc.Metrics.PostgresqlDeadlocks),
		metricPostgresqlDiskRead:                                newMetricPostgresqlDiskRead(mbc.Metrics.PostgresqlDiskRead),
		metricPostgresqlHeapBlocksHit:                           newMetricPostgresqlHeapBlocksHit(mbc.Metrics.PostgresqlHeapBlocksHit),
		metricPostgresqlHeapBlocksRead:                          newMetricPostgresqlHeapBlocksRead(mbc.Metrics.PostgresqlHeapBlocksRead),
		metricPostgresqlIndexBlocksHit:                          newMetricPostgresqlIndexBlocksHit(mbc.Metrics.PostgresqlIndexBlocksHit),
		metricPostgresqlIndexBlocksRead:                         newMetricPostgresqlIndexBlocksRead(mbc.Metrics.PostgresqlIndexBlocksRead),
		metricPostgresqlIndexScans:                              newMetricPostgresqlIndexScans(mbc.Metrics.PostgresqlIndexScans),
		metricPostgresqlIndexSize:                               newMetricPostgresqlIndexSize(mbc.Metrics.PostgresqlIndexSize),
		metricPostgresqlIndexTuplesFetched:                      newMetricPostgresqlIndexTuplesFetched(mbc.Metrics.PostgresqlIndexTuplesFetched),
		metricPostgresqlIndexTuplesRead:                         newMetricPostgresqlIndexTuplesRead(mbc.Metrics.PostgresqlIndexTuplesRead),
		metricPostgresqlLastAnalyzeAge:                          newMetricPostgresqlLastAnalyzeAge(mbc.Metrics.PostgresqlLastAnalyzeAge),
		metricPostgresqlLastAutoanalyzeAge:                      newMetricPostgresqlLastAutoanalyzeAge(mbc.Metrics.PostgresqlLastAutoanalyzeAge),
		metricPostgresqlLastAutovacuumAge:                       newMetricPostgresqlLastAutovacuumAge(mbc.Metrics.PostgresqlLastAutovacuumAge),
		metricPostgresqlLastVacuumAge:                           newMetricPostgresqlLastVacuumAge(mbc.Metrics.PostgresqlLastVacuumAge),
		metricPostgresqlMaxConnections:                          newMetricPostgresqlMaxConnections(mbc.Metrics.PostgresqlMaxConnections),
		metricPostgresqlPercentUsageConnections:                 newMetricPostgresqlPercentUsageConnections(mbc.Metrics.PostgresqlPercentUsageConnections),
		metricPostgresqlPgStatStatementsDealloc:                 newMetricPostgresqlPgStatStatementsDealloc(mbc.Metrics.PostgresqlPgStatStatementsDealloc),
		metricPostgresqlRecoveryPrefetchBlockDistance:           newMetricPostgresqlRecoveryPrefetchBlockDistance(mbc.Metrics.PostgresqlRecoveryPrefetchBlockDistance),
		metricPostgresqlRecoveryPrefetchHit:                     newMetricPostgresqlRecoveryPrefetchHit(mbc.Metrics.PostgresqlRecoveryPrefetchHit),
		metricPostgresqlRecoveryPrefetchIoDepth:                 newMetricPostgresqlRecoveryPrefetchIoDepth(mbc.Metrics.PostgresqlRecoveryPrefetchIoDepth),
		metricPostgresqlRecoveryPrefetchPrefetch:                newMetricPostgresqlRecoveryPrefetchPrefetch(mbc.Metrics.PostgresqlRecoveryPrefetchPrefetch),
		metricPostgresqlRecoveryPrefetchSkipFpw:                 newMetricPostgresqlRecoveryPrefetchSkipFpw(mbc.Metrics.PostgresqlRecoveryPrefetchSkipFpw),
		metricPostgresqlRecoveryPrefetchSkipInit:                newMetricPostgresqlRecoveryPrefetchSkipInit(mbc.Metrics.PostgresqlRecoveryPrefetchSkipInit),
		metricPostgresqlRecoveryPrefetchSkipNew:                 newMetricPostgresqlRecoveryPrefetchSkipNew(mbc.Metrics.PostgresqlRecoveryPrefetchSkipNew),
		metricPostgresqlRecoveryPrefetchSkipRep:                 newMetricPostgresqlRecoveryPrefetchSkipRep(mbc.Metrics.PostgresqlRecoveryPrefetchSkipRep),
		metricPostgresqlRecoveryPrefetchWalDistance:             newMetricPostgresqlRecoveryPrefetchWalDistance(mbc.Metrics.PostgresqlRecoveryPrefetchWalDistance),
		metricPostgresqlRelationAllVisible:                      newMetricPostgresqlRelationAllVisible(mbc.Metrics.PostgresqlRelationAllVisible),
		metricPostgresqlRelationPages:                           newMetricPostgresqlRelationPages(mbc.Metrics.PostgresqlRelationPages),
		metricPostgresqlRelationTuples:                          newMetricPostgresqlRelationTuples(mbc.Metrics.PostgresqlRelationTuples),
		metricPostgresqlRelationXmin:                            newMetricPostgresqlRelationXmin(mbc.Metrics.PostgresqlRelationXmin),
		metricPostgresqlRelationSize:                            newMetricPostgresqlRelationSize(mbc.Metrics.PostgresqlRelationSize),
		metricPostgresqlReplicationBackendXminAge:               newMetricPostgresqlReplicationBackendXminAge(mbc.Metrics.PostgresqlReplicationBackendXminAge),
		metricPostgresqlReplicationFlushLsnDelay:                newMetricPostgresqlReplicationFlushLsnDelay(mbc.Metrics.PostgresqlReplicationFlushLsnDelay),
		metricPostgresqlReplicationReplayLsnDelay:               newMetricPostgresqlReplicationReplayLsnDelay(mbc.Metrics.PostgresqlReplicationReplayLsnDelay),
		metricPostgresqlReplicationSentLsnDelay:                 newMetricPostgresqlReplicationSentLsnDelay(mbc.Metrics.PostgresqlReplicationSentLsnDelay),
		metricPostgresqlReplicationWalFlushLag:                  newMetricPostgresqlReplicationWalFlushLag(mbc.Metrics.PostgresqlReplicationWalFlushLag),
		metricPostgresqlReplicationWalReplayLag:                 newMetricPostgresqlReplicationWalReplayLag(mbc.Metrics.PostgresqlReplicationWalReplayLag),
		metricPostgresqlReplicationWalWriteLag:                  newMetricPostgresqlReplicationWalWriteLag(mbc.Metrics.PostgresqlReplicationWalWriteLag),
		metricPostgresqlReplicationWriteLsnDelay:                newMetricPostgresqlReplicationWriteLsnDelay(mbc.Metrics.PostgresqlReplicationWriteLsnDelay),
		metricPostgresqlReplicationDelay:                        newMetricPostgresqlReplicationDelay(mbc.Metrics.PostgresqlReplicationDelay),
		metricPostgresqlReplicationDelayBytes:                   newMetricPostgresqlReplicationDelayBytes(mbc.Metrics.PostgresqlReplicationDelayBytes),
		metricPostgresqlReplicationSlotCatalogXminAge:           newMetricPostgresqlReplicationSlotCatalogXminAge(mbc.Metrics.PostgresqlReplicationSlotCatalogXminAge),
		metricPostgresqlReplicationSlotConfirmedFlushDelayBytes: newMetricPostgresqlReplicationSlotConfirmedFlushDelayBytes(mbc.Metrics.PostgresqlReplicationSlotConfirmedFlushDelayBytes),
		metricPostgresqlReplicationSlotRestartDelayBytes:        newMetricPostgresqlReplicationSlotRestartDelayBytes(mbc.Metrics.PostgresqlReplicationSlotRestartDelayBytes),
		metricPostgresqlReplicationSlotSpillBytes:               newMetricPostgresqlReplicationSlotSpillBytes(mbc.Metrics.PostgresqlReplicationSlotSpillBytes),
		metricPostgresqlReplicationSlotSpillCount:               newMetricPostgresqlReplicationSlotSpillCount(mbc.Metrics.PostgresqlReplicationSlotSpillCount),
		metricPostgresqlReplicationSlotSpillTxns:                newMetricPostgresqlReplicationSlotSpillTxns(mbc.Metrics.PostgresqlReplicationSlotSpillTxns),
		metricPostgresqlReplicationSlotStreamBytes:              newMetricPostgresqlReplicationSlotStreamBytes(mbc.Metrics.PostgresqlReplicationSlotStreamBytes),
		metricPostgresqlReplicationSlotStreamCount:              newMetricPostgresqlReplicationSlotStreamCount(mbc.Metrics.PostgresqlReplicationSlotStreamCount),
		metricPostgresqlReplicationSlotStreamTxns:               newMetricPostgresqlReplicationSlotStreamTxns(mbc.Metrics.PostgresqlReplicationSlotStreamTxns),
		metricPostgresqlReplicationSlotTotalBytes:               newMetricPostgresqlReplicationSlotTotalBytes(mbc.Metrics.PostgresqlReplicationSlotTotalBytes),
		metricPostgresqlReplicationSlotTotalTxns:                newMetricPostgresqlReplicationSlotTotalTxns(mbc.Metrics.PostgresqlReplicationSlotTotalTxns),
		metricPostgresqlReplicationSlotXminAge:                  newMetricPostgresqlReplicationSlotXminAge(mbc.Metrics.PostgresqlReplicationSlotXminAge),
		metricPostgresqlRollbacks:                               newMetricPostgresqlRollbacks(mbc.Metrics.PostgresqlRollbacks),
		metricPostgresqlRowsDeleted:                             newMetricPostgresqlRowsDeleted(mbc.Metrics.PostgresqlRowsDeleted),
		metricPostgresqlRowsFetched:                             newMetricPostgresqlRowsFetched(mbc.Metrics.PostgresqlRowsFetched),
		metricPostgresqlRowsInserted:                            newMetricPostgresqlRowsInserted(mbc.Metrics.PostgresqlRowsInserted),
		metricPostgresqlRowsReturned:                            newMetricPostgresqlRowsReturned(mbc.Metrics.PostgresqlRowsReturned),
		metricPostgresqlRowsUpdated:                             newMetricPostgresqlRowsUpdated(mbc.Metrics.PostgresqlRowsUpdated),
		metricPostgresqlRunning:                                 newMetricPostgresqlRunning(mbc.Metrics.PostgresqlRunning),
		metricPostgresqlSessionsAbandoned:                       newMetricPostgresqlSessionsAbandoned(mbc.Metrics.PostgresqlSessionsAbandoned),
		metricPostgresqlSessionsActiveTime:                      newMetricPostgresqlSessionsActiveTime(mbc.Metrics.PostgresqlSessionsActiveTime),
		metricPostgresqlSessionsCount:                           newMetricPostgresqlSessionsCount(mbc.Metrics.PostgresqlSessionsCount),
		metricPostgresqlSessionsFatal:                           newMetricPostgresqlSessionsFatal(mbc.Metrics.PostgresqlSessionsFatal),
		metricPostgresqlSessionsIdleInTransactionTime:           newMetricPostgresqlSessionsIdleInTransactionTime(mbc.Metrics.PostgresqlSessionsIdleInTransactionTime),
		metricPostgresqlSessionsKilled:                          newMetricPostgresqlSessionsKilled(mbc.Metrics.PostgresqlSessionsKilled),
		metricPostgresqlSessionsSessionTime:                     newMetricPostgresqlSessionsSessionTime(mbc.Metrics.PostgresqlSessionsSessionTime),
		metricPostgresqlSlruBlksExists:                          newMetricPostgresqlSlruBlksExists(mbc.Metrics.PostgresqlSlruBlksExists),
		metricPostgresqlSlruBlksHit:                             newMetricPostgresqlSlruBlksHit(mbc.Metrics.PostgresqlSlruBlksHit),
		metricPostgresqlSlruBlksRead:                            newMetricPostgresqlSlruBlksRead(mbc.Metrics.PostgresqlSlruBlksRead),
		metricPostgresqlSlruBlksWritten:                         newMetricPostgresqlSlruBlksWritten(mbc.Metrics.PostgresqlSlruBlksWritten),
		metricPostgresqlSlruBlksZeroed:                          newMetricPostgresqlSlruBlksZeroed(mbc.Metrics.PostgresqlSlruBlksZeroed),
		metricPostgresqlSlruFlushes:                             newMetricPostgresqlSlruFlushes(mbc.Metrics.PostgresqlSlruFlushes),
		metricPostgresqlSlruTruncates:                           newMetricPostgresqlSlruTruncates(mbc.Metrics.PostgresqlSlruTruncates),
		metricPostgresqlSnapshotXipCount:                        newMetricPostgresqlSnapshotXipCount(mbc.Metrics.PostgresqlSnapshotXipCount),
		metricPostgresqlSnapshotXmax:                            newMetricPostgresqlSnapshotXmax(mbc.Metrics.PostgresqlSnapshotXmax),
		metricPostgresqlSnapshotXmin:                            newMetricPostgresqlSnapshotXmin(mbc.Metrics.PostgresqlSnapshotXmin),
		metricPostgresqlSubscriptionApplyError:                  newMetricPostgresqlSubscriptionApplyError(mbc.Metrics.PostgresqlSubscriptionApplyError),
		metricPostgresqlSubscriptionLastMsgReceiptAge:           newMetricPostgresqlSubscriptionLastMsgReceiptAge(mbc.Metrics.PostgresqlSubscriptionLastMsgReceiptAge),
		metricPostgresqlSubscriptionLastMsgSendAge:              newMetricPostgresqlSubscriptionLastMsgSendAge(mbc.Metrics.PostgresqlSubscriptionLastMsgSendAge),
		metricPostgresqlSubscriptionLatestEndAge:                newMetricPostgresqlSubscriptionLatestEndAge(mbc.Metrics.PostgresqlSubscriptionLatestEndAge),
		metricPostgresqlSubscriptionSyncError:                   newMetricPostgresqlSubscriptionSyncError(mbc.Metrics.PostgresqlSubscriptionSyncError),
		metricPostgresqlTempBytes:                               newMetricPostgresqlTempBytes(mbc.Metrics.PostgresqlTempBytes),
		metricPostgresqlTempFiles:                               newMetricPostgresqlTempFiles(mbc.Metrics.PostgresqlTempFiles),
		metricPostgresqlToastAutovacuumed:                       newMetricPostgresqlToastAutovacuumed(mbc.Metrics.PostgresqlToastAutovacuumed),
		metricPostgresqlToastLastAutovacuumAge:                  newMetricPostgresqlToastLastAutovacuumAge(mbc.Metrics.PostgresqlToastLastAutovacuumAge),
		metricPostgresqlToastLastVacuumAge:                      newMetricPostgresqlToastLastVacuumAge(mbc.Metrics.PostgresqlToastLastVacuumAge),
		metricPostgresqlToastVacuumed:                           newMetricPostgresqlToastVacuumed(mbc.Metrics.PostgresqlToastVacuumed),
		metricPostgresqlToastBlocksHit:                          newMetricPostgresqlToastBlocksHit(mbc.Metrics.PostgresqlToastBlocksHit),
		metricPostgresqlToastBlocksRead:                         newMetricPostgresqlToastBlocksRead(mbc.Metrics.PostgresqlToastBlocksRead),
		metricPostgresqlToastIndexBlocksHit:                     newMetricPostgresqlToastIndexBlocksHit(mbc.Metrics.PostgresqlToastIndexBlocksHit),
		metricPostgresqlToastIndexBlocksRead:                    newMetricPostgresqlToastIndexBlocksRead(mbc.Metrics.PostgresqlToastIndexBlocksRead),
		metricPostgresqlToastSize:                               newMetricPostgresqlToastSize(mbc.Metrics.PostgresqlToastSize),
		metricPostgresqlTransactionsDurationMax:                 newMetricPostgresqlTransactionsDurationMax(mbc.Metrics.PostgresqlTransactionsDurationMax),
		metricPostgresqlTransactionsDurationSum:                 newMetricPostgresqlTransactionsDurationSum(mbc.Metrics.PostgresqlTransactionsDurationSum),
		metricPostgresqlUptime:                                  newMetricPostgresqlUptime(mbc.Metrics.PostgresqlUptime),
		metricPostgresqlVacuumHeapBlksScanned:                   newMetricPostgresqlVacuumHeapBlksScanned(mbc.Metrics.PostgresqlVacuumHeapBlksScanned),
		metricPostgresqlVacuumHeapBlksTotal:                     newMetricPostgresqlVacuumHeapBlksTotal(mbc.Metrics.PostgresqlVacuumHeapBlksTotal),
		metricPostgresqlVacuumHeapBlksVacuumed:                  newMetricPostgresqlVacuumHeapBlksVacuumed(mbc.Metrics.PostgresqlVacuumHeapBlksVacuumed),
		metricPostgresqlVacuumIndexVacuumCount:                  newMetricPostgresqlVacuumIndexVacuumCount(mbc.Metrics.PostgresqlVacuumIndexVacuumCount),
		metricPostgresqlVacuumMaxDeadTuples:                     newMetricPostgresqlVacuumMaxDeadTuples(mbc.Metrics.PostgresqlVacuumMaxDeadTuples),
		metricPostgresqlVacuumNumDeadTuples:                     newMetricPostgresqlVacuumNumDeadTuples(mbc.Metrics.PostgresqlVacuumNumDeadTuples),
		metricPostgresqlVacuumed:                                newMetricPostgresqlVacuumed(mbc.Metrics.PostgresqlVacuumed),
		metricPostgresqlWalBuffersFull:                          newMetricPostgresqlWalBuffersFull(mbc.Metrics.PostgresqlWalBuffersFull),
		metricPostgresqlWalBytes:                                newMetricPostgresqlWalBytes(mbc.Metrics.PostgresqlWalBytes),
		metricPostgresqlWalFpi:                                  newMetricPostgresqlWalFpi(mbc.Metrics.PostgresqlWalFpi),
		metricPostgresqlWalRecords:                              newMetricPostgresqlWalRecords(mbc.Metrics.PostgresqlWalRecords),
		metricPostgresqlWalSync:                                 newMetricPostgresqlWalSync(mbc.Metrics.PostgresqlWalSync),
		metricPostgresqlWalSyncTime:                             newMetricPostgresqlWalSyncTime(mbc.Metrics.PostgresqlWalSyncTime),
		metricPostgresqlWalWrite:                                newMetricPostgresqlWalWrite(mbc.Metrics.PostgresqlWalWrite),
		metricPostgresqlWalWriteTime:                            newMetricPostgresqlWalWriteTime(mbc.Metrics.PostgresqlWalWriteTime),
		metricPostgresqlWalFilesAge:                             newMetricPostgresqlWalFilesAge(mbc.Metrics.PostgresqlWalFilesAge),
		metricPostgresqlWalFilesCount:                           newMetricPostgresqlWalFilesCount(mbc.Metrics.PostgresqlWalFilesCount),
		metricPostgresqlWalFilesSize:                            newMetricPostgresqlWalFilesSize(mbc.Metrics.PostgresqlWalFilesSize),
		metricPostgresqlWalReceiverConnected:                    newMetricPostgresqlWalReceiverConnected(mbc.Metrics.PostgresqlWalReceiverConnected),
		metricPostgresqlWalReceiverLastMsgReceiptAge:            newMetricPostgresqlWalReceiverLastMsgReceiptAge(mbc.Metrics.PostgresqlWalReceiverLastMsgReceiptAge),
		metricPostgresqlWalReceiverLastMsgSendAge:               newMetricPostgresqlWalReceiverLastMsgSendAge(mbc.Metrics.PostgresqlWalReceiverLastMsgSendAge),
		metricPostgresqlWalReceiverLatestEndAge:                 newMetricPostgresqlWalReceiverLatestEndAge(mbc.Metrics.PostgresqlWalReceiverLatestEndAge),
		metricPostgresqlWalReceiverReceivedTimeline:             newMetricPostgresqlWalReceiverReceivedTimeline(mbc.Metrics.PostgresqlWalReceiverReceivedTimeline),
		resourceAttributeIncludeFilter:                          make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:                          make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.DatabaseName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["database_name"] = filter.CreateFilter(mbc.ResourceAttributes.DatabaseName.MetricsInclude)
	}
	if mbc.ResourceAttributes.DatabaseName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["database_name"] = filter.CreateFilter(mbc.ResourceAttributes.DatabaseName.MetricsExclude)
	}
	if mbc.ResourceAttributes.DbSystem.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["db.system"] = filter.CreateFilter(mbc.ResourceAttributes.DbSystem.MetricsInclude)
	}
	if mbc.ResourceAttributes.DbSystem.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["db.system"] = filter.CreateFilter(mbc.ResourceAttributes.DbSystem.MetricsExclude)
	}
	if mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["newrelicpostgresql.instance_name"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsInclude)
	}
	if mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["newrelicpostgresql.instance_name"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlVersion.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.version"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlVersion.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlVersion.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.version"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlVersion.MetricsExclude)
	}
	if mbc.ResourceAttributes.ServerAddress.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["server.address"] = filter.CreateFilter(mbc.ResourceAttributes.ServerAddress.MetricsInclude)
	}
	if mbc.ResourceAttributes.ServerAddress.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["server.address"] = filter.CreateFilter(mbc.ResourceAttributes.ServerAddress.MetricsExclude)
	}
	if mbc.ResourceAttributes.ServerPort.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["server.port"] = filter.CreateFilter(mbc.ResourceAttributes.ServerPort.MetricsInclude)
	}
	if mbc.ResourceAttributes.ServerPort.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["server.port"] = filter.CreateFilter(mbc.ResourceAttributes.ServerPort.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricPgbouncerStatsAvgBytesIn.emit(ils.Metrics())
	mb.metricPgbouncerStatsAvgBytesOut.emit(ils.Metrics())
	mb.metricPgbouncerStatsAvgRequestsPerSecond.emit(ils.Metrics())
	mb.metricPgbouncerStatsAvgServerAssignmentCount.emit(ils.Metrics())
	mb.metricPgbouncerStatsAvgTransactionCount.emit(ils.Metrics())
	mb.metricPgbouncerStatsAvgTransactionDurationMilliseconds.emit(ils.Metrics())
	mb.metricPgbouncerStatsBytesInPerSecond.emit(ils.Metrics())
	mb.metricPgbouncerStatsBytesOutPerSecond.emit(ils.Metrics())
	mb.metricPgbouncerStatsQueriesPerSecond.emit(ils.Metrics())
	mb.metricPgbouncerStatsRequestsPerSecond.emit(ils.Metrics())
	mb.metricPgbouncerStatsTotalServerAssignmentCount.emit(ils.Metrics())
	mb.metricPgbouncerStatsTransactionsPerSecond.emit(ils.Metrics())
	mb.metricPostgresqlActiveWaitingQueries.emit(ils.Metrics())
	mb.metricPostgresqlActivityBackendXidAge.emit(ils.Metrics())
	mb.metricPostgresqlActivityBackendXminAge.emit(ils.Metrics())
	mb.metricPostgresqlActivityWaitEvent.emit(ils.Metrics())
	mb.metricPostgresqlActivityXactStartAge.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeChildTablesDone.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeChildTablesTotal.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeExtStatsComputed.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeExtStatsTotal.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeSampleBlksScanned.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzeSampleBlksTotal.emit(ils.Metrics())
	mb.metricPostgresqlAnalyzed.emit(ils.Metrics())
	mb.metricPostgresqlArchiverArchivedCount.emit(ils.Metrics())
	mb.metricPostgresqlArchiverFailedCount.emit(ils.Metrics())
	mb.metricPostgresqlAutoanalyzed.emit(ils.Metrics())
	mb.metricPostgresqlAutovacuumed.emit(ils.Metrics())
	mb.metricPostgresqlBeforeXidWraparound.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersAlloc.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersBackend.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersBackendFsync.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersCheckpoint.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersClean.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterCheckpointsRequested.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterCheckpointsTimed.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterMaxwrittenClean.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterSyncTime.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterWriteTime.emit(ils.Metrics())
	mb.metricPostgresqlBlkReadTime.emit(ils.Metrics())
	mb.metricPostgresqlBlkWriteTime.emit(ils.Metrics())
	mb.metricPostgresqlBufferHit.emit(ils.Metrics())
	mb.metricPostgresqlBuffercacheDirtyBuffers.emit(ils.Metrics())
	mb.metricPostgresqlBuffercachePinningBackends.emit(ils.Metrics())
	mb.metricPostgresqlBuffercacheUnusedBuffers.emit(ils.Metrics())
	mb.metricPostgresqlBuffercacheUsageCount.emit(ils.Metrics())
	mb.metricPostgresqlBuffercacheUsedBuffers.emit(ils.Metrics())
	mb.metricPostgresqlChecksumsEnabled.emit(ils.Metrics())
	mb.metricPostgresqlChecksumsFailures.emit(ils.Metrics())
	mb.metricPostgresqlClusterVacuumHeapBlksScanned.emit(ils.Metrics())
	mb.metricPostgresqlClusterVacuumHeapBlksTotal.emit(ils.Metrics())
	mb.metricPostgresqlClusterVacuumHeapTuplesScanned.emit(ils.Metrics())
	mb.metricPostgresqlClusterVacuumHeapTuplesWritten.emit(ils.Metrics())
	mb.metricPostgresqlCommits.emit(ils.Metrics())
	mb.metricPostgresqlConflicts.emit(ils.Metrics())
	mb.metricPostgresqlConflictsBufferpin.emit(ils.Metrics())
	mb.metricPostgresqlConflictsDeadlock.emit(ils.Metrics())
	mb.metricPostgresqlConflictsLock.emit(ils.Metrics())
	mb.metricPostgresqlConflictsSnapshot.emit(ils.Metrics())
	mb.metricPostgresqlConflictsTablespace.emit(ils.Metrics())
	mb.metricPostgresqlConnections.emit(ils.Metrics())
	mb.metricPostgresqlControlCheckpointDelay.emit(ils.Metrics())
	mb.metricPostgresqlControlCheckpointDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlControlRedoDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlControlTimelineID.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexBlocksDone.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexBlocksTotal.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexLockersDone.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexLockersTotal.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexPartitionsDone.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexPartitionsTotal.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexTuplesDone.emit(ils.Metrics())
	mb.metricPostgresqlCreateIndexTuplesTotal.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseSize.emit(ils.Metrics())
	mb.metricPostgresqlDbCount.emit(ils.Metrics())
	mb.metricPostgresqlDeadlocks.emit(ils.Metrics())
	mb.metricPostgresqlDiskRead.emit(ils.Metrics())
	mb.metricPostgresqlHeapBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlHeapBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlIndexBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlIndexBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlIndexScans.emit(ils.Metrics())
	mb.metricPostgresqlIndexSize.emit(ils.Metrics())
	mb.metricPostgresqlIndexTuplesFetched.emit(ils.Metrics())
	mb.metricPostgresqlIndexTuplesRead.emit(ils.Metrics())
	mb.metricPostgresqlLastAnalyzeAge.emit(ils.Metrics())
	mb.metricPostgresqlLastAutoanalyzeAge.emit(ils.Metrics())
	mb.metricPostgresqlLastAutovacuumAge.emit(ils.Metrics())
	mb.metricPostgresqlLastVacuumAge.emit(ils.Metrics())
	mb.metricPostgresqlMaxConnections.emit(ils.Metrics())
	mb.metricPostgresqlPercentUsageConnections.emit(ils.Metrics())
	mb.metricPostgresqlPgStatStatementsDealloc.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchBlockDistance.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchHit.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchIoDepth.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchPrefetch.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchSkipFpw.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchSkipInit.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchSkipNew.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchSkipRep.emit(ils.Metrics())
	mb.metricPostgresqlRecoveryPrefetchWalDistance.emit(ils.Metrics())
	mb.metricPostgresqlRelationAllVisible.emit(ils.Metrics())
	mb.metricPostgresqlRelationPages.emit(ils.Metrics())
	mb.metricPostgresqlRelationTuples.emit(ils.Metrics())
	mb.metricPostgresqlRelationXmin.emit(ils.Metrics())
	mb.metricPostgresqlRelationSize.emit(ils.Metrics())
	mb.metricPostgresqlReplicationBackendXminAge.emit(ils.Metrics())
	mb.metricPostgresqlReplicationFlushLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationReplayLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSentLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalFlushLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalReplayLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalWriteLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWriteLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotCatalogXminAge.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotConfirmedFlushDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotRestartDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillCount.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamCount.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotTotalBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotTotalTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotXminAge.emit(ils.Metrics())
	mb.metricPostgresqlRollbacks.emit(ils.Metrics())
	mb.metricPostgresqlRowsDeleted.emit(ils.Metrics())
	mb.metricPostgresqlRowsFetched.emit(ils.Metrics())
	mb.metricPostgresqlRowsInserted.emit(ils.Metrics())
	mb.metricPostgresqlRowsReturned.emit(ils.Metrics())
	mb.metricPostgresqlRowsUpdated.emit(ils.Metrics())
	mb.metricPostgresqlRunning.emit(ils.Metrics())
	mb.metricPostgresqlSessionsAbandoned.emit(ils.Metrics())
	mb.metricPostgresqlSessionsActiveTime.emit(ils.Metrics())
	mb.metricPostgresqlSessionsCount.emit(ils.Metrics())
	mb.metricPostgresqlSessionsFatal.emit(ils.Metrics())
	mb.metricPostgresqlSessionsIdleInTransactionTime.emit(ils.Metrics())
	mb.metricPostgresqlSessionsKilled.emit(ils.Metrics())
	mb.metricPostgresqlSessionsSessionTime.emit(ils.Metrics())
	mb.metricPostgresqlSlruBlksExists.emit(ils.Metrics())
	mb.metricPostgresqlSlruBlksHit.emit(ils.Metrics())
	mb.metricPostgresqlSlruBlksRead.emit(ils.Metrics())
	mb.metricPostgresqlSlruBlksWritten.emit(ils.Metrics())
	mb.metricPostgresqlSlruBlksZeroed.emit(ils.Metrics())
	mb.metricPostgresqlSlruFlushes.emit(ils.Metrics())
	mb.metricPostgresqlSlruTruncates.emit(ils.Metrics())
	mb.metricPostgresqlSnapshotXipCount.emit(ils.Metrics())
	mb.metricPostgresqlSnapshotXmax.emit(ils.Metrics())
	mb.metricPostgresqlSnapshotXmin.emit(ils.Metrics())
	mb.metricPostgresqlSubscriptionApplyError.emit(ils.Metrics())
	mb.metricPostgresqlSubscriptionLastMsgReceiptAge.emit(ils.Metrics())
	mb.metricPostgresqlSubscriptionLastMsgSendAge.emit(ils.Metrics())
	mb.metricPostgresqlSubscriptionLatestEndAge.emit(ils.Metrics())
	mb.metricPostgresqlSubscriptionSyncError.emit(ils.Metrics())
	mb.metricPostgresqlTempBytes.emit(ils.Metrics())
	mb.metricPostgresqlTempFiles.emit(ils.Metrics())
	mb.metricPostgresqlToastAutovacuumed.emit(ils.Metrics())
	mb.metricPostgresqlToastLastAutovacuumAge.emit(ils.Metrics())
	mb.metricPostgresqlToastLastVacuumAge.emit(ils.Metrics())
	mb.metricPostgresqlToastVacuumed.emit(ils.Metrics())
	mb.metricPostgresqlToastBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlToastBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlToastIndexBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlToastIndexBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlToastSize.emit(ils.Metrics())
	mb.metricPostgresqlTransactionsDurationMax.emit(ils.Metrics())
	mb.metricPostgresqlTransactionsDurationSum.emit(ils.Metrics())
	mb.metricPostgresqlUptime.emit(ils.Metrics())
	mb.metricPostgresqlVacuumHeapBlksScanned.emit(ils.Metrics())
	mb.metricPostgresqlVacuumHeapBlksTotal.emit(ils.Metrics())
	mb.metricPostgresqlVacuumHeapBlksVacuumed.emit(ils.Metrics())
	mb.metricPostgresqlVacuumIndexVacuumCount.emit(ils.Metrics())
	mb.metricPostgresqlVacuumMaxDeadTuples.emit(ils.Metrics())
	mb.metricPostgresqlVacuumNumDeadTuples.emit(ils.Metrics())
	mb.metricPostgresqlVacuumed.emit(ils.Metrics())
	mb.metricPostgresqlWalBuffersFull.emit(ils.Metrics())
	mb.metricPostgresqlWalBytes.emit(ils.Metrics())
	mb.metricPostgresqlWalFpi.emit(ils.Metrics())
	mb.metricPostgresqlWalRecords.emit(ils.Metrics())
	mb.metricPostgresqlWalSync.emit(ils.Metrics())
	mb.metricPostgresqlWalSyncTime.emit(ils.Metrics())
	mb.metricPostgresqlWalWrite.emit(ils.Metrics())
	mb.metricPostgresqlWalWriteTime.emit(ils.Metrics())
	mb.metricPostgresqlWalFilesAge.emit(ils.Metrics())
	mb.metricPostgresqlWalFilesCount.emit(ils.Metrics())
	mb.metricPostgresqlWalFilesSize.emit(ils.Metrics())
	mb.metricPostgresqlWalReceiverConnected.emit(ils.Metrics())
	mb.metricPostgresqlWalReceiverLastMsgReceiptAge.emit(ils.Metrics())
	mb.metricPostgresqlWalReceiverLastMsgSendAge.emit(ils.Metrics())
	mb.metricPostgresqlWalReceiverLatestEndAge.emit(ils.Metrics())
	mb.metricPostgresqlWalReceiverReceivedTimeline.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordPgbouncerStatsAvgBytesInDataPoint adds a data point to pgbouncer.stats.avg_bytes_in metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgBytesInDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgBytesIn.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsAvgBytesOutDataPoint adds a data point to pgbouncer.stats.avg_bytes_out metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgBytesOutDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgBytesOut.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsAvgRequestsPerSecondDataPoint adds a data point to pgbouncer.stats.avg_requests_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgRequestsPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgRequestsPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsAvgServerAssignmentCountDataPoint adds a data point to pgbouncer.stats.avg_server_assignment_count metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgServerAssignmentCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgServerAssignmentCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsAvgTransactionCountDataPoint adds a data point to pgbouncer.stats.avg_transaction_count metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgTransactionCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgTransactionCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsAvgTransactionDurationMillisecondsDataPoint adds a data point to pgbouncer.stats.avg_transaction_duration_milliseconds metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsAvgTransactionDurationMillisecondsDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsAvgTransactionDurationMilliseconds.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsBytesInPerSecondDataPoint adds a data point to pgbouncer.stats.bytes_in_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsBytesInPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsBytesInPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsBytesOutPerSecondDataPoint adds a data point to pgbouncer.stats.bytes_out_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsBytesOutPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsBytesOutPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsQueriesPerSecondDataPoint adds a data point to pgbouncer.stats.queries_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsQueriesPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsQueriesPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsRequestsPerSecondDataPoint adds a data point to pgbouncer.stats.requests_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsRequestsPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsRequestsPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsTotalServerAssignmentCountDataPoint adds a data point to pgbouncer.stats.total_server_assignment_count metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsTotalServerAssignmentCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsTotalServerAssignmentCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPgbouncerStatsTransactionsPerSecondDataPoint adds a data point to pgbouncer.stats.transactions_per_second metric.
func (mb *MetricsBuilder) RecordPgbouncerStatsTransactionsPerSecondDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string) {
	mb.metricPgbouncerStatsTransactionsPerSecond.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue)
}

// RecordPostgresqlActiveWaitingQueriesDataPoint adds a data point to postgresql.active_waiting_queries metric.
func (mb *MetricsBuilder) RecordPostgresqlActiveWaitingQueriesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlActiveWaitingQueries.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlActivityBackendXidAgeDataPoint adds a data point to postgresql.activity.backend_xid_age metric.
func (mb *MetricsBuilder) RecordPostgresqlActivityBackendXidAgeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlActivityBackendXidAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlActivityBackendXminAgeDataPoint adds a data point to postgresql.activity.backend_xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlActivityBackendXminAgeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlActivityBackendXminAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlActivityWaitEventDataPoint adds a data point to postgresql.activity.wait_event metric.
func (mb *MetricsBuilder) RecordPostgresqlActivityWaitEventDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string, waitEventAttributeValue string) {
	mb.metricPostgresqlActivityWaitEvent.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue, waitEventAttributeValue)
}

// RecordPostgresqlActivityXactStartAgeDataPoint adds a data point to postgresql.activity.xact_start_age metric.
func (mb *MetricsBuilder) RecordPostgresqlActivityXactStartAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlActivityXactStartAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlAnalyzeChildTablesDoneDataPoint adds a data point to postgresql.analyze.child_tables_done metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeChildTablesDoneDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeChildTablesDone.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzeChildTablesTotalDataPoint adds a data point to postgresql.analyze.child_tables_total metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeChildTablesTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeChildTablesTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzeExtStatsComputedDataPoint adds a data point to postgresql.analyze.ext_stats_computed metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeExtStatsComputedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeExtStatsComputed.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzeExtStatsTotalDataPoint adds a data point to postgresql.analyze.ext_stats_total metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeExtStatsTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeExtStatsTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzeSampleBlksScannedDataPoint adds a data point to postgresql.analyze.sample_blks_scanned metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeSampleBlksScannedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeSampleBlksScanned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzeSampleBlksTotalDataPoint adds a data point to postgresql.analyze.sample_blks_total metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzeSampleBlksTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzeSampleBlksTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAnalyzedDataPoint adds a data point to postgresql.analyzed metric.
func (mb *MetricsBuilder) RecordPostgresqlAnalyzedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAnalyzed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlArchiverArchivedCountDataPoint adds a data point to postgresql.archiver.archived_count metric.
func (mb *MetricsBuilder) RecordPostgresqlArchiverArchivedCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlArchiverArchivedCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlArchiverFailedCountDataPoint adds a data point to postgresql.archiver.failed_count metric.
func (mb *MetricsBuilder) RecordPostgresqlArchiverFailedCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlArchiverFailedCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlAutoanalyzedDataPoint adds a data point to postgresql.autoanalyzed metric.
func (mb *MetricsBuilder) RecordPostgresqlAutoanalyzedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAutoanalyzed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlAutovacuumedDataPoint adds a data point to postgresql.autovacuumed metric.
func (mb *MetricsBuilder) RecordPostgresqlAutovacuumedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlAutovacuumed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlBeforeXidWraparoundDataPoint adds a data point to postgresql.before_xid_wraparound metric.
func (mb *MetricsBuilder) RecordPostgresqlBeforeXidWraparoundDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBeforeXidWraparound.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterBuffersAllocDataPoint adds a data point to postgresql.bgwriter.buffers_alloc metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersAllocDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterBuffersAlloc.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterBuffersBackendDataPoint adds a data point to postgresql.bgwriter.buffers_backend metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersBackendDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterBuffersBackend.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterBuffersBackendFsyncDataPoint adds a data point to postgresql.bgwriter.buffers_backend_fsync metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersBackendFsyncDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterBuffersBackendFsync.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterBuffersCheckpointDataPoint adds a data point to postgresql.bgwriter.buffers_checkpoint metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersCheckpointDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterBuffersCheckpoint.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterBuffersCleanDataPoint adds a data point to postgresql.bgwriter.buffers_clean metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersCleanDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterBuffersClean.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterCheckpointsRequestedDataPoint adds a data point to postgresql.bgwriter.checkpoints_requested metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterCheckpointsRequestedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterCheckpointsRequested.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterCheckpointsTimedDataPoint adds a data point to postgresql.bgwriter.checkpoints_timed metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterCheckpointsTimedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterCheckpointsTimed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterMaxwrittenCleanDataPoint adds a data point to postgresql.bgwriter.maxwritten_clean metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterMaxwrittenCleanDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterMaxwrittenClean.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterSyncTimeDataPoint adds a data point to postgresql.bgwriter.sync_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterSyncTimeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterSyncTime.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBgwriterWriteTimeDataPoint adds a data point to postgresql.bgwriter.write_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterWriteTimeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBgwriterWriteTime.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBlkReadTimeDataPoint adds a data point to postgresql.blk_read_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBlkReadTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBlkReadTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBlkWriteTimeDataPoint adds a data point to postgresql.blk_write_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBlkWriteTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBlkWriteTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBufferHitDataPoint adds a data point to postgresql.buffer_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlBufferHitDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBufferHit.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBuffercacheDirtyBuffersDataPoint adds a data point to postgresql.buffercache.dirty_buffers metric.
func (mb *MetricsBuilder) RecordPostgresqlBuffercacheDirtyBuffersDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlBuffercacheDirtyBuffers.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlBuffercachePinningBackendsDataPoint adds a data point to postgresql.buffercache.pinning_backends metric.
func (mb *MetricsBuilder) RecordPostgresqlBuffercachePinningBackendsDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlBuffercachePinningBackends.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlBuffercacheUnusedBuffersDataPoint adds a data point to postgresql.buffercache.unused_buffers metric.
func (mb *MetricsBuilder) RecordPostgresqlBuffercacheUnusedBuffersDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlBuffercacheUnusedBuffers.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlBuffercacheUsageCountDataPoint adds a data point to postgresql.buffercache.usage_count metric.
func (mb *MetricsBuilder) RecordPostgresqlBuffercacheUsageCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlBuffercacheUsageCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlBuffercacheUsedBuffersDataPoint adds a data point to postgresql.buffercache.used_buffers metric.
func (mb *MetricsBuilder) RecordPostgresqlBuffercacheUsedBuffersDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlBuffercacheUsedBuffers.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlChecksumsEnabledDataPoint adds a data point to postgresql.checksums.enabled metric.
func (mb *MetricsBuilder) RecordPostgresqlChecksumsEnabledDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlChecksumsEnabled.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlChecksumsFailuresDataPoint adds a data point to postgresql.checksums.failures metric.
func (mb *MetricsBuilder) RecordPostgresqlChecksumsFailuresDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlChecksumsFailures.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlClusterVacuumHeapBlksScannedDataPoint adds a data point to postgresql.cluster_vacuum.heap_blks_scanned metric.
func (mb *MetricsBuilder) RecordPostgresqlClusterVacuumHeapBlksScannedDataPoint(ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlClusterVacuumHeapBlksScanned.recordDataPoint(mb.startTime, ts, val, commandAttributeValue, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlClusterVacuumHeapBlksTotalDataPoint adds a data point to postgresql.cluster_vacuum.heap_blks_total metric.
func (mb *MetricsBuilder) RecordPostgresqlClusterVacuumHeapBlksTotalDataPoint(ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlClusterVacuumHeapBlksTotal.recordDataPoint(mb.startTime, ts, val, commandAttributeValue, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlClusterVacuumHeapTuplesScannedDataPoint adds a data point to postgresql.cluster_vacuum.heap_tuples_scanned metric.
func (mb *MetricsBuilder) RecordPostgresqlClusterVacuumHeapTuplesScannedDataPoint(ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlClusterVacuumHeapTuplesScanned.recordDataPoint(mb.startTime, ts, val, commandAttributeValue, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlClusterVacuumHeapTuplesWrittenDataPoint adds a data point to postgresql.cluster_vacuum.heap_tuples_written metric.
func (mb *MetricsBuilder) RecordPostgresqlClusterVacuumHeapTuplesWrittenDataPoint(ts pcommon.Timestamp, val int64, commandAttributeValue string, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlClusterVacuumHeapTuplesWritten.recordDataPoint(mb.startTime, ts, val, commandAttributeValue, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCommitsDataPoint adds a data point to postgresql.commits metric.
func (mb *MetricsBuilder) RecordPostgresqlCommitsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlCommits.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsDataPoint adds a data point to postgresql.conflicts metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflicts.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsBufferpinDataPoint adds a data point to postgresql.conflicts.bufferpin metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsBufferpinDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsBufferpin.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsDeadlockDataPoint adds a data point to postgresql.conflicts.deadlock metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsDeadlockDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsDeadlock.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsLockDataPoint adds a data point to postgresql.conflicts.lock metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsLockDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsLock.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsSnapshotDataPoint adds a data point to postgresql.conflicts.snapshot metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsSnapshotDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsSnapshot.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsTablespaceDataPoint adds a data point to postgresql.conflicts.tablespace metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsTablespaceDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsTablespace.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConnectionsDataPoint adds a data point to postgresql.connections metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConnections.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlControlCheckpointDelayDataPoint adds a data point to postgresql.control.checkpoint_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlControlCheckpointDelayDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlControlCheckpointDelay.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlControlCheckpointDelayBytesDataPoint adds a data point to postgresql.control.checkpoint_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlControlCheckpointDelayBytesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlControlCheckpointDelayBytes.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlControlRedoDelayBytesDataPoint adds a data point to postgresql.control.redo_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlControlRedoDelayBytesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlControlRedoDelayBytes.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlControlTimelineIDDataPoint adds a data point to postgresql.control.timeline_id metric.
func (mb *MetricsBuilder) RecordPostgresqlControlTimelineIDDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlControlTimelineID.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlCreateIndexBlocksDoneDataPoint adds a data point to postgresql.create_index.blocks_done metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexBlocksDoneDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexBlocksDone.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexBlocksTotalDataPoint adds a data point to postgresql.create_index.blocks_total metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexBlocksTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexBlocksTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexLockersDoneDataPoint adds a data point to postgresql.create_index.lockers_done metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexLockersDoneDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexLockersDone.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexLockersTotalDataPoint adds a data point to postgresql.create_index.lockers_total metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexLockersTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexLockersTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexPartitionsDoneDataPoint adds a data point to postgresql.create_index.partitions_done metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexPartitionsDoneDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexPartitionsDone.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexPartitionsTotalDataPoint adds a data point to postgresql.create_index.partitions_total metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexPartitionsTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexPartitionsTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexTuplesDoneDataPoint adds a data point to postgresql.create_index.tuples_done metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexTuplesDoneDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexTuplesDone.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlCreateIndexTuplesTotalDataPoint adds a data point to postgresql.create_index.tuples_total metric.
func (mb *MetricsBuilder) RecordPostgresqlCreateIndexTuplesTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, indexNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlCreateIndexTuplesTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, indexNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlDatabaseSizeDataPoint adds a data point to postgresql.database_size metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseSizeDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDatabaseSize.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDbCountDataPoint adds a data point to postgresql.db.count metric.
func (mb *MetricsBuilder) RecordPostgresqlDbCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDbCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDeadlocksDataPoint adds a data point to postgresql.deadlocks metric.
func (mb *MetricsBuilder) RecordPostgresqlDeadlocksDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDeadlocks.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDiskReadDataPoint adds a data point to postgresql.disk_read metric.
func (mb *MetricsBuilder) RecordPostgresqlDiskReadDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDiskRead.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlHeapBlocksHitDataPoint adds a data point to postgresql.heap_blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlHeapBlocksHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlHeapBlocksHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlHeapBlocksReadDataPoint adds a data point to postgresql.heap_blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlHeapBlocksReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlHeapBlocksRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlIndexBlocksHitDataPoint adds a data point to postgresql.index_blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexBlocksHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlIndexBlocksHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlIndexBlocksReadDataPoint adds a data point to postgresql.index_blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexBlocksReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlIndexBlocksRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlIndexScansDataPoint adds a data point to postgresql.index_scans metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexScansDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	mb.metricPostgresqlIndexScans.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue, indexNameAttributeValue)
}

// RecordPostgresqlIndexSizeDataPoint adds a data point to postgresql.index_size metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexSizeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	mb.metricPostgresqlIndexSize.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue, indexNameAttributeValue)
}

// RecordPostgresqlIndexTuplesFetchedDataPoint adds a data point to postgresql.index_tuples_fetched metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexTuplesFetchedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	mb.metricPostgresqlIndexTuplesFetched.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue, indexNameAttributeValue)
}

// RecordPostgresqlIndexTuplesReadDataPoint adds a data point to postgresql.index_tuples_read metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexTuplesReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string, indexNameAttributeValue string) {
	mb.metricPostgresqlIndexTuplesRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue, indexNameAttributeValue)
}

// RecordPostgresqlLastAnalyzeAgeDataPoint adds a data point to postgresql.last_analyze_age metric.
func (mb *MetricsBuilder) RecordPostgresqlLastAnalyzeAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlLastAnalyzeAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlLastAutoanalyzeAgeDataPoint adds a data point to postgresql.last_autoanalyze_age metric.
func (mb *MetricsBuilder) RecordPostgresqlLastAutoanalyzeAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlLastAutoanalyzeAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlLastAutovacuumAgeDataPoint adds a data point to postgresql.last_autovacuum_age metric.
func (mb *MetricsBuilder) RecordPostgresqlLastAutovacuumAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlLastAutovacuumAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlLastVacuumAgeDataPoint adds a data point to postgresql.last_vacuum_age metric.
func (mb *MetricsBuilder) RecordPostgresqlLastVacuumAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlLastVacuumAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlMaxConnectionsDataPoint adds a data point to postgresql.max_connections metric.
func (mb *MetricsBuilder) RecordPostgresqlMaxConnectionsDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlMaxConnections.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlPercentUsageConnectionsDataPoint adds a data point to postgresql.percent_usage_connections metric.
func (mb *MetricsBuilder) RecordPostgresqlPercentUsageConnectionsDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlPercentUsageConnections.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlPgStatStatementsDeallocDataPoint adds a data point to postgresql.pg_stat_statements.dealloc metric.
func (mb *MetricsBuilder) RecordPostgresqlPgStatStatementsDeallocDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlPgStatStatementsDealloc.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchBlockDistanceDataPoint adds a data point to postgresql.recovery_prefetch.block_distance metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchBlockDistanceDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchBlockDistance.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchHitDataPoint adds a data point to postgresql.recovery_prefetch.hit metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchIoDepthDataPoint adds a data point to postgresql.recovery_prefetch.io_depth metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchIoDepthDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchIoDepth.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchPrefetchDataPoint adds a data point to postgresql.recovery_prefetch.prefetch metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchPrefetchDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchPrefetch.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchSkipFpwDataPoint adds a data point to postgresql.recovery_prefetch.skip_fpw metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchSkipFpwDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchSkipFpw.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchSkipInitDataPoint adds a data point to postgresql.recovery_prefetch.skip_init metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchSkipInitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchSkipInit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchSkipNewDataPoint adds a data point to postgresql.recovery_prefetch.skip_new metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchSkipNewDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchSkipNew.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchSkipRepDataPoint adds a data point to postgresql.recovery_prefetch.skip_rep metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchSkipRepDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchSkipRep.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRecoveryPrefetchWalDistanceDataPoint adds a data point to postgresql.recovery_prefetch.wal_distance metric.
func (mb *MetricsBuilder) RecordPostgresqlRecoveryPrefetchWalDistanceDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRecoveryPrefetchWalDistance.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRelationAllVisibleDataPoint adds a data point to postgresql.relation.all_visible metric.
func (mb *MetricsBuilder) RecordPostgresqlRelationAllVisibleDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlRelationAllVisible.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlRelationPagesDataPoint adds a data point to postgresql.relation.pages metric.
func (mb *MetricsBuilder) RecordPostgresqlRelationPagesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlRelationPages.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlRelationTuplesDataPoint adds a data point to postgresql.relation.tuples metric.
func (mb *MetricsBuilder) RecordPostgresqlRelationTuplesDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlRelationTuples.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlRelationXminDataPoint adds a data point to postgresql.relation.xmin metric.
func (mb *MetricsBuilder) RecordPostgresqlRelationXminDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlRelationXmin.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlRelationSizeDataPoint adds a data point to postgresql.relation_size metric.
func (mb *MetricsBuilder) RecordPostgresqlRelationSizeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlRelationSize.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlReplicationBackendXminAgeDataPoint adds a data point to postgresql.replication.backend_xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationBackendXminAgeDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationBackendXminAge.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationFlushLsnDelayDataPoint adds a data point to postgresql.replication.flush_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationFlushLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationFlushLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationReplayLsnDelayDataPoint adds a data point to postgresql.replication.replay_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationReplayLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationReplayLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationSentLsnDelayDataPoint adds a data point to postgresql.replication.sent_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSentLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationSentLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalFlushLagDataPoint adds a data point to postgresql.replication.wal_flush_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalFlushLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalFlushLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalReplayLagDataPoint adds a data point to postgresql.replication.wal_replay_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalReplayLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalReplayLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalWriteLagDataPoint adds a data point to postgresql.replication.wal_write_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalWriteLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalWriteLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWriteLsnDelayDataPoint adds a data point to postgresql.replication.write_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWriteLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWriteLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationDelayDataPoint adds a data point to postgresql.replication_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationDelayDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlReplicationDelay.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlReplicationDelayBytesDataPoint adds a data point to postgresql.replication_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationDelayBytesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlReplicationDelayBytes.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlReplicationSlotCatalogXminAgeDataPoint adds a data point to postgresql.replication_slot.catalog_xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotCatalogXminAgeDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotCatalogXminAge.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotConfirmedFlushDelayBytesDataPoint adds a data point to postgresql.replication_slot.confirmed_flush_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotConfirmedFlushDelayBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotConfirmedFlushDelayBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotRestartDelayBytesDataPoint adds a data point to postgresql.replication_slot.restart_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotRestartDelayBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotRestartDelayBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillBytesDataPoint adds a data point to postgresql.replication_slot.spill_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillCountDataPoint adds a data point to postgresql.replication_slot.spill_count metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillCountDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillCount.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillTxnsDataPoint adds a data point to postgresql.replication_slot.spill_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamBytesDataPoint adds a data point to postgresql.replication_slot.stream_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamCountDataPoint adds a data point to postgresql.replication_slot.stream_count metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamCountDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamCount.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamTxnsDataPoint adds a data point to postgresql.replication_slot.stream_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotTotalBytesDataPoint adds a data point to postgresql.replication_slot.total_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotTotalBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotTotalBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotTotalTxnsDataPoint adds a data point to postgresql.replication_slot.total_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotTotalTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotTotalTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotXminAgeDataPoint adds a data point to postgresql.replication_slot.xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotXminAgeDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotXminAge.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlRollbacksDataPoint adds a data point to postgresql.rollbacks metric.
func (mb *MetricsBuilder) RecordPostgresqlRollbacksDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRollbacks.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsDeletedDataPoint adds a data point to postgresql.rows_deleted metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsDeletedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsDeleted.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsFetchedDataPoint adds a data point to postgresql.rows_fetched metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsFetchedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsFetched.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsInsertedDataPoint adds a data point to postgresql.rows_inserted metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsInsertedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsInserted.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsReturnedDataPoint adds a data point to postgresql.rows_returned metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsReturnedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsReturned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsUpdatedDataPoint adds a data point to postgresql.rows_updated metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsUpdatedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsUpdated.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRunningDataPoint adds a data point to postgresql.running metric.
func (mb *MetricsBuilder) RecordPostgresqlRunningDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRunning.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsAbandonedDataPoint adds a data point to postgresql.sessions.abandoned metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsAbandonedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsAbandoned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsActiveTimeDataPoint adds a data point to postgresql.sessions.active_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsActiveTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsActiveTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsCountDataPoint adds a data point to postgresql.sessions.count metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsCountDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsCount.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsFatalDataPoint adds a data point to postgresql.sessions.fatal metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsFatalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsFatal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsIdleInTransactionTimeDataPoint adds a data point to postgresql.sessions.idle_in_transaction_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsIdleInTransactionTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsIdleInTransactionTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsKilledDataPoint adds a data point to postgresql.sessions.killed metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsKilledDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsKilled.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsSessionTimeDataPoint adds a data point to postgresql.sessions.session_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsSessionTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsSessionTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSlruBlksExistsDataPoint adds a data point to postgresql.slru.blks_exists metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruBlksExistsDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruBlksExists.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruBlksHitDataPoint adds a data point to postgresql.slru.blks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruBlksHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruBlksHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruBlksReadDataPoint adds a data point to postgresql.slru.blks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruBlksReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruBlksRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruBlksWrittenDataPoint adds a data point to postgresql.slru.blks_written metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruBlksWrittenDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruBlksWritten.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruBlksZeroedDataPoint adds a data point to postgresql.slru.blks_zeroed metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruBlksZeroedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruBlksZeroed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruFlushesDataPoint adds a data point to postgresql.slru.flushes metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruFlushesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruFlushes.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSlruTruncatesDataPoint adds a data point to postgresql.slru.truncates metric.
func (mb *MetricsBuilder) RecordPostgresqlSlruTruncatesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, slruNameAttributeValue string) {
	mb.metricPostgresqlSlruTruncates.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, slruNameAttributeValue)
}

// RecordPostgresqlSnapshotXipCountDataPoint adds a data point to postgresql.snapshot.xip_count metric.
func (mb *MetricsBuilder) RecordPostgresqlSnapshotXipCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSnapshotXipCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSnapshotXmaxDataPoint adds a data point to postgresql.snapshot.xmax metric.
func (mb *MetricsBuilder) RecordPostgresqlSnapshotXmaxDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSnapshotXmax.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSnapshotXminDataPoint adds a data point to postgresql.snapshot.xmin metric.
func (mb *MetricsBuilder) RecordPostgresqlSnapshotXminDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSnapshotXmin.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSubscriptionApplyErrorDataPoint adds a data point to postgresql.subscription.apply_error metric.
func (mb *MetricsBuilder) RecordPostgresqlSubscriptionApplyErrorDataPoint(ts pcommon.Timestamp, val int64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSubscriptionApplyError.recordDataPoint(mb.startTime, ts, val, subscriptionNameAttributeValue, stateAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSubscriptionLastMsgReceiptAgeDataPoint adds a data point to postgresql.subscription.last_msg_receipt_age metric.
func (mb *MetricsBuilder) RecordPostgresqlSubscriptionLastMsgReceiptAgeDataPoint(ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSubscriptionLastMsgReceiptAge.recordDataPoint(mb.startTime, ts, val, subscriptionNameAttributeValue, stateAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSubscriptionLastMsgSendAgeDataPoint adds a data point to postgresql.subscription.last_msg_send_age metric.
func (mb *MetricsBuilder) RecordPostgresqlSubscriptionLastMsgSendAgeDataPoint(ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSubscriptionLastMsgSendAge.recordDataPoint(mb.startTime, ts, val, subscriptionNameAttributeValue, stateAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSubscriptionLatestEndAgeDataPoint adds a data point to postgresql.subscription.latest_end_age metric.
func (mb *MetricsBuilder) RecordPostgresqlSubscriptionLatestEndAgeDataPoint(ts pcommon.Timestamp, val float64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSubscriptionLatestEndAge.recordDataPoint(mb.startTime, ts, val, subscriptionNameAttributeValue, stateAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSubscriptionSyncErrorDataPoint adds a data point to postgresql.subscription.sync_error metric.
func (mb *MetricsBuilder) RecordPostgresqlSubscriptionSyncErrorDataPoint(ts pcommon.Timestamp, val int64, subscriptionNameAttributeValue string, stateAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSubscriptionSyncError.recordDataPoint(mb.startTime, ts, val, subscriptionNameAttributeValue, stateAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlTempBytesDataPoint adds a data point to postgresql.temp_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlTempBytesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlTempBytes.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlTempFilesDataPoint adds a data point to postgresql.temp_files metric.
func (mb *MetricsBuilder) RecordPostgresqlTempFilesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlTempFiles.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlToastAutovacuumedDataPoint adds a data point to postgresql.toast.autovacuumed metric.
func (mb *MetricsBuilder) RecordPostgresqlToastAutovacuumedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastAutovacuumed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastLastAutovacuumAgeDataPoint adds a data point to postgresql.toast.last_autovacuum_age metric.
func (mb *MetricsBuilder) RecordPostgresqlToastLastAutovacuumAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastLastAutovacuumAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastLastVacuumAgeDataPoint adds a data point to postgresql.toast.last_vacuum_age metric.
func (mb *MetricsBuilder) RecordPostgresqlToastLastVacuumAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastLastVacuumAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastVacuumedDataPoint adds a data point to postgresql.toast.vacuumed metric.
func (mb *MetricsBuilder) RecordPostgresqlToastVacuumedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastVacuumed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastBlocksHitDataPoint adds a data point to postgresql.toast_blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlToastBlocksHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastBlocksHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastBlocksReadDataPoint adds a data point to postgresql.toast_blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlToastBlocksReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastBlocksRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastIndexBlocksHitDataPoint adds a data point to postgresql.toast_index_blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlToastIndexBlocksHitDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastIndexBlocksHit.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastIndexBlocksReadDataPoint adds a data point to postgresql.toast_index_blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlToastIndexBlocksReadDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastIndexBlocksRead.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlToastSizeDataPoint adds a data point to postgresql.toast_size metric.
func (mb *MetricsBuilder) RecordPostgresqlToastSizeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlToastSize.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlTransactionsDurationMaxDataPoint adds a data point to postgresql.transactions.duration.max metric.
func (mb *MetricsBuilder) RecordPostgresqlTransactionsDurationMaxDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlTransactionsDurationMax.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlTransactionsDurationSumDataPoint adds a data point to postgresql.transactions.duration.sum metric.
func (mb *MetricsBuilder) RecordPostgresqlTransactionsDurationSumDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string, databaseNameAttributeValue string, userNameAttributeValue string, applicationNameAttributeValue string, backendTypeAttributeValue string) {
	mb.metricPostgresqlTransactionsDurationSum.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, databaseNameAttributeValue, userNameAttributeValue, applicationNameAttributeValue, backendTypeAttributeValue)
}

// RecordPostgresqlUptimeDataPoint adds a data point to postgresql.uptime metric.
func (mb *MetricsBuilder) RecordPostgresqlUptimeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlUptime.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlVacuumHeapBlksScannedDataPoint adds a data point to postgresql.vacuum.heap_blks_scanned metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumHeapBlksScannedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumHeapBlksScanned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumHeapBlksTotalDataPoint adds a data point to postgresql.vacuum.heap_blks_total metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumHeapBlksTotalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumHeapBlksTotal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumHeapBlksVacuumedDataPoint adds a data point to postgresql.vacuum.heap_blks_vacuumed metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumHeapBlksVacuumedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumHeapBlksVacuumed.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumIndexVacuumCountDataPoint adds a data point to postgresql.vacuum.index_vacuum_count metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumIndexVacuumCountDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumIndexVacuumCount.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumMaxDeadTuplesDataPoint adds a data point to postgresql.vacuum.max_dead_tuples metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumMaxDeadTuplesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumMaxDeadTuples.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumNumDeadTuplesDataPoint adds a data point to postgresql.vacuum.num_dead_tuples metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumNumDeadTuplesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumNumDeadTuples.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlVacuumedDataPoint adds a data point to postgresql.vacuumed metric.
func (mb *MetricsBuilder) RecordPostgresqlVacuumedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string, schemaNameAttributeValue string, tableNameAttributeValue string) {
	mb.metricPostgresqlVacuumed.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue, schemaNameAttributeValue, tableNameAttributeValue)
}

// RecordPostgresqlWalBuffersFullDataPoint adds a data point to postgresql.wal.buffers_full metric.
func (mb *MetricsBuilder) RecordPostgresqlWalBuffersFullDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalBuffersFull.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalBytesDataPoint adds a data point to postgresql.wal.bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlWalBytesDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalBytes.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalFpiDataPoint adds a data point to postgresql.wal.fpi metric.
func (mb *MetricsBuilder) RecordPostgresqlWalFpiDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalFpi.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalRecordsDataPoint adds a data point to postgresql.wal.records metric.
func (mb *MetricsBuilder) RecordPostgresqlWalRecordsDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalRecords.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalSyncDataPoint adds a data point to postgresql.wal.sync metric.
func (mb *MetricsBuilder) RecordPostgresqlWalSyncDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalSync.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalSyncTimeDataPoint adds a data point to postgresql.wal.sync_time metric.
func (mb *MetricsBuilder) RecordPostgresqlWalSyncTimeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalSyncTime.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalWriteDataPoint adds a data point to postgresql.wal.write metric.
func (mb *MetricsBuilder) RecordPostgresqlWalWriteDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalWrite.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalWriteTimeDataPoint adds a data point to postgresql.wal.write_time metric.
func (mb *MetricsBuilder) RecordPostgresqlWalWriteTimeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalWriteTime.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalFilesAgeDataPoint adds a data point to postgresql.wal_files.age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalFilesAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalFilesAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalFilesCountDataPoint adds a data point to postgresql.wal_files.count metric.
func (mb *MetricsBuilder) RecordPostgresqlWalFilesCountDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalFilesCount.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalFilesSizeDataPoint adds a data point to postgresql.wal_files.size metric.
func (mb *MetricsBuilder) RecordPostgresqlWalFilesSizeDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalFilesSize.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalReceiverConnectedDataPoint adds a data point to postgresql.wal_receiver.connected metric.
func (mb *MetricsBuilder) RecordPostgresqlWalReceiverConnectedDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalReceiverConnected.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalReceiverLastMsgReceiptAgeDataPoint adds a data point to postgresql.wal_receiver.last_msg_receipt_age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalReceiverLastMsgReceiptAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalReceiverLastMsgReceiptAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalReceiverLastMsgSendAgeDataPoint adds a data point to postgresql.wal_receiver.last_msg_send_age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalReceiverLastMsgSendAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalReceiverLastMsgSendAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalReceiverLatestEndAgeDataPoint adds a data point to postgresql.wal_receiver.latest_end_age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalReceiverLatestEndAgeDataPoint(ts pcommon.Timestamp, val float64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalReceiverLatestEndAge.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlWalReceiverReceivedTimelineDataPoint adds a data point to postgresql.wal_receiver.received_timeline metric.
func (mb *MetricsBuilder) RecordPostgresqlWalReceiverReceivedTimelineDataPoint(ts pcommon.Timestamp, val int64, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlWalReceiverReceivedTimeline.recordDataPoint(mb.startTime, ts, val, newrelicpostgresqlInstanceNameAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
