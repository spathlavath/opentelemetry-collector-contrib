// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

var MetricsInfo = metricsInfo{
	PostgresqlBeforeXidWraparound: metricInfo{
		Name: "postgresql.before_xid_wraparound",
	},
	PostgresqlBlkReadTime: metricInfo{
		Name: "postgresql.blk_read_time",
	},
	PostgresqlBlkWriteTime: metricInfo{
		Name: "postgresql.blk_write_time",
	},
	PostgresqlBufferHit: metricInfo{
		Name: "postgresql.buffer_hit",
	},
	PostgresqlChecksumsEnabled: metricInfo{
		Name: "postgresql.checksums.enabled",
	},
	PostgresqlChecksumsFailures: metricInfo{
		Name: "postgresql.checksums.failures",
	},
	PostgresqlCommits: metricInfo{
		Name: "postgresql.commits",
	},
	PostgresqlConflicts: metricInfo{
		Name: "postgresql.conflicts",
	},
	PostgresqlConflictsBufferpin: metricInfo{
		Name: "postgresql.conflicts.bufferpin",
	},
	PostgresqlConflictsDeadlock: metricInfo{
		Name: "postgresql.conflicts.deadlock",
	},
	PostgresqlConflictsLock: metricInfo{
		Name: "postgresql.conflicts.lock",
	},
	PostgresqlConflictsSnapshot: metricInfo{
		Name: "postgresql.conflicts.snapshot",
	},
	PostgresqlConflictsTablespace: metricInfo{
		Name: "postgresql.conflicts.tablespace",
	},
	PostgresqlConnections: metricInfo{
		Name: "postgresql.connections",
	},
	PostgresqlDatabaseSize: metricInfo{
		Name: "postgresql.database_size",
	},
	PostgresqlDeadlocks: metricInfo{
		Name: "postgresql.deadlocks",
	},
	PostgresqlDiskRead: metricInfo{
		Name: "postgresql.disk_read",
	},
	PostgresqlReplicationBackendXminAge: metricInfo{
		Name: "postgresql.replication.backend_xmin_age",
	},
	PostgresqlReplicationFlushLsnDelay: metricInfo{
		Name: "postgresql.replication.flush_lsn_delay",
	},
	PostgresqlReplicationReplayLsnDelay: metricInfo{
		Name: "postgresql.replication.replay_lsn_delay",
	},
	PostgresqlReplicationSentLsnDelay: metricInfo{
		Name: "postgresql.replication.sent_lsn_delay",
	},
	PostgresqlReplicationWalFlushLag: metricInfo{
		Name: "postgresql.replication.wal_flush_lag",
	},
	PostgresqlReplicationWalReplayLag: metricInfo{
		Name: "postgresql.replication.wal_replay_lag",
	},
	PostgresqlReplicationWalWriteLag: metricInfo{
		Name: "postgresql.replication.wal_write_lag",
	},
	PostgresqlReplicationWriteLsnDelay: metricInfo{
		Name: "postgresql.replication.write_lsn_delay",
	},
	PostgresqlReplicationSlotCatalogXminAge: metricInfo{
		Name: "postgresql.replication_slot.catalog_xmin_age",
	},
	PostgresqlReplicationSlotConfirmedFlushDelayBytes: metricInfo{
		Name: "postgresql.replication_slot.confirmed_flush_delay_bytes",
	},
	PostgresqlReplicationSlotRestartDelayBytes: metricInfo{
		Name: "postgresql.replication_slot.restart_delay_bytes",
	},
	PostgresqlReplicationSlotSpillBytes: metricInfo{
		Name: "postgresql.replication_slot.spill_bytes",
	},
	PostgresqlReplicationSlotSpillCount: metricInfo{
		Name: "postgresql.replication_slot.spill_count",
	},
	PostgresqlReplicationSlotSpillTxns: metricInfo{
		Name: "postgresql.replication_slot.spill_txns",
	},
	PostgresqlReplicationSlotStreamBytes: metricInfo{
		Name: "postgresql.replication_slot.stream_bytes",
	},
	PostgresqlReplicationSlotStreamCount: metricInfo{
		Name: "postgresql.replication_slot.stream_count",
	},
	PostgresqlReplicationSlotStreamTxns: metricInfo{
		Name: "postgresql.replication_slot.stream_txns",
	},
	PostgresqlReplicationSlotTotalBytes: metricInfo{
		Name: "postgresql.replication_slot.total_bytes",
	},
	PostgresqlReplicationSlotTotalTxns: metricInfo{
		Name: "postgresql.replication_slot.total_txns",
	},
	PostgresqlReplicationSlotXminAge: metricInfo{
		Name: "postgresql.replication_slot.xmin_age",
	},
	PostgresqlRollbacks: metricInfo{
		Name: "postgresql.rollbacks",
	},
	PostgresqlRowsDeleted: metricInfo{
		Name: "postgresql.rows_deleted",
	},
	PostgresqlRowsFetched: metricInfo{
		Name: "postgresql.rows_fetched",
	},
	PostgresqlRowsInserted: metricInfo{
		Name: "postgresql.rows_inserted",
	},
	PostgresqlRowsReturned: metricInfo{
		Name: "postgresql.rows_returned",
	},
	PostgresqlRowsUpdated: metricInfo{
		Name: "postgresql.rows_updated",
	},
	PostgresqlSessionsAbandoned: metricInfo{
		Name: "postgresql.sessions.abandoned",
	},
	PostgresqlSessionsActiveTime: metricInfo{
		Name: "postgresql.sessions.active_time",
	},
	PostgresqlSessionsCount: metricInfo{
		Name: "postgresql.sessions.count",
	},
	PostgresqlSessionsFatal: metricInfo{
		Name: "postgresql.sessions.fatal",
	},
	PostgresqlSessionsIdleInTransactionTime: metricInfo{
		Name: "postgresql.sessions.idle_in_transaction_time",
	},
	PostgresqlSessionsKilled: metricInfo{
		Name: "postgresql.sessions.killed",
	},
	PostgresqlSessionsSessionTime: metricInfo{
		Name: "postgresql.sessions.session_time",
	},
	PostgresqlTempBytes: metricInfo{
		Name: "postgresql.temp_bytes",
	},
	PostgresqlTempFiles: metricInfo{
		Name: "postgresql.temp_files",
	},
}

type metricsInfo struct {
	PostgresqlBeforeXidWraparound                     metricInfo
	PostgresqlBlkReadTime                             metricInfo
	PostgresqlBlkWriteTime                            metricInfo
	PostgresqlBufferHit                               metricInfo
	PostgresqlChecksumsEnabled                        metricInfo
	PostgresqlChecksumsFailures                       metricInfo
	PostgresqlCommits                                 metricInfo
	PostgresqlConflicts                               metricInfo
	PostgresqlConflictsBufferpin                      metricInfo
	PostgresqlConflictsDeadlock                       metricInfo
	PostgresqlConflictsLock                           metricInfo
	PostgresqlConflictsSnapshot                       metricInfo
	PostgresqlConflictsTablespace                     metricInfo
	PostgresqlConnections                             metricInfo
	PostgresqlDatabaseSize                            metricInfo
	PostgresqlDeadlocks                               metricInfo
	PostgresqlDiskRead                                metricInfo
	PostgresqlReplicationBackendXminAge               metricInfo
	PostgresqlReplicationFlushLsnDelay                metricInfo
	PostgresqlReplicationReplayLsnDelay               metricInfo
	PostgresqlReplicationSentLsnDelay                 metricInfo
	PostgresqlReplicationWalFlushLag                  metricInfo
	PostgresqlReplicationWalReplayLag                 metricInfo
	PostgresqlReplicationWalWriteLag                  metricInfo
	PostgresqlReplicationWriteLsnDelay                metricInfo
	PostgresqlReplicationSlotCatalogXminAge           metricInfo
	PostgresqlReplicationSlotConfirmedFlushDelayBytes metricInfo
	PostgresqlReplicationSlotRestartDelayBytes        metricInfo
	PostgresqlReplicationSlotSpillBytes               metricInfo
	PostgresqlReplicationSlotSpillCount               metricInfo
	PostgresqlReplicationSlotSpillTxns                metricInfo
	PostgresqlReplicationSlotStreamBytes              metricInfo
	PostgresqlReplicationSlotStreamCount              metricInfo
	PostgresqlReplicationSlotStreamTxns               metricInfo
	PostgresqlReplicationSlotTotalBytes               metricInfo
	PostgresqlReplicationSlotTotalTxns                metricInfo
	PostgresqlReplicationSlotXminAge                  metricInfo
	PostgresqlRollbacks                               metricInfo
	PostgresqlRowsDeleted                             metricInfo
	PostgresqlRowsFetched                             metricInfo
	PostgresqlRowsInserted                            metricInfo
	PostgresqlRowsReturned                            metricInfo
	PostgresqlRowsUpdated                             metricInfo
	PostgresqlSessionsAbandoned                       metricInfo
	PostgresqlSessionsActiveTime                      metricInfo
	PostgresqlSessionsCount                           metricInfo
	PostgresqlSessionsFatal                           metricInfo
	PostgresqlSessionsIdleInTransactionTime           metricInfo
	PostgresqlSessionsKilled                          metricInfo
	PostgresqlSessionsSessionTime                     metricInfo
	PostgresqlTempBytes                               metricInfo
	PostgresqlTempFiles                               metricInfo
}

type metricInfo struct {
	Name string
}

type metricPostgresqlBeforeXidWraparound struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.before_xid_wraparound metric with initial data.
func (m *metricPostgresqlBeforeXidWraparound) init() {
	m.data.SetName("postgresql.before_xid_wraparound")
	m.data.SetDescription("Number of transactions before XID wraparound occurs")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBeforeXidWraparound) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBeforeXidWraparound) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBeforeXidWraparound) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBeforeXidWraparound(cfg MetricConfig) metricPostgresqlBeforeXidWraparound {
	m := metricPostgresqlBeforeXidWraparound{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlkReadTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blk_read_time metric with initial data.
func (m *metricPostgresqlBlkReadTime) init() {
	m.data.SetName("postgresql.blk_read_time")
	m.data.SetDescription("Time spent reading data file blocks (milliseconds)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlkReadTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlkReadTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlkReadTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlkReadTime(cfg MetricConfig) metricPostgresqlBlkReadTime {
	m := metricPostgresqlBlkReadTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlkWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blk_write_time metric with initial data.
func (m *metricPostgresqlBlkWriteTime) init() {
	m.data.SetName("postgresql.blk_write_time")
	m.data.SetDescription("Time spent writing data file blocks (milliseconds)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlkWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlkWriteTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlkWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlkWriteTime(cfg MetricConfig) metricPostgresqlBlkWriteTime {
	m := metricPostgresqlBlkWriteTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBufferHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.buffer_hit metric with initial data.
func (m *metricPostgresqlBufferHit) init() {
	m.data.SetName("postgresql.buffer_hit")
	m.data.SetDescription("Number of times disk blocks were found in the buffer cache")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBufferHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBufferHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBufferHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBufferHit(cfg MetricConfig) metricPostgresqlBufferHit {
	m := metricPostgresqlBufferHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlChecksumsEnabled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.checksums.enabled metric with initial data.
func (m *metricPostgresqlChecksumsEnabled) init() {
	m.data.SetName("postgresql.checksums.enabled")
	m.data.SetDescription("Whether data checksums are enabled for this PostgreSQL cluster (PostgreSQL 12+)")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlChecksumsEnabled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlChecksumsEnabled) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlChecksumsEnabled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlChecksumsEnabled(cfg MetricConfig) metricPostgresqlChecksumsEnabled {
	m := metricPostgresqlChecksumsEnabled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlChecksumsFailures struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.checksums.failures metric with initial data.
func (m *metricPostgresqlChecksumsFailures) init() {
	m.data.SetName("postgresql.checksums.failures")
	m.data.SetDescription("Number of data page checksum failures detected (PostgreSQL 12+)")
	m.data.SetUnit("{failures}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlChecksumsFailures) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlChecksumsFailures) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlChecksumsFailures) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlChecksumsFailures(cfg MetricConfig) metricPostgresqlChecksumsFailures {
	m := metricPostgresqlChecksumsFailures{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCommits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.commits metric with initial data.
func (m *metricPostgresqlCommits) init() {
	m.data.SetName("postgresql.commits")
	m.data.SetDescription("Number of transactions that have been committed")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCommits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCommits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCommits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCommits(cfg MetricConfig) metricPostgresqlCommits {
	m := metricPostgresqlCommits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflicts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts metric with initial data.
func (m *metricPostgresqlConflicts) init() {
	m.data.SetName("postgresql.conflicts")
	m.data.SetDescription("Number of queries canceled due to conflicts with recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflicts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflicts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflicts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflicts(cfg MetricConfig) metricPostgresqlConflicts {
	m := metricPostgresqlConflicts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsBufferpin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.bufferpin metric with initial data.
func (m *metricPostgresqlConflictsBufferpin) init() {
	m.data.SetName("postgresql.conflicts.bufferpin")
	m.data.SetDescription("Queries canceled due to pinned buffers during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsBufferpin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsBufferpin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsBufferpin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsBufferpin(cfg MetricConfig) metricPostgresqlConflictsBufferpin {
	m := metricPostgresqlConflictsBufferpin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsDeadlock struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.deadlock metric with initial data.
func (m *metricPostgresqlConflictsDeadlock) init() {
	m.data.SetName("postgresql.conflicts.deadlock")
	m.data.SetDescription("Queries canceled due to deadlocks during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsDeadlock) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsDeadlock) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsDeadlock) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsDeadlock(cfg MetricConfig) metricPostgresqlConflictsDeadlock {
	m := metricPostgresqlConflictsDeadlock{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsLock struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.lock metric with initial data.
func (m *metricPostgresqlConflictsLock) init() {
	m.data.SetName("postgresql.conflicts.lock")
	m.data.SetDescription("Queries canceled due to lock timeouts during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsLock) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsLock) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsLock) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsLock(cfg MetricConfig) metricPostgresqlConflictsLock {
	m := metricPostgresqlConflictsLock{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsSnapshot struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.snapshot metric with initial data.
func (m *metricPostgresqlConflictsSnapshot) init() {
	m.data.SetName("postgresql.conflicts.snapshot")
	m.data.SetDescription("Queries canceled due to old snapshots during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsSnapshot) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsSnapshot) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsSnapshot) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsSnapshot(cfg MetricConfig) metricPostgresqlConflictsSnapshot {
	m := metricPostgresqlConflictsSnapshot{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConflictsTablespace struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.conflicts.tablespace metric with initial data.
func (m *metricPostgresqlConflictsTablespace) init() {
	m.data.SetName("postgresql.conflicts.tablespace")
	m.data.SetDescription("Queries canceled due to dropped tablespaces during recovery")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConflictsTablespace) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConflictsTablespace) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConflictsTablespace) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConflictsTablespace(cfg MetricConfig) metricPostgresqlConflictsTablespace {
	m := metricPostgresqlConflictsTablespace{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connections metric with initial data.
func (m *metricPostgresqlConnections) init() {
	m.data.SetName("postgresql.connections")
	m.data.SetDescription("Number of active connections (backends) to the database")
	m.data.SetUnit("{connections}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnections(cfg MetricConfig) metricPostgresqlConnections {
	m := metricPostgresqlConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database_size metric with initial data.
func (m *metricPostgresqlDatabaseSize) init() {
	m.data.SetName("postgresql.database_size")
	m.data.SetDescription("Size of the database in bytes")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDatabaseSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseSize(cfg MetricConfig) metricPostgresqlDatabaseSize {
	m := metricPostgresqlDatabaseSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDeadlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.deadlocks metric with initial data.
func (m *metricPostgresqlDeadlocks) init() {
	m.data.SetName("postgresql.deadlocks")
	m.data.SetDescription("Number of deadlocks detected")
	m.data.SetUnit("{deadlocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDeadlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDeadlocks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDeadlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDeadlocks(cfg MetricConfig) metricPostgresqlDeadlocks {
	m := metricPostgresqlDeadlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDiskRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.disk_read metric with initial data.
func (m *metricPostgresqlDiskRead) init() {
	m.data.SetName("postgresql.disk_read")
	m.data.SetDescription("Number of disk blocks read")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDiskRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDiskRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDiskRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDiskRead(cfg MetricConfig) metricPostgresqlDiskRead {
	m := metricPostgresqlDiskRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationBackendXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.backend_xmin_age metric with initial data.
func (m *metricPostgresqlReplicationBackendXminAge) init() {
	m.data.SetName("postgresql.replication.backend_xmin_age")
	m.data.SetDescription("Age of the oldest transaction on the standby server that is holding back vacuum")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationBackendXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationBackendXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationBackendXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationBackendXminAge(cfg MetricConfig) metricPostgresqlReplicationBackendXminAge {
	m := metricPostgresqlReplicationBackendXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationFlushLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.flush_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationFlushLsnDelay) init() {
	m.data.SetName("postgresql.replication.flush_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL flushed but not yet applied on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationFlushLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationFlushLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationFlushLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationFlushLsnDelay(cfg MetricConfig) metricPostgresqlReplicationFlushLsnDelay {
	m := metricPostgresqlReplicationFlushLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationReplayLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.replay_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationReplayLsnDelay) init() {
	m.data.SetName("postgresql.replication.replay_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL not yet replayed on standby (total replication lag in bytes)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationReplayLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationReplayLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationReplayLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationReplayLsnDelay(cfg MetricConfig) metricPostgresqlReplicationReplayLsnDelay {
	m := metricPostgresqlReplicationReplayLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSentLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.sent_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationSentLsnDelay) init() {
	m.data.SetName("postgresql.replication.sent_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL sent but not yet written to disk on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSentLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSentLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSentLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSentLsnDelay(cfg MetricConfig) metricPostgresqlReplicationSentLsnDelay {
	m := metricPostgresqlReplicationSentLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalFlushLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_flush_lag metric with initial data.
func (m *metricPostgresqlReplicationWalFlushLag) init() {
	m.data.SetName("postgresql.replication.wal_flush_lag")
	m.data.SetDescription("Time elapsed between WAL flush on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalFlushLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalFlushLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalFlushLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalFlushLag(cfg MetricConfig) metricPostgresqlReplicationWalFlushLag {
	m := metricPostgresqlReplicationWalFlushLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalReplayLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_replay_lag metric with initial data.
func (m *metricPostgresqlReplicationWalReplayLag) init() {
	m.data.SetName("postgresql.replication.wal_replay_lag")
	m.data.SetDescription("Time elapsed between WAL replay on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalReplayLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalReplayLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalReplayLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalReplayLag(cfg MetricConfig) metricPostgresqlReplicationWalReplayLag {
	m := metricPostgresqlReplicationWalReplayLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWalWriteLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.wal_write_lag metric with initial data.
func (m *metricPostgresqlReplicationWalWriteLag) init() {
	m.data.SetName("postgresql.replication.wal_write_lag")
	m.data.SetDescription("Time elapsed between WAL write on primary and confirmation from standby (PostgreSQL 10+)")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWalWriteLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWalWriteLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWalWriteLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWalWriteLag(cfg MetricConfig) metricPostgresqlReplicationWalWriteLag {
	m := metricPostgresqlReplicationWalWriteLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationWriteLsnDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.write_lsn_delay metric with initial data.
func (m *metricPostgresqlReplicationWriteLsnDelay) init() {
	m.data.SetName("postgresql.replication.write_lsn_delay")
	m.data.SetDescription("Number of bytes of WAL written but not yet flushed on standby")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationWriteLsnDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("client_address", clientAddressAttributeValue)
	dp.Attributes().PutStr("replication_state", replicationStateAttributeValue)
	dp.Attributes().PutStr("sync_state", syncStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationWriteLsnDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationWriteLsnDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationWriteLsnDelay(cfg MetricConfig) metricPostgresqlReplicationWriteLsnDelay {
	m := metricPostgresqlReplicationWriteLsnDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotCatalogXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.catalog_xmin_age metric with initial data.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) init() {
	m.data.SetName("postgresql.replication_slot.catalog_xmin_age")
	m.data.SetDescription("Age of oldest transaction affecting system catalogs that this slot needs to keep (PostgreSQL 9.4+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotCatalogXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotCatalogXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotCatalogXminAge(cfg MetricConfig) metricPostgresqlReplicationSlotCatalogXminAge {
	m := metricPostgresqlReplicationSlotCatalogXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotConfirmedFlushDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.confirmed_flush_delay_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) init() {
	m.data.SetName("postgresql.replication_slot.confirmed_flush_delay_bytes")
	m.data.SetDescription("Number of bytes between current WAL position and confirmed_flush_lsn (logical slots only, PostgreSQL 9.4+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotConfirmedFlushDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotConfirmedFlushDelayBytes(cfg MetricConfig) metricPostgresqlReplicationSlotConfirmedFlushDelayBytes {
	m := metricPostgresqlReplicationSlotConfirmedFlushDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotRestartDelayBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.restart_delay_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) init() {
	m.data.SetName("postgresql.replication_slot.restart_delay_bytes")
	m.data.SetDescription("Number of bytes of WAL between current position and slot's restart_lsn (PostgreSQL 9.4+)")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotRestartDelayBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotRestartDelayBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotRestartDelayBytes(cfg MetricConfig) metricPostgresqlReplicationSlotRestartDelayBytes {
	m := metricPostgresqlReplicationSlotRestartDelayBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillBytes) init() {
	m.data.SetName("postgresql.replication_slot.spill_bytes")
	m.data.SetDescription("Total amount of data spilled to disk for logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillBytes(cfg MetricConfig) metricPostgresqlReplicationSlotSpillBytes {
	m := metricPostgresqlReplicationSlotSpillBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_count metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillCount) init() {
	m.data.SetName("postgresql.replication_slot.spill_count")
	m.data.SetDescription("Number of times transactions were spilled to disk during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{spills}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillCount(cfg MetricConfig) metricPostgresqlReplicationSlotSpillCount {
	m := metricPostgresqlReplicationSlotSpillCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotSpillTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.spill_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotSpillTxns) init() {
	m.data.SetName("postgresql.replication_slot.spill_txns")
	m.data.SetDescription("Number of transactions spilled to disk during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotSpillTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotSpillTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotSpillTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotSpillTxns(cfg MetricConfig) metricPostgresqlReplicationSlotSpillTxns {
	m := metricPostgresqlReplicationSlotSpillTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamBytes) init() {
	m.data.SetName("postgresql.replication_slot.stream_bytes")
	m.data.SetDescription("Total amount of data streamed for in-progress transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamBytes(cfg MetricConfig) metricPostgresqlReplicationSlotStreamBytes {
	m := metricPostgresqlReplicationSlotStreamBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_count metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamCount) init() {
	m.data.SetName("postgresql.replication_slot.stream_count")
	m.data.SetDescription("Number of times in-progress transactions were streamed during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{streams}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamCount(cfg MetricConfig) metricPostgresqlReplicationSlotStreamCount {
	m := metricPostgresqlReplicationSlotStreamCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotStreamTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.stream_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotStreamTxns) init() {
	m.data.SetName("postgresql.replication_slot.stream_txns")
	m.data.SetDescription("Number of in-progress transactions streamed during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotStreamTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotStreamTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotStreamTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotStreamTxns(cfg MetricConfig) metricPostgresqlReplicationSlotStreamTxns {
	m := metricPostgresqlReplicationSlotStreamTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotTotalBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.total_bytes metric with initial data.
func (m *metricPostgresqlReplicationSlotTotalBytes) init() {
	m.data.SetName("postgresql.replication_slot.total_bytes")
	m.data.SetDescription("Total amount of data decoded for transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotTotalBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotTotalBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotTotalBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotTotalBytes(cfg MetricConfig) metricPostgresqlReplicationSlotTotalBytes {
	m := metricPostgresqlReplicationSlotTotalBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotTotalTxns struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.total_txns metric with initial data.
func (m *metricPostgresqlReplicationSlotTotalTxns) init() {
	m.data.SetName("postgresql.replication_slot.total_txns")
	m.data.SetDescription("Total number of decoded transactions during logical decoding (PostgreSQL 14+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotTotalTxns) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotTotalTxns) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotTotalTxns) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotTotalTxns(cfg MetricConfig) metricPostgresqlReplicationSlotTotalTxns {
	m := metricPostgresqlReplicationSlotTotalTxns{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationSlotXminAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication_slot.xmin_age metric with initial data.
func (m *metricPostgresqlReplicationSlotXminAge) init() {
	m.data.SetName("postgresql.replication_slot.xmin_age")
	m.data.SetDescription("Age of oldest transaction that this replication slot needs to keep (PostgreSQL 9.4+)")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationSlotXminAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("slot_name", slotNameAttributeValue)
	dp.Attributes().PutStr("slot_type", slotTypeAttributeValue)
	dp.Attributes().PutStr("plugin", pluginAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationSlotXminAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationSlotXminAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationSlotXminAge(cfg MetricConfig) metricPostgresqlReplicationSlotXminAge {
	m := metricPostgresqlReplicationSlotXminAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRollbacks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rollbacks metric with initial data.
func (m *metricPostgresqlRollbacks) init() {
	m.data.SetName("postgresql.rollbacks")
	m.data.SetDescription("Number of transactions that have been rolled back")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRollbacks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRollbacks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRollbacks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRollbacks(cfg MetricConfig) metricPostgresqlRollbacks {
	m := metricPostgresqlRollbacks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsDeleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_deleted metric with initial data.
func (m *metricPostgresqlRowsDeleted) init() {
	m.data.SetName("postgresql.rows_deleted")
	m.data.SetDescription("Number of rows deleted")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsDeleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsDeleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsDeleted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsDeleted(cfg MetricConfig) metricPostgresqlRowsDeleted {
	m := metricPostgresqlRowsDeleted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_fetched metric with initial data.
func (m *metricPostgresqlRowsFetched) init() {
	m.data.SetName("postgresql.rows_fetched")
	m.data.SetDescription("Number of rows fetched by queries")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsFetched) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsFetched(cfg MetricConfig) metricPostgresqlRowsFetched {
	m := metricPostgresqlRowsFetched{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsInserted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_inserted metric with initial data.
func (m *metricPostgresqlRowsInserted) init() {
	m.data.SetName("postgresql.rows_inserted")
	m.data.SetDescription("Number of rows inserted")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsInserted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsInserted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsInserted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsInserted(cfg MetricConfig) metricPostgresqlRowsInserted {
	m := metricPostgresqlRowsInserted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsReturned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_returned metric with initial data.
func (m *metricPostgresqlRowsReturned) init() {
	m.data.SetName("postgresql.rows_returned")
	m.data.SetDescription("Number of rows returned by queries")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsReturned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsReturned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsReturned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsReturned(cfg MetricConfig) metricPostgresqlRowsReturned {
	m := metricPostgresqlRowsReturned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRowsUpdated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows_updated metric with initial data.
func (m *metricPostgresqlRowsUpdated) init() {
	m.data.SetName("postgresql.rows_updated")
	m.data.SetDescription("Number of rows updated")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRowsUpdated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRowsUpdated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRowsUpdated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRowsUpdated(cfg MetricConfig) metricPostgresqlRowsUpdated {
	m := metricPostgresqlRowsUpdated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsAbandoned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.abandoned metric with initial data.
func (m *metricPostgresqlSessionsAbandoned) init() {
	m.data.SetName("postgresql.sessions.abandoned")
	m.data.SetDescription("Number of sessions abandoned due to client disconnection (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsAbandoned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsAbandoned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsAbandoned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsAbandoned(cfg MetricConfig) metricPostgresqlSessionsAbandoned {
	m := metricPostgresqlSessionsAbandoned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsActiveTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.active_time metric with initial data.
func (m *metricPostgresqlSessionsActiveTime) init() {
	m.data.SetName("postgresql.sessions.active_time")
	m.data.SetDescription("Time spent executing queries in this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsActiveTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsActiveTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsActiveTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsActiveTime(cfg MetricConfig) metricPostgresqlSessionsActiveTime {
	m := metricPostgresqlSessionsActiveTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.count metric with initial data.
func (m *metricPostgresqlSessionsCount) init() {
	m.data.SetName("postgresql.sessions.count")
	m.data.SetDescription("Number of sessions established to this database (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsCount(cfg MetricConfig) metricPostgresqlSessionsCount {
	m := metricPostgresqlSessionsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsFatal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.fatal metric with initial data.
func (m *metricPostgresqlSessionsFatal) init() {
	m.data.SetName("postgresql.sessions.fatal")
	m.data.SetDescription("Number of sessions terminated by fatal errors (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsFatal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsFatal) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsFatal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsFatal(cfg MetricConfig) metricPostgresqlSessionsFatal {
	m := metricPostgresqlSessionsFatal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsIdleInTransactionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.idle_in_transaction_time metric with initial data.
func (m *metricPostgresqlSessionsIdleInTransactionTime) init() {
	m.data.SetName("postgresql.sessions.idle_in_transaction_time")
	m.data.SetDescription("Time spent idle in transactions in this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsIdleInTransactionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsIdleInTransactionTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsIdleInTransactionTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsIdleInTransactionTime(cfg MetricConfig) metricPostgresqlSessionsIdleInTransactionTime {
	m := metricPostgresqlSessionsIdleInTransactionTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsKilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.killed metric with initial data.
func (m *metricPostgresqlSessionsKilled) init() {
	m.data.SetName("postgresql.sessions.killed")
	m.data.SetDescription("Number of sessions terminated by operator intervention (PostgreSQL 14+)")
	m.data.SetUnit("{sessions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsKilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsKilled) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsKilled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsKilled(cfg MetricConfig) metricPostgresqlSessionsKilled {
	m := metricPostgresqlSessionsKilled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSessionsSessionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sessions.session_time metric with initial data.
func (m *metricPostgresqlSessionsSessionTime) init() {
	m.data.SetName("postgresql.sessions.session_time")
	m.data.SetDescription("Time spent in sessions for this database (PostgreSQL 14+)")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSessionsSessionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSessionsSessionTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSessionsSessionTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSessionsSessionTime(cfg MetricConfig) metricPostgresqlSessionsSessionTime {
	m := metricPostgresqlSessionsSessionTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTempBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.temp_bytes metric with initial data.
func (m *metricPostgresqlTempBytes) init() {
	m.data.SetName("postgresql.temp_bytes")
	m.data.SetDescription("Total amount of data written to temporary files")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTempBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTempBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTempBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTempBytes(cfg MetricConfig) metricPostgresqlTempBytes {
	m := metricPostgresqlTempBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTempFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.temp_files metric with initial data.
func (m *metricPostgresqlTempFiles) init() {
	m.data.SetName("postgresql.temp_files")
	m.data.SetDescription("Number of temporary files created")
	m.data.SetUnit("{files}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTempFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("newrelicpostgresql.instance_name", newrelicpostgresqlInstanceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTempFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTempFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTempFiles(cfg MetricConfig) metricPostgresqlTempFiles {
	m := metricPostgresqlTempFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                                  MetricsBuilderConfig // config of the metrics builder.
	startTime                                               pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                         int                  // maximum observed number of metrics per resource.
	metricsBuffer                                           pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                               component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter                          map[string]filter.Filter
	resourceAttributeExcludeFilter                          map[string]filter.Filter
	metricPostgresqlBeforeXidWraparound                     metricPostgresqlBeforeXidWraparound
	metricPostgresqlBlkReadTime                             metricPostgresqlBlkReadTime
	metricPostgresqlBlkWriteTime                            metricPostgresqlBlkWriteTime
	metricPostgresqlBufferHit                               metricPostgresqlBufferHit
	metricPostgresqlChecksumsEnabled                        metricPostgresqlChecksumsEnabled
	metricPostgresqlChecksumsFailures                       metricPostgresqlChecksumsFailures
	metricPostgresqlCommits                                 metricPostgresqlCommits
	metricPostgresqlConflicts                               metricPostgresqlConflicts
	metricPostgresqlConflictsBufferpin                      metricPostgresqlConflictsBufferpin
	metricPostgresqlConflictsDeadlock                       metricPostgresqlConflictsDeadlock
	metricPostgresqlConflictsLock                           metricPostgresqlConflictsLock
	metricPostgresqlConflictsSnapshot                       metricPostgresqlConflictsSnapshot
	metricPostgresqlConflictsTablespace                     metricPostgresqlConflictsTablespace
	metricPostgresqlConnections                             metricPostgresqlConnections
	metricPostgresqlDatabaseSize                            metricPostgresqlDatabaseSize
	metricPostgresqlDeadlocks                               metricPostgresqlDeadlocks
	metricPostgresqlDiskRead                                metricPostgresqlDiskRead
	metricPostgresqlReplicationBackendXminAge               metricPostgresqlReplicationBackendXminAge
	metricPostgresqlReplicationFlushLsnDelay                metricPostgresqlReplicationFlushLsnDelay
	metricPostgresqlReplicationReplayLsnDelay               metricPostgresqlReplicationReplayLsnDelay
	metricPostgresqlReplicationSentLsnDelay                 metricPostgresqlReplicationSentLsnDelay
	metricPostgresqlReplicationWalFlushLag                  metricPostgresqlReplicationWalFlushLag
	metricPostgresqlReplicationWalReplayLag                 metricPostgresqlReplicationWalReplayLag
	metricPostgresqlReplicationWalWriteLag                  metricPostgresqlReplicationWalWriteLag
	metricPostgresqlReplicationWriteLsnDelay                metricPostgresqlReplicationWriteLsnDelay
	metricPostgresqlReplicationSlotCatalogXminAge           metricPostgresqlReplicationSlotCatalogXminAge
	metricPostgresqlReplicationSlotConfirmedFlushDelayBytes metricPostgresqlReplicationSlotConfirmedFlushDelayBytes
	metricPostgresqlReplicationSlotRestartDelayBytes        metricPostgresqlReplicationSlotRestartDelayBytes
	metricPostgresqlReplicationSlotSpillBytes               metricPostgresqlReplicationSlotSpillBytes
	metricPostgresqlReplicationSlotSpillCount               metricPostgresqlReplicationSlotSpillCount
	metricPostgresqlReplicationSlotSpillTxns                metricPostgresqlReplicationSlotSpillTxns
	metricPostgresqlReplicationSlotStreamBytes              metricPostgresqlReplicationSlotStreamBytes
	metricPostgresqlReplicationSlotStreamCount              metricPostgresqlReplicationSlotStreamCount
	metricPostgresqlReplicationSlotStreamTxns               metricPostgresqlReplicationSlotStreamTxns
	metricPostgresqlReplicationSlotTotalBytes               metricPostgresqlReplicationSlotTotalBytes
	metricPostgresqlReplicationSlotTotalTxns                metricPostgresqlReplicationSlotTotalTxns
	metricPostgresqlReplicationSlotXminAge                  metricPostgresqlReplicationSlotXminAge
	metricPostgresqlRollbacks                               metricPostgresqlRollbacks
	metricPostgresqlRowsDeleted                             metricPostgresqlRowsDeleted
	metricPostgresqlRowsFetched                             metricPostgresqlRowsFetched
	metricPostgresqlRowsInserted                            metricPostgresqlRowsInserted
	metricPostgresqlRowsReturned                            metricPostgresqlRowsReturned
	metricPostgresqlRowsUpdated                             metricPostgresqlRowsUpdated
	metricPostgresqlSessionsAbandoned                       metricPostgresqlSessionsAbandoned
	metricPostgresqlSessionsActiveTime                      metricPostgresqlSessionsActiveTime
	metricPostgresqlSessionsCount                           metricPostgresqlSessionsCount
	metricPostgresqlSessionsFatal                           metricPostgresqlSessionsFatal
	metricPostgresqlSessionsIdleInTransactionTime           metricPostgresqlSessionsIdleInTransactionTime
	metricPostgresqlSessionsKilled                          metricPostgresqlSessionsKilled
	metricPostgresqlSessionsSessionTime                     metricPostgresqlSessionsSessionTime
	metricPostgresqlTempBytes                               metricPostgresqlTempBytes
	metricPostgresqlTempFiles                               metricPostgresqlTempFiles
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                                  mbc,
		startTime:                                               pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                           pmetric.NewMetrics(),
		buildInfo:                                               settings.BuildInfo,
		metricPostgresqlBeforeXidWraparound:                     newMetricPostgresqlBeforeXidWraparound(mbc.Metrics.PostgresqlBeforeXidWraparound),
		metricPostgresqlBlkReadTime:                             newMetricPostgresqlBlkReadTime(mbc.Metrics.PostgresqlBlkReadTime),
		metricPostgresqlBlkWriteTime:                            newMetricPostgresqlBlkWriteTime(mbc.Metrics.PostgresqlBlkWriteTime),
		metricPostgresqlBufferHit:                               newMetricPostgresqlBufferHit(mbc.Metrics.PostgresqlBufferHit),
		metricPostgresqlChecksumsEnabled:                        newMetricPostgresqlChecksumsEnabled(mbc.Metrics.PostgresqlChecksumsEnabled),
		metricPostgresqlChecksumsFailures:                       newMetricPostgresqlChecksumsFailures(mbc.Metrics.PostgresqlChecksumsFailures),
		metricPostgresqlCommits:                                 newMetricPostgresqlCommits(mbc.Metrics.PostgresqlCommits),
		metricPostgresqlConflicts:                               newMetricPostgresqlConflicts(mbc.Metrics.PostgresqlConflicts),
		metricPostgresqlConflictsBufferpin:                      newMetricPostgresqlConflictsBufferpin(mbc.Metrics.PostgresqlConflictsBufferpin),
		metricPostgresqlConflictsDeadlock:                       newMetricPostgresqlConflictsDeadlock(mbc.Metrics.PostgresqlConflictsDeadlock),
		metricPostgresqlConflictsLock:                           newMetricPostgresqlConflictsLock(mbc.Metrics.PostgresqlConflictsLock),
		metricPostgresqlConflictsSnapshot:                       newMetricPostgresqlConflictsSnapshot(mbc.Metrics.PostgresqlConflictsSnapshot),
		metricPostgresqlConflictsTablespace:                     newMetricPostgresqlConflictsTablespace(mbc.Metrics.PostgresqlConflictsTablespace),
		metricPostgresqlConnections:                             newMetricPostgresqlConnections(mbc.Metrics.PostgresqlConnections),
		metricPostgresqlDatabaseSize:                            newMetricPostgresqlDatabaseSize(mbc.Metrics.PostgresqlDatabaseSize),
		metricPostgresqlDeadlocks:                               newMetricPostgresqlDeadlocks(mbc.Metrics.PostgresqlDeadlocks),
		metricPostgresqlDiskRead:                                newMetricPostgresqlDiskRead(mbc.Metrics.PostgresqlDiskRead),
		metricPostgresqlReplicationBackendXminAge:               newMetricPostgresqlReplicationBackendXminAge(mbc.Metrics.PostgresqlReplicationBackendXminAge),
		metricPostgresqlReplicationFlushLsnDelay:                newMetricPostgresqlReplicationFlushLsnDelay(mbc.Metrics.PostgresqlReplicationFlushLsnDelay),
		metricPostgresqlReplicationReplayLsnDelay:               newMetricPostgresqlReplicationReplayLsnDelay(mbc.Metrics.PostgresqlReplicationReplayLsnDelay),
		metricPostgresqlReplicationSentLsnDelay:                 newMetricPostgresqlReplicationSentLsnDelay(mbc.Metrics.PostgresqlReplicationSentLsnDelay),
		metricPostgresqlReplicationWalFlushLag:                  newMetricPostgresqlReplicationWalFlushLag(mbc.Metrics.PostgresqlReplicationWalFlushLag),
		metricPostgresqlReplicationWalReplayLag:                 newMetricPostgresqlReplicationWalReplayLag(mbc.Metrics.PostgresqlReplicationWalReplayLag),
		metricPostgresqlReplicationWalWriteLag:                  newMetricPostgresqlReplicationWalWriteLag(mbc.Metrics.PostgresqlReplicationWalWriteLag),
		metricPostgresqlReplicationWriteLsnDelay:                newMetricPostgresqlReplicationWriteLsnDelay(mbc.Metrics.PostgresqlReplicationWriteLsnDelay),
		metricPostgresqlReplicationSlotCatalogXminAge:           newMetricPostgresqlReplicationSlotCatalogXminAge(mbc.Metrics.PostgresqlReplicationSlotCatalogXminAge),
		metricPostgresqlReplicationSlotConfirmedFlushDelayBytes: newMetricPostgresqlReplicationSlotConfirmedFlushDelayBytes(mbc.Metrics.PostgresqlReplicationSlotConfirmedFlushDelayBytes),
		metricPostgresqlReplicationSlotRestartDelayBytes:        newMetricPostgresqlReplicationSlotRestartDelayBytes(mbc.Metrics.PostgresqlReplicationSlotRestartDelayBytes),
		metricPostgresqlReplicationSlotSpillBytes:               newMetricPostgresqlReplicationSlotSpillBytes(mbc.Metrics.PostgresqlReplicationSlotSpillBytes),
		metricPostgresqlReplicationSlotSpillCount:               newMetricPostgresqlReplicationSlotSpillCount(mbc.Metrics.PostgresqlReplicationSlotSpillCount),
		metricPostgresqlReplicationSlotSpillTxns:                newMetricPostgresqlReplicationSlotSpillTxns(mbc.Metrics.PostgresqlReplicationSlotSpillTxns),
		metricPostgresqlReplicationSlotStreamBytes:              newMetricPostgresqlReplicationSlotStreamBytes(mbc.Metrics.PostgresqlReplicationSlotStreamBytes),
		metricPostgresqlReplicationSlotStreamCount:              newMetricPostgresqlReplicationSlotStreamCount(mbc.Metrics.PostgresqlReplicationSlotStreamCount),
		metricPostgresqlReplicationSlotStreamTxns:               newMetricPostgresqlReplicationSlotStreamTxns(mbc.Metrics.PostgresqlReplicationSlotStreamTxns),
		metricPostgresqlReplicationSlotTotalBytes:               newMetricPostgresqlReplicationSlotTotalBytes(mbc.Metrics.PostgresqlReplicationSlotTotalBytes),
		metricPostgresqlReplicationSlotTotalTxns:                newMetricPostgresqlReplicationSlotTotalTxns(mbc.Metrics.PostgresqlReplicationSlotTotalTxns),
		metricPostgresqlReplicationSlotXminAge:                  newMetricPostgresqlReplicationSlotXminAge(mbc.Metrics.PostgresqlReplicationSlotXminAge),
		metricPostgresqlRollbacks:                               newMetricPostgresqlRollbacks(mbc.Metrics.PostgresqlRollbacks),
		metricPostgresqlRowsDeleted:                             newMetricPostgresqlRowsDeleted(mbc.Metrics.PostgresqlRowsDeleted),
		metricPostgresqlRowsFetched:                             newMetricPostgresqlRowsFetched(mbc.Metrics.PostgresqlRowsFetched),
		metricPostgresqlRowsInserted:                            newMetricPostgresqlRowsInserted(mbc.Metrics.PostgresqlRowsInserted),
		metricPostgresqlRowsReturned:                            newMetricPostgresqlRowsReturned(mbc.Metrics.PostgresqlRowsReturned),
		metricPostgresqlRowsUpdated:                             newMetricPostgresqlRowsUpdated(mbc.Metrics.PostgresqlRowsUpdated),
		metricPostgresqlSessionsAbandoned:                       newMetricPostgresqlSessionsAbandoned(mbc.Metrics.PostgresqlSessionsAbandoned),
		metricPostgresqlSessionsActiveTime:                      newMetricPostgresqlSessionsActiveTime(mbc.Metrics.PostgresqlSessionsActiveTime),
		metricPostgresqlSessionsCount:                           newMetricPostgresqlSessionsCount(mbc.Metrics.PostgresqlSessionsCount),
		metricPostgresqlSessionsFatal:                           newMetricPostgresqlSessionsFatal(mbc.Metrics.PostgresqlSessionsFatal),
		metricPostgresqlSessionsIdleInTransactionTime:           newMetricPostgresqlSessionsIdleInTransactionTime(mbc.Metrics.PostgresqlSessionsIdleInTransactionTime),
		metricPostgresqlSessionsKilled:                          newMetricPostgresqlSessionsKilled(mbc.Metrics.PostgresqlSessionsKilled),
		metricPostgresqlSessionsSessionTime:                     newMetricPostgresqlSessionsSessionTime(mbc.Metrics.PostgresqlSessionsSessionTime),
		metricPostgresqlTempBytes:                               newMetricPostgresqlTempBytes(mbc.Metrics.PostgresqlTempBytes),
		metricPostgresqlTempFiles:                               newMetricPostgresqlTempFiles(mbc.Metrics.PostgresqlTempFiles),
		resourceAttributeIncludeFilter:                          make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:                          make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.DatabaseName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["database_name"] = filter.CreateFilter(mbc.ResourceAttributes.DatabaseName.MetricsInclude)
	}
	if mbc.ResourceAttributes.DatabaseName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["database_name"] = filter.CreateFilter(mbc.ResourceAttributes.DatabaseName.MetricsExclude)
	}
	if mbc.ResourceAttributes.DbSystem.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["db.system"] = filter.CreateFilter(mbc.ResourceAttributes.DbSystem.MetricsInclude)
	}
	if mbc.ResourceAttributes.DbSystem.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["db.system"] = filter.CreateFilter(mbc.ResourceAttributes.DbSystem.MetricsExclude)
	}
	if mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["newrelicpostgresql.instance_name"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsInclude)
	}
	if mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["newrelicpostgresql.instance_name"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicpostgresqlInstanceName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlVersion.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.version"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlVersion.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlVersion.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.version"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlVersion.MetricsExclude)
	}
	if mbc.ResourceAttributes.ServerAddress.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["server.address"] = filter.CreateFilter(mbc.ResourceAttributes.ServerAddress.MetricsInclude)
	}
	if mbc.ResourceAttributes.ServerAddress.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["server.address"] = filter.CreateFilter(mbc.ResourceAttributes.ServerAddress.MetricsExclude)
	}
	if mbc.ResourceAttributes.ServerPort.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["server.port"] = filter.CreateFilter(mbc.ResourceAttributes.ServerPort.MetricsInclude)
	}
	if mbc.ResourceAttributes.ServerPort.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["server.port"] = filter.CreateFilter(mbc.ResourceAttributes.ServerPort.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricPostgresqlBeforeXidWraparound.emit(ils.Metrics())
	mb.metricPostgresqlBlkReadTime.emit(ils.Metrics())
	mb.metricPostgresqlBlkWriteTime.emit(ils.Metrics())
	mb.metricPostgresqlBufferHit.emit(ils.Metrics())
	mb.metricPostgresqlChecksumsEnabled.emit(ils.Metrics())
	mb.metricPostgresqlChecksumsFailures.emit(ils.Metrics())
	mb.metricPostgresqlCommits.emit(ils.Metrics())
	mb.metricPostgresqlConflicts.emit(ils.Metrics())
	mb.metricPostgresqlConflictsBufferpin.emit(ils.Metrics())
	mb.metricPostgresqlConflictsDeadlock.emit(ils.Metrics())
	mb.metricPostgresqlConflictsLock.emit(ils.Metrics())
	mb.metricPostgresqlConflictsSnapshot.emit(ils.Metrics())
	mb.metricPostgresqlConflictsTablespace.emit(ils.Metrics())
	mb.metricPostgresqlConnections.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseSize.emit(ils.Metrics())
	mb.metricPostgresqlDeadlocks.emit(ils.Metrics())
	mb.metricPostgresqlDiskRead.emit(ils.Metrics())
	mb.metricPostgresqlReplicationBackendXminAge.emit(ils.Metrics())
	mb.metricPostgresqlReplicationFlushLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationReplayLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSentLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalFlushLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalReplayLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWalWriteLag.emit(ils.Metrics())
	mb.metricPostgresqlReplicationWriteLsnDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotCatalogXminAge.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotConfirmedFlushDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotRestartDelayBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillCount.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotSpillTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamCount.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotStreamTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotTotalBytes.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotTotalTxns.emit(ils.Metrics())
	mb.metricPostgresqlReplicationSlotXminAge.emit(ils.Metrics())
	mb.metricPostgresqlRollbacks.emit(ils.Metrics())
	mb.metricPostgresqlRowsDeleted.emit(ils.Metrics())
	mb.metricPostgresqlRowsFetched.emit(ils.Metrics())
	mb.metricPostgresqlRowsInserted.emit(ils.Metrics())
	mb.metricPostgresqlRowsReturned.emit(ils.Metrics())
	mb.metricPostgresqlRowsUpdated.emit(ils.Metrics())
	mb.metricPostgresqlSessionsAbandoned.emit(ils.Metrics())
	mb.metricPostgresqlSessionsActiveTime.emit(ils.Metrics())
	mb.metricPostgresqlSessionsCount.emit(ils.Metrics())
	mb.metricPostgresqlSessionsFatal.emit(ils.Metrics())
	mb.metricPostgresqlSessionsIdleInTransactionTime.emit(ils.Metrics())
	mb.metricPostgresqlSessionsKilled.emit(ils.Metrics())
	mb.metricPostgresqlSessionsSessionTime.emit(ils.Metrics())
	mb.metricPostgresqlTempBytes.emit(ils.Metrics())
	mb.metricPostgresqlTempFiles.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordPostgresqlBeforeXidWraparoundDataPoint adds a data point to postgresql.before_xid_wraparound metric.
func (mb *MetricsBuilder) RecordPostgresqlBeforeXidWraparoundDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBeforeXidWraparound.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBlkReadTimeDataPoint adds a data point to postgresql.blk_read_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBlkReadTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBlkReadTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBlkWriteTimeDataPoint adds a data point to postgresql.blk_write_time metric.
func (mb *MetricsBuilder) RecordPostgresqlBlkWriteTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBlkWriteTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlBufferHitDataPoint adds a data point to postgresql.buffer_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlBufferHitDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlBufferHit.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlChecksumsEnabledDataPoint adds a data point to postgresql.checksums.enabled metric.
func (mb *MetricsBuilder) RecordPostgresqlChecksumsEnabledDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlChecksumsEnabled.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlChecksumsFailuresDataPoint adds a data point to postgresql.checksums.failures metric.
func (mb *MetricsBuilder) RecordPostgresqlChecksumsFailuresDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlChecksumsFailures.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlCommitsDataPoint adds a data point to postgresql.commits metric.
func (mb *MetricsBuilder) RecordPostgresqlCommitsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlCommits.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsDataPoint adds a data point to postgresql.conflicts metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflicts.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsBufferpinDataPoint adds a data point to postgresql.conflicts.bufferpin metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsBufferpinDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsBufferpin.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsDeadlockDataPoint adds a data point to postgresql.conflicts.deadlock metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsDeadlockDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsDeadlock.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsLockDataPoint adds a data point to postgresql.conflicts.lock metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsLockDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsLock.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsSnapshotDataPoint adds a data point to postgresql.conflicts.snapshot metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsSnapshotDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsSnapshot.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConflictsTablespaceDataPoint adds a data point to postgresql.conflicts.tablespace metric.
func (mb *MetricsBuilder) RecordPostgresqlConflictsTablespaceDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConflictsTablespace.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlConnectionsDataPoint adds a data point to postgresql.connections metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionsDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlConnections.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDatabaseSizeDataPoint adds a data point to postgresql.database_size metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseSizeDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDatabaseSize.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDeadlocksDataPoint adds a data point to postgresql.deadlocks metric.
func (mb *MetricsBuilder) RecordPostgresqlDeadlocksDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDeadlocks.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlDiskReadDataPoint adds a data point to postgresql.disk_read metric.
func (mb *MetricsBuilder) RecordPostgresqlDiskReadDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlDiskRead.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlReplicationBackendXminAgeDataPoint adds a data point to postgresql.replication.backend_xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationBackendXminAgeDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationBackendXminAge.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationFlushLsnDelayDataPoint adds a data point to postgresql.replication.flush_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationFlushLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationFlushLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationReplayLsnDelayDataPoint adds a data point to postgresql.replication.replay_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationReplayLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationReplayLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationSentLsnDelayDataPoint adds a data point to postgresql.replication.sent_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSentLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationSentLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalFlushLagDataPoint adds a data point to postgresql.replication.wal_flush_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalFlushLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalFlushLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalReplayLagDataPoint adds a data point to postgresql.replication.wal_replay_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalReplayLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalReplayLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWalWriteLagDataPoint adds a data point to postgresql.replication.wal_write_lag metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWalWriteLagDataPoint(ts pcommon.Timestamp, val float64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWalWriteLag.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationWriteLsnDelayDataPoint adds a data point to postgresql.replication.write_lsn_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationWriteLsnDelayDataPoint(ts pcommon.Timestamp, val int64, applicationNameAttributeValue string, clientAddressAttributeValue string, replicationStateAttributeValue string, syncStateAttributeValue string) {
	mb.metricPostgresqlReplicationWriteLsnDelay.recordDataPoint(mb.startTime, ts, val, applicationNameAttributeValue, clientAddressAttributeValue, replicationStateAttributeValue, syncStateAttributeValue)
}

// RecordPostgresqlReplicationSlotCatalogXminAgeDataPoint adds a data point to postgresql.replication_slot.catalog_xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotCatalogXminAgeDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotCatalogXminAge.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotConfirmedFlushDelayBytesDataPoint adds a data point to postgresql.replication_slot.confirmed_flush_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotConfirmedFlushDelayBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotConfirmedFlushDelayBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotRestartDelayBytesDataPoint adds a data point to postgresql.replication_slot.restart_delay_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotRestartDelayBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotRestartDelayBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillBytesDataPoint adds a data point to postgresql.replication_slot.spill_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillCountDataPoint adds a data point to postgresql.replication_slot.spill_count metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillCountDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillCount.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotSpillTxnsDataPoint adds a data point to postgresql.replication_slot.spill_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotSpillTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotSpillTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamBytesDataPoint adds a data point to postgresql.replication_slot.stream_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamCountDataPoint adds a data point to postgresql.replication_slot.stream_count metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamCountDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamCount.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotStreamTxnsDataPoint adds a data point to postgresql.replication_slot.stream_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotStreamTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotStreamTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotTotalBytesDataPoint adds a data point to postgresql.replication_slot.total_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotTotalBytesDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotTotalBytes.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotTotalTxnsDataPoint adds a data point to postgresql.replication_slot.total_txns metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotTotalTxnsDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, stateAttributeValue string) {
	mb.metricPostgresqlReplicationSlotTotalTxns.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, stateAttributeValue)
}

// RecordPostgresqlReplicationSlotXminAgeDataPoint adds a data point to postgresql.replication_slot.xmin_age metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationSlotXminAgeDataPoint(ts pcommon.Timestamp, val int64, slotNameAttributeValue string, slotTypeAttributeValue string, pluginAttributeValue string) {
	mb.metricPostgresqlReplicationSlotXminAge.recordDataPoint(mb.startTime, ts, val, slotNameAttributeValue, slotTypeAttributeValue, pluginAttributeValue)
}

// RecordPostgresqlRollbacksDataPoint adds a data point to postgresql.rollbacks metric.
func (mb *MetricsBuilder) RecordPostgresqlRollbacksDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRollbacks.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsDeletedDataPoint adds a data point to postgresql.rows_deleted metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsDeletedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsDeleted.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsFetchedDataPoint adds a data point to postgresql.rows_fetched metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsFetchedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsFetched.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsInsertedDataPoint adds a data point to postgresql.rows_inserted metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsInsertedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsInserted.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsReturnedDataPoint adds a data point to postgresql.rows_returned metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsReturnedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsReturned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlRowsUpdatedDataPoint adds a data point to postgresql.rows_updated metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsUpdatedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlRowsUpdated.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsAbandonedDataPoint adds a data point to postgresql.sessions.abandoned metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsAbandonedDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsAbandoned.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsActiveTimeDataPoint adds a data point to postgresql.sessions.active_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsActiveTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsActiveTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsCountDataPoint adds a data point to postgresql.sessions.count metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsCountDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsCount.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsFatalDataPoint adds a data point to postgresql.sessions.fatal metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsFatalDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsFatal.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsIdleInTransactionTimeDataPoint adds a data point to postgresql.sessions.idle_in_transaction_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsIdleInTransactionTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsIdleInTransactionTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsKilledDataPoint adds a data point to postgresql.sessions.killed metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsKilledDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsKilled.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlSessionsSessionTimeDataPoint adds a data point to postgresql.sessions.session_time metric.
func (mb *MetricsBuilder) RecordPostgresqlSessionsSessionTimeDataPoint(ts pcommon.Timestamp, val float64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlSessionsSessionTime.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlTempBytesDataPoint adds a data point to postgresql.temp_bytes metric.
func (mb *MetricsBuilder) RecordPostgresqlTempBytesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlTempBytes.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// RecordPostgresqlTempFilesDataPoint adds a data point to postgresql.temp_files metric.
func (mb *MetricsBuilder) RecordPostgresqlTempFilesDataPoint(ts pcommon.Timestamp, val int64, databaseNameAttributeValue string, newrelicpostgresqlInstanceNameAttributeValue string) {
	mb.metricPostgresqlTempFiles.recordDataPoint(mb.startTime, ts, val, databaseNameAttributeValue, newrelicpostgresqlInstanceNameAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
