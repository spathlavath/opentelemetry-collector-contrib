// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"fmt"
	"strconv"
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// AttributeCommand specifies the value command attribute.
type AttributeCommand int

const (
	_ AttributeCommand = iota
	AttributeCommandCommit
	AttributeCommandDelete
	AttributeCommandDeleteMulti
	AttributeCommandInsert
	AttributeCommandInsertSelect
	AttributeCommandLoad
	AttributeCommandReplace
	AttributeCommandReplaceSelect
	AttributeCommandRollback
	AttributeCommandSelect
	AttributeCommandUpdate
	AttributeCommandUpdateMulti
)

// String returns the string representation of the AttributeCommand.
func (av AttributeCommand) String() string {
	switch av {
	case AttributeCommandCommit:
		return "commit"
	case AttributeCommandDelete:
		return "delete"
	case AttributeCommandDeleteMulti:
		return "delete_multi"
	case AttributeCommandInsert:
		return "insert"
	case AttributeCommandInsertSelect:
		return "insert_select"
	case AttributeCommandLoad:
		return "load"
	case AttributeCommandReplace:
		return "replace"
	case AttributeCommandReplaceSelect:
		return "replace_select"
	case AttributeCommandRollback:
		return "rollback"
	case AttributeCommandSelect:
		return "select"
	case AttributeCommandUpdate:
		return "update"
	case AttributeCommandUpdateMulti:
		return "update_multi"
	}
	return ""
}

// MapAttributeCommand is a helper map of string to AttributeCommand attribute value.
var MapAttributeCommand = map[string]AttributeCommand{
	"commit":         AttributeCommandCommit,
	"delete":         AttributeCommandDelete,
	"delete_multi":   AttributeCommandDeleteMulti,
	"insert":         AttributeCommandInsert,
	"insert_select":  AttributeCommandInsertSelect,
	"load":           AttributeCommandLoad,
	"replace":        AttributeCommandReplace,
	"replace_select": AttributeCommandReplaceSelect,
	"rollback":       AttributeCommandRollback,
	"select":         AttributeCommandSelect,
	"update":         AttributeCommandUpdate,
	"update_multi":   AttributeCommandUpdateMulti,
}

var MetricsInfo = metricsInfo{
	NewrelicmysqlBinlogCacheDiskUse: metricInfo{
		Name: "newrelicmysql.binlog.cache_disk_use",
	},
	NewrelicmysqlBinlogCacheUse: metricInfo{
		Name: "newrelicmysql.binlog.cache_use",
	},
	NewrelicmysqlCommands: metricInfo{
		Name: "newrelicmysql.commands",
	},
	NewrelicmysqlConnectionCount: metricInfo{
		Name: "newrelicmysql.connection.count",
	},
	NewrelicmysqlDbHandlerRollback: metricInfo{
		Name: "newrelicmysql.db.handler_rollback",
	},
	NewrelicmysqlDbOpenedTables: metricInfo{
		Name: "newrelicmysql.db.opened_tables",
	},
	NewrelicmysqlInnodbBufferPoolDirty: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_dirty",
	},
	NewrelicmysqlInnodbBufferPoolFree: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_free",
	},
	NewrelicmysqlInnodbBufferPoolReadRequests: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_read_requests",
	},
	NewrelicmysqlInnodbBufferPoolReads: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_reads",
	},
	NewrelicmysqlInnodbBufferPoolTotal: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_total",
	},
	NewrelicmysqlInnodbBufferPoolUsed: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_used",
	},
	NewrelicmysqlInnodbBufferPoolUtilization: metricInfo{
		Name: "newrelicmysql.innodb.buffer_pool_utilization",
	},
	NewrelicmysqlInnodbCurrentRowLocks: metricInfo{
		Name: "newrelicmysql.innodb.current_row_locks",
	},
	NewrelicmysqlInnodbDataReads: metricInfo{
		Name: "newrelicmysql.innodb.data_reads",
	},
	NewrelicmysqlInnodbDataWrites: metricInfo{
		Name: "newrelicmysql.innodb.data_writes",
	},
	NewrelicmysqlInnodbDataWritten: metricInfo{
		Name: "newrelicmysql.innodb.data_written",
	},
	NewrelicmysqlInnodbLogWaits: metricInfo{
		Name: "newrelicmysql.innodb.log_waits",
	},
	NewrelicmysqlInnodbMutexOsWaits: metricInfo{
		Name: "newrelicmysql.innodb.mutex_os_waits",
	},
	NewrelicmysqlInnodbMutexSpinRounds: metricInfo{
		Name: "newrelicmysql.innodb.mutex_spin_rounds",
	},
	NewrelicmysqlInnodbMutexSpinWaits: metricInfo{
		Name: "newrelicmysql.innodb.mutex_spin_waits",
	},
	NewrelicmysqlInnodbOsLogFsyncs: metricInfo{
		Name: "newrelicmysql.innodb.os_log_fsyncs",
	},
	NewrelicmysqlInnodbRowLockCurrentWaits: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_current_waits",
	},
	NewrelicmysqlInnodbRowLockTime: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_time",
	},
	NewrelicmysqlInnodbRowLockWaits: metricInfo{
		Name: "newrelicmysql.innodb.row_lock_waits",
	},
	NewrelicmysqlMyisamKeyBufferBytesUnflushed: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_bytes_unflushed",
	},
	NewrelicmysqlMyisamKeyBufferBytesUsed: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_bytes_used",
	},
	NewrelicmysqlMyisamKeyBufferSize: metricInfo{
		Name: "newrelicmysql.myisam.key_buffer_size",
	},
	NewrelicmysqlMyisamKeyReadRequests: metricInfo{
		Name: "newrelicmysql.myisam.key_read_requests",
	},
	NewrelicmysqlMyisamKeyReads: metricInfo{
		Name: "newrelicmysql.myisam.key_reads",
	},
	NewrelicmysqlMyisamKeyWriteRequests: metricInfo{
		Name: "newrelicmysql.myisam.key_write_requests",
	},
	NewrelicmysqlMyisamKeyWrites: metricInfo{
		Name: "newrelicmysql.myisam.key_writes",
	},
	NewrelicmysqlNetAbortedClients: metricInfo{
		Name: "newrelicmysql.net.aborted_clients",
	},
	NewrelicmysqlNetAbortedConnects: metricInfo{
		Name: "newrelicmysql.net.aborted_connects",
	},
	NewrelicmysqlNetConnections: metricInfo{
		Name: "newrelicmysql.net.connections",
	},
	NewrelicmysqlNetMaxConnections: metricInfo{
		Name: "newrelicmysql.net.max_connections",
	},
	NewrelicmysqlNetMaxConnectionsAvailable: metricInfo{
		Name: "newrelicmysql.net.max_connections_available",
	},
	NewrelicmysqlNetMaxUsedConnections: metricInfo{
		Name: "newrelicmysql.net.max_used_connections",
	},
	NewrelicmysqlPerformanceBytesReceived: metricInfo{
		Name: "newrelicmysql.performance.bytes_received",
	},
	NewrelicmysqlPerformanceBytesSent: metricInfo{
		Name: "newrelicmysql.performance.bytes_sent",
	},
	NewrelicmysqlPerformanceCreatedTmpDiskTables: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_disk_tables",
	},
	NewrelicmysqlPerformanceCreatedTmpFiles: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_files",
	},
	NewrelicmysqlPerformanceCreatedTmpTables: metricInfo{
		Name: "newrelicmysql.performance.created_tmp_tables",
	},
	NewrelicmysqlPerformanceHandlerCommit: metricInfo{
		Name: "newrelicmysql.performance.handler_commit",
	},
	NewrelicmysqlPerformanceHandlerDelete: metricInfo{
		Name: "newrelicmysql.performance.handler_delete",
	},
	NewrelicmysqlPerformanceHandlerPrepare: metricInfo{
		Name: "newrelicmysql.performance.handler_prepare",
	},
	NewrelicmysqlPerformanceHandlerReadFirst: metricInfo{
		Name: "newrelicmysql.performance.handler_read_first",
	},
	NewrelicmysqlPerformanceHandlerReadKey: metricInfo{
		Name: "newrelicmysql.performance.handler_read_key",
	},
	NewrelicmysqlPerformanceHandlerReadNext: metricInfo{
		Name: "newrelicmysql.performance.handler_read_next",
	},
	NewrelicmysqlPerformanceHandlerReadPrev: metricInfo{
		Name: "newrelicmysql.performance.handler_read_prev",
	},
	NewrelicmysqlPerformanceHandlerReadRnd: metricInfo{
		Name: "newrelicmysql.performance.handler_read_rnd",
	},
	NewrelicmysqlPerformanceHandlerReadRndNext: metricInfo{
		Name: "newrelicmysql.performance.handler_read_rnd_next",
	},
	NewrelicmysqlPerformanceHandlerRollback: metricInfo{
		Name: "newrelicmysql.performance.handler_rollback",
	},
	NewrelicmysqlPerformanceHandlerUpdate: metricInfo{
		Name: "newrelicmysql.performance.handler_update",
	},
	NewrelicmysqlPerformanceHandlerWrite: metricInfo{
		Name: "newrelicmysql.performance.handler_write",
	},
	NewrelicmysqlPerformanceKeyCacheUtilization: metricInfo{
		Name: "newrelicmysql.performance.key_cache_utilization",
	},
	NewrelicmysqlPerformanceMaxPreparedStmtCount: metricInfo{
		Name: "newrelicmysql.performance.max_prepared_stmt_count",
	},
	NewrelicmysqlPerformanceOpenFiles: metricInfo{
		Name: "newrelicmysql.performance.open_files",
	},
	NewrelicmysqlPerformanceOpenTables: metricInfo{
		Name: "newrelicmysql.performance.open_tables",
	},
	NewrelicmysqlPerformanceOpenedTables: metricInfo{
		Name: "newrelicmysql.performance.opened_tables",
	},
	NewrelicmysqlPerformancePerformanceSchemaDigestLost: metricInfo{
		Name: "newrelicmysql.performance.performance_schema_digest_lost",
	},
	NewrelicmysqlPerformancePreparedStmtCount: metricInfo{
		Name: "newrelicmysql.performance.prepared_stmt_count",
	},
	NewrelicmysqlPerformanceQcacheFreeBlocks: metricInfo{
		Name: "newrelicmysql.performance.qcache_free_blocks",
	},
	NewrelicmysqlPerformanceQcacheFreeMemory: metricInfo{
		Name: "newrelicmysql.performance.qcache_free_memory",
	},
	NewrelicmysqlPerformanceQcacheHits: metricInfo{
		Name: "newrelicmysql.performance.qcache_hits",
	},
	NewrelicmysqlPerformanceQcacheInserts: metricInfo{
		Name: "newrelicmysql.performance.qcache_inserts",
	},
	NewrelicmysqlPerformanceQcacheLowmemPrunes: metricInfo{
		Name: "newrelicmysql.performance.qcache_lowmem_prunes",
	},
	NewrelicmysqlPerformanceQcacheNotCached: metricInfo{
		Name: "newrelicmysql.performance.qcache_not_cached",
	},
	NewrelicmysqlPerformanceQcacheQueriesInCache: metricInfo{
		Name: "newrelicmysql.performance.qcache_queries_in_cache",
	},
	NewrelicmysqlPerformanceQcacheSize: metricInfo{
		Name: "newrelicmysql.performance.qcache_size",
	},
	NewrelicmysqlPerformanceQcacheTotalBlocks: metricInfo{
		Name: "newrelicmysql.performance.qcache_total_blocks",
	},
	NewrelicmysqlPerformanceQuestions: metricInfo{
		Name: "newrelicmysql.performance.questions",
	},
	NewrelicmysqlPerformanceSelectFullJoin: metricInfo{
		Name: "newrelicmysql.performance.select_full_join",
	},
	NewrelicmysqlPerformanceSelectFullRangeJoin: metricInfo{
		Name: "newrelicmysql.performance.select_full_range_join",
	},
	NewrelicmysqlPerformanceSelectRange: metricInfo{
		Name: "newrelicmysql.performance.select_range",
	},
	NewrelicmysqlPerformanceSelectRangeCheck: metricInfo{
		Name: "newrelicmysql.performance.select_range_check",
	},
	NewrelicmysqlPerformanceSelectScan: metricInfo{
		Name: "newrelicmysql.performance.select_scan",
	},
	NewrelicmysqlPerformanceSlowQueries: metricInfo{
		Name: "newrelicmysql.performance.slow_queries",
	},
	NewrelicmysqlPerformanceSortMergePasses: metricInfo{
		Name: "newrelicmysql.performance.sort_merge_passes",
	},
	NewrelicmysqlPerformanceSortRange: metricInfo{
		Name: "newrelicmysql.performance.sort_range",
	},
	NewrelicmysqlPerformanceSortRows: metricInfo{
		Name: "newrelicmysql.performance.sort_rows",
	},
	NewrelicmysqlPerformanceSortScan: metricInfo{
		Name: "newrelicmysql.performance.sort_scan",
	},
	NewrelicmysqlPerformanceTableLocksImmediate: metricInfo{
		Name: "newrelicmysql.performance.table_locks_immediate",
	},
	NewrelicmysqlPerformanceTableLocksImmediateRate: metricInfo{
		Name: "newrelicmysql.performance.table_locks_immediate.rate",
	},
	NewrelicmysqlPerformanceTableLocksWaited: metricInfo{
		Name: "newrelicmysql.performance.table_locks_waited",
	},
	NewrelicmysqlPerformanceTableOpenCache: metricInfo{
		Name: "newrelicmysql.performance.table_open_cache",
	},
	NewrelicmysqlPerformanceThreadCacheSize: metricInfo{
		Name: "newrelicmysql.performance.thread_cache_size",
	},
	NewrelicmysqlPerformanceThreadsCached: metricInfo{
		Name: "newrelicmysql.performance.threads_cached",
	},
	NewrelicmysqlPerformanceThreadsConnected: metricInfo{
		Name: "newrelicmysql.performance.threads_connected",
	},
	NewrelicmysqlPerformanceThreadsCreated: metricInfo{
		Name: "newrelicmysql.performance.threads_created",
	},
	NewrelicmysqlPerformanceThreadsRunning: metricInfo{
		Name: "newrelicmysql.performance.threads_running",
	},
	NewrelicmysqlQueryCount: metricInfo{
		Name: "newrelicmysql.query.count",
	},
	NewrelicmysqlReplicationExecMasterLogPos: metricInfo{
		Name: "newrelicmysql.replication.exec_master_log_pos",
	},
	NewrelicmysqlReplicationLastIoErrno: metricInfo{
		Name: "newrelicmysql.replication.last_io_errno",
	},
	NewrelicmysqlReplicationLastSQLErrno: metricInfo{
		Name: "newrelicmysql.replication.last_sql_errno",
	},
	NewrelicmysqlReplicationReadMasterLogPos: metricInfo{
		Name: "newrelicmysql.replication.read_master_log_pos",
	},
	NewrelicmysqlReplicationRelayLogSpace: metricInfo{
		Name: "newrelicmysql.replication.relay_log_space",
	},
	NewrelicmysqlReplicationSecondsBehindMaster: metricInfo{
		Name: "newrelicmysql.replication.seconds_behind_master",
	},
	NewrelicmysqlReplicationSlaveIoRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_io_running",
	},
	NewrelicmysqlReplicationSlaveRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_running",
	},
	NewrelicmysqlReplicationSlaveSQLRunning: metricInfo{
		Name: "newrelicmysql.replication.slave_sql_running",
	},
	NewrelicmysqlUptime: metricInfo{
		Name: "newrelicmysql.uptime",
	},
}

type metricsInfo struct {
	NewrelicmysqlBinlogCacheDiskUse                     metricInfo
	NewrelicmysqlBinlogCacheUse                         metricInfo
	NewrelicmysqlCommands                               metricInfo
	NewrelicmysqlConnectionCount                        metricInfo
	NewrelicmysqlDbHandlerRollback                      metricInfo
	NewrelicmysqlDbOpenedTables                         metricInfo
	NewrelicmysqlInnodbBufferPoolDirty                  metricInfo
	NewrelicmysqlInnodbBufferPoolFree                   metricInfo
	NewrelicmysqlInnodbBufferPoolReadRequests           metricInfo
	NewrelicmysqlInnodbBufferPoolReads                  metricInfo
	NewrelicmysqlInnodbBufferPoolTotal                  metricInfo
	NewrelicmysqlInnodbBufferPoolUsed                   metricInfo
	NewrelicmysqlInnodbBufferPoolUtilization            metricInfo
	NewrelicmysqlInnodbCurrentRowLocks                  metricInfo
	NewrelicmysqlInnodbDataReads                        metricInfo
	NewrelicmysqlInnodbDataWrites                       metricInfo
	NewrelicmysqlInnodbDataWritten                      metricInfo
	NewrelicmysqlInnodbLogWaits                         metricInfo
	NewrelicmysqlInnodbMutexOsWaits                     metricInfo
	NewrelicmysqlInnodbMutexSpinRounds                  metricInfo
	NewrelicmysqlInnodbMutexSpinWaits                   metricInfo
	NewrelicmysqlInnodbOsLogFsyncs                      metricInfo
	NewrelicmysqlInnodbRowLockCurrentWaits              metricInfo
	NewrelicmysqlInnodbRowLockTime                      metricInfo
	NewrelicmysqlInnodbRowLockWaits                     metricInfo
	NewrelicmysqlMyisamKeyBufferBytesUnflushed          metricInfo
	NewrelicmysqlMyisamKeyBufferBytesUsed               metricInfo
	NewrelicmysqlMyisamKeyBufferSize                    metricInfo
	NewrelicmysqlMyisamKeyReadRequests                  metricInfo
	NewrelicmysqlMyisamKeyReads                         metricInfo
	NewrelicmysqlMyisamKeyWriteRequests                 metricInfo
	NewrelicmysqlMyisamKeyWrites                        metricInfo
	NewrelicmysqlNetAbortedClients                      metricInfo
	NewrelicmysqlNetAbortedConnects                     metricInfo
	NewrelicmysqlNetConnections                         metricInfo
	NewrelicmysqlNetMaxConnections                      metricInfo
	NewrelicmysqlNetMaxConnectionsAvailable             metricInfo
	NewrelicmysqlNetMaxUsedConnections                  metricInfo
	NewrelicmysqlPerformanceBytesReceived               metricInfo
	NewrelicmysqlPerformanceBytesSent                   metricInfo
	NewrelicmysqlPerformanceCreatedTmpDiskTables        metricInfo
	NewrelicmysqlPerformanceCreatedTmpFiles             metricInfo
	NewrelicmysqlPerformanceCreatedTmpTables            metricInfo
	NewrelicmysqlPerformanceHandlerCommit               metricInfo
	NewrelicmysqlPerformanceHandlerDelete               metricInfo
	NewrelicmysqlPerformanceHandlerPrepare              metricInfo
	NewrelicmysqlPerformanceHandlerReadFirst            metricInfo
	NewrelicmysqlPerformanceHandlerReadKey              metricInfo
	NewrelicmysqlPerformanceHandlerReadNext             metricInfo
	NewrelicmysqlPerformanceHandlerReadPrev             metricInfo
	NewrelicmysqlPerformanceHandlerReadRnd              metricInfo
	NewrelicmysqlPerformanceHandlerReadRndNext          metricInfo
	NewrelicmysqlPerformanceHandlerRollback             metricInfo
	NewrelicmysqlPerformanceHandlerUpdate               metricInfo
	NewrelicmysqlPerformanceHandlerWrite                metricInfo
	NewrelicmysqlPerformanceKeyCacheUtilization         metricInfo
	NewrelicmysqlPerformanceMaxPreparedStmtCount        metricInfo
	NewrelicmysqlPerformanceOpenFiles                   metricInfo
	NewrelicmysqlPerformanceOpenTables                  metricInfo
	NewrelicmysqlPerformanceOpenedTables                metricInfo
	NewrelicmysqlPerformancePerformanceSchemaDigestLost metricInfo
	NewrelicmysqlPerformancePreparedStmtCount           metricInfo
	NewrelicmysqlPerformanceQcacheFreeBlocks            metricInfo
	NewrelicmysqlPerformanceQcacheFreeMemory            metricInfo
	NewrelicmysqlPerformanceQcacheHits                  metricInfo
	NewrelicmysqlPerformanceQcacheInserts               metricInfo
	NewrelicmysqlPerformanceQcacheLowmemPrunes          metricInfo
	NewrelicmysqlPerformanceQcacheNotCached             metricInfo
	NewrelicmysqlPerformanceQcacheQueriesInCache        metricInfo
	NewrelicmysqlPerformanceQcacheSize                  metricInfo
	NewrelicmysqlPerformanceQcacheTotalBlocks           metricInfo
	NewrelicmysqlPerformanceQuestions                   metricInfo
	NewrelicmysqlPerformanceSelectFullJoin              metricInfo
	NewrelicmysqlPerformanceSelectFullRangeJoin         metricInfo
	NewrelicmysqlPerformanceSelectRange                 metricInfo
	NewrelicmysqlPerformanceSelectRangeCheck            metricInfo
	NewrelicmysqlPerformanceSelectScan                  metricInfo
	NewrelicmysqlPerformanceSlowQueries                 metricInfo
	NewrelicmysqlPerformanceSortMergePasses             metricInfo
	NewrelicmysqlPerformanceSortRange                   metricInfo
	NewrelicmysqlPerformanceSortRows                    metricInfo
	NewrelicmysqlPerformanceSortScan                    metricInfo
	NewrelicmysqlPerformanceTableLocksImmediate         metricInfo
	NewrelicmysqlPerformanceTableLocksImmediateRate     metricInfo
	NewrelicmysqlPerformanceTableLocksWaited            metricInfo
	NewrelicmysqlPerformanceTableOpenCache              metricInfo
	NewrelicmysqlPerformanceThreadCacheSize             metricInfo
	NewrelicmysqlPerformanceThreadsCached               metricInfo
	NewrelicmysqlPerformanceThreadsConnected            metricInfo
	NewrelicmysqlPerformanceThreadsCreated              metricInfo
	NewrelicmysqlPerformanceThreadsRunning              metricInfo
	NewrelicmysqlQueryCount                             metricInfo
	NewrelicmysqlReplicationExecMasterLogPos            metricInfo
	NewrelicmysqlReplicationLastIoErrno                 metricInfo
	NewrelicmysqlReplicationLastSQLErrno                metricInfo
	NewrelicmysqlReplicationReadMasterLogPos            metricInfo
	NewrelicmysqlReplicationRelayLogSpace               metricInfo
	NewrelicmysqlReplicationSecondsBehindMaster         metricInfo
	NewrelicmysqlReplicationSlaveIoRunning              metricInfo
	NewrelicmysqlReplicationSlaveRunning                metricInfo
	NewrelicmysqlReplicationSlaveSQLRunning             metricInfo
	NewrelicmysqlUptime                                 metricInfo
}

type metricInfo struct {
	Name string
}

type metricNewrelicmysqlBinlogCacheDiskUse struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.binlog.cache_disk_use metric with initial data.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) init() {
	m.data.SetName("newrelicmysql.binlog.cache_disk_use")
	m.data.SetDescription("The number of transactions that used the temporary binary log cache but exceeded binlog_cache_size and used a temporary file.")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlBinlogCacheDiskUse) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlBinlogCacheDiskUse) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlBinlogCacheDiskUse(cfg MetricConfig) metricNewrelicmysqlBinlogCacheDiskUse {
	m := metricNewrelicmysqlBinlogCacheDiskUse{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlBinlogCacheUse struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.binlog.cache_use metric with initial data.
func (m *metricNewrelicmysqlBinlogCacheUse) init() {
	m.data.SetName("newrelicmysql.binlog.cache_use")
	m.data.SetDescription("The number of transactions that used the binary log cache.")
	m.data.SetUnit("{transactions}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlBinlogCacheUse) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlBinlogCacheUse) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlBinlogCacheUse) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlBinlogCacheUse(cfg MetricConfig) metricNewrelicmysqlBinlogCacheUse {
	m := metricNewrelicmysqlBinlogCacheUse{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlCommands struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.commands metric with initial data.
func (m *metricNewrelicmysqlCommands) init() {
	m.data.SetName("newrelicmysql.commands")
	m.data.SetDescription("The number of times each type of command has been executed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricNewrelicmysqlCommands) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, commandAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("command", commandAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlCommands) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlCommands) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlCommands(cfg MetricConfig) metricNewrelicmysqlCommands {
	m := metricNewrelicmysqlCommands{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlConnectionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.connection.count metric with initial data.
func (m *metricNewrelicmysqlConnectionCount) init() {
	m.data.SetName("newrelicmysql.connection.count")
	m.data.SetDescription("The number of connection attempts (successful or not) to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlConnectionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlConnectionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlConnectionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlConnectionCount(cfg MetricConfig) metricNewrelicmysqlConnectionCount {
	m := metricNewrelicmysqlConnectionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlDbHandlerRollback struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.db.handler_rollback metric with initial data.
func (m *metricNewrelicmysqlDbHandlerRollback) init() {
	m.data.SetName("newrelicmysql.db.handler_rollback")
	m.data.SetDescription("The number of internal ROLLBACK statements.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlDbHandlerRollback) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlDbHandlerRollback) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlDbHandlerRollback) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlDbHandlerRollback(cfg MetricConfig) metricNewrelicmysqlDbHandlerRollback {
	m := metricNewrelicmysqlDbHandlerRollback{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlDbOpenedTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.db.opened_tables metric with initial data.
func (m *metricNewrelicmysqlDbOpenedTables) init() {
	m.data.SetName("newrelicmysql.db.opened_tables")
	m.data.SetDescription("The number of tables that have been opened.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlDbOpenedTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlDbOpenedTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlDbOpenedTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlDbOpenedTables(cfg MetricConfig) metricNewrelicmysqlDbOpenedTables {
	m := metricNewrelicmysqlDbOpenedTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolDirty struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_dirty metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_dirty")
	m.data.SetDescription("The number of dirty pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolDirty) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolDirty) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolDirty(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolDirty {
	m := metricNewrelicmysqlInnodbBufferPoolDirty{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_free metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_free")
	m.data.SetDescription("The number of free pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolFree(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolFree {
	m := metricNewrelicmysqlInnodbBufferPoolFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReadRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_read_requests metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_read_requests")
	m.data.SetDescription("The number of logical read requests.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReadRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReadRequests(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReadRequests {
	m := metricNewrelicmysqlInnodbBufferPoolReadRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_reads")
	m.data.SetDescription("The number of reads that InnoDB could not satisfy from the buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbBufferPoolReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolReads(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolReads {
	m := metricNewrelicmysqlInnodbBufferPoolReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_total metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_total")
	m.data.SetDescription("The total size of the InnoDB buffer pool in pages.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolTotal(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolTotal {
	m := metricNewrelicmysqlInnodbBufferPoolTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_used metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_used")
	m.data.SetDescription("The number of used pages in the InnoDB buffer pool.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolUsed(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolUsed {
	m := metricNewrelicmysqlInnodbBufferPoolUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbBufferPoolUtilization struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.buffer_pool_utilization metric with initial data.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) init() {
	m.data.SetName("newrelicmysql.innodb.buffer_pool_utilization")
	m.data.SetDescription("The InnoDB buffer pool utilization percentage.")
	m.data.SetUnit("%")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbBufferPoolUtilization) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbBufferPoolUtilization(cfg MetricConfig) metricNewrelicmysqlInnodbBufferPoolUtilization {
	m := metricNewrelicmysqlInnodbBufferPoolUtilization{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbCurrentRowLocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.current_row_locks metric with initial data.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) init() {
	m.data.SetName("newrelicmysql.innodb.current_row_locks")
	m.data.SetDescription("The number of current row locks.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbCurrentRowLocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbCurrentRowLocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbCurrentRowLocks(cfg MetricConfig) metricNewrelicmysqlInnodbCurrentRowLocks {
	m := metricNewrelicmysqlInnodbCurrentRowLocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_reads metric with initial data.
func (m *metricNewrelicmysqlInnodbDataReads) init() {
	m.data.SetName("newrelicmysql.innodb.data_reads")
	m.data.SetDescription("The amount of data read.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataReads(cfg MetricConfig) metricNewrelicmysqlInnodbDataReads {
	m := metricNewrelicmysqlInnodbDataReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_writes metric with initial data.
func (m *metricNewrelicmysqlInnodbDataWrites) init() {
	m.data.SetName("newrelicmysql.innodb.data_writes")
	m.data.SetDescription("The amount of data written.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataWrites(cfg MetricConfig) metricNewrelicmysqlInnodbDataWrites {
	m := metricNewrelicmysqlInnodbDataWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbDataWritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.data_written metric with initial data.
func (m *metricNewrelicmysqlInnodbDataWritten) init() {
	m.data.SetName("newrelicmysql.innodb.data_written")
	m.data.SetDescription("The amount of data written to InnoDB tables in bytes.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbDataWritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbDataWritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbDataWritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbDataWritten(cfg MetricConfig) metricNewrelicmysqlInnodbDataWritten {
	m := metricNewrelicmysqlInnodbDataWritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbLogWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.log_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbLogWaits) init() {
	m.data.SetName("newrelicmysql.innodb.log_waits")
	m.data.SetDescription("The number of times the log buffer was too small and a wait was required for it to be flushed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbLogWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbLogWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbLogWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbLogWaits(cfg MetricConfig) metricNewrelicmysqlInnodbLogWaits {
	m := metricNewrelicmysqlInnodbLogWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexOsWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_os_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_os_waits")
	m.data.SetDescription("The number of mutex OS waits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexOsWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexOsWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexOsWaits(cfg MetricConfig) metricNewrelicmysqlInnodbMutexOsWaits {
	m := metricNewrelicmysqlInnodbMutexOsWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexSpinRounds struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_spin_rounds metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_spin_rounds")
	m.data.SetDescription("The number of mutex spin rounds.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexSpinRounds) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexSpinRounds) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexSpinRounds(cfg MetricConfig) metricNewrelicmysqlInnodbMutexSpinRounds {
	m := metricNewrelicmysqlInnodbMutexSpinRounds{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbMutexSpinWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.mutex_spin_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) init() {
	m.data.SetName("newrelicmysql.innodb.mutex_spin_waits")
	m.data.SetDescription("The number of mutex spin waits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbMutexSpinWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbMutexSpinWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbMutexSpinWaits(cfg MetricConfig) metricNewrelicmysqlInnodbMutexSpinWaits {
	m := metricNewrelicmysqlInnodbMutexSpinWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbOsLogFsyncs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.os_log_fsyncs metric with initial data.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) init() {
	m.data.SetName("newrelicmysql.innodb.os_log_fsyncs")
	m.data.SetDescription("The number of fsync writes done to the InnoDB redo log files.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbOsLogFsyncs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbOsLogFsyncs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbOsLogFsyncs(cfg MetricConfig) metricNewrelicmysqlInnodbOsLogFsyncs {
	m := metricNewrelicmysqlInnodbOsLogFsyncs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockCurrentWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_current_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_current_waits")
	m.data.SetDescription("The number of row locks currently being waited for.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockCurrentWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockCurrentWaits(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockCurrentWaits {
	m := metricNewrelicmysqlInnodbRowLockCurrentWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_time metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockTime) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_time")
	m.data.SetDescription("The total time spent in acquiring row locks.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowLockTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockTime(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockTime {
	m := metricNewrelicmysqlInnodbRowLockTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlInnodbRowLockWaits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.innodb.row_lock_waits metric with initial data.
func (m *metricNewrelicmysqlInnodbRowLockWaits) init() {
	m.data.SetName("newrelicmysql.innodb.row_lock_waits")
	m.data.SetDescription("The number of times operations had to wait for a row lock.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlInnodbRowLockWaits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlInnodbRowLockWaits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlInnodbRowLockWaits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlInnodbRowLockWaits(cfg MetricConfig) metricNewrelicmysqlInnodbRowLockWaits {
	m := metricNewrelicmysqlInnodbRowLockWaits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferBytesUnflushed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_bytes_unflushed metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_bytes_unflushed")
	m.data.SetDescription("MyISAM key buffer bytes unflushed.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUnflushed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferBytesUnflushed(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferBytesUnflushed {
	m := metricNewrelicmysqlMyisamKeyBufferBytesUnflushed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferBytesUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_bytes_used metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_bytes_used")
	m.data.SetDescription("MyISAM key buffer bytes used.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferBytesUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferBytesUsed(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferBytesUsed {
	m := metricNewrelicmysqlMyisamKeyBufferBytesUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyBufferSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_buffer_size metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) init() {
	m.data.SetName("newrelicmysql.myisam.key_buffer_size")
	m.data.SetDescription("Size of the buffer used for index blocks.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlMyisamKeyBufferSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyBufferSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyBufferSize(cfg MetricConfig) metricNewrelicmysqlMyisamKeyBufferSize {
	m := metricNewrelicmysqlMyisamKeyBufferSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyReadRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_read_requests metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) init() {
	m.data.SetName("newrelicmysql.myisam.key_read_requests")
	m.data.SetDescription("The number of requests to read a key block from the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyReadRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyReadRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyReadRequests(cfg MetricConfig) metricNewrelicmysqlMyisamKeyReadRequests {
	m := metricNewrelicmysqlMyisamKeyReadRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_reads metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyReads) init() {
	m.data.SetName("newrelicmysql.myisam.key_reads")
	m.data.SetDescription("The number of physical reads of a key block from disk into the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyReads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyReads(cfg MetricConfig) metricNewrelicmysqlMyisamKeyReads {
	m := metricNewrelicmysqlMyisamKeyReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyWriteRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_write_requests metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) init() {
	m.data.SetName("newrelicmysql.myisam.key_write_requests")
	m.data.SetDescription("The number of requests to write a key block to the MyISAM key cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyWriteRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyWriteRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyWriteRequests(cfg MetricConfig) metricNewrelicmysqlMyisamKeyWriteRequests {
	m := metricNewrelicmysqlMyisamKeyWriteRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlMyisamKeyWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.myisam.key_writes metric with initial data.
func (m *metricNewrelicmysqlMyisamKeyWrites) init() {
	m.data.SetName("newrelicmysql.myisam.key_writes")
	m.data.SetDescription("The number of physical writes of a key block from the MyISAM key cache to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlMyisamKeyWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlMyisamKeyWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlMyisamKeyWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlMyisamKeyWrites(cfg MetricConfig) metricNewrelicmysqlMyisamKeyWrites {
	m := metricNewrelicmysqlMyisamKeyWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetAbortedClients struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.aborted_clients metric with initial data.
func (m *metricNewrelicmysqlNetAbortedClients) init() {
	m.data.SetName("newrelicmysql.net.aborted_clients")
	m.data.SetDescription("The number of connections that were aborted because the client died without closing the connection properly.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetAbortedClients) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetAbortedClients) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetAbortedClients) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetAbortedClients(cfg MetricConfig) metricNewrelicmysqlNetAbortedClients {
	m := metricNewrelicmysqlNetAbortedClients{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetAbortedConnects struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.aborted_connects metric with initial data.
func (m *metricNewrelicmysqlNetAbortedConnects) init() {
	m.data.SetName("newrelicmysql.net.aborted_connects")
	m.data.SetDescription("The number of failed attempts to connect to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetAbortedConnects) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetAbortedConnects) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetAbortedConnects) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetAbortedConnects(cfg MetricConfig) metricNewrelicmysqlNetAbortedConnects {
	m := metricNewrelicmysqlNetAbortedConnects{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.connections metric with initial data.
func (m *metricNewrelicmysqlNetConnections) init() {
	m.data.SetName("newrelicmysql.net.connections")
	m.data.SetDescription("The number of connection attempts (successful or not) to the MySQL server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlNetConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetConnections(cfg MetricConfig) metricNewrelicmysqlNetConnections {
	m := metricNewrelicmysqlNetConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_connections metric with initial data.
func (m *metricNewrelicmysqlNetMaxConnections) init() {
	m.data.SetName("newrelicmysql.net.max_connections")
	m.data.SetDescription("The maximum permitted number of simultaneous client connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxConnections(cfg MetricConfig) metricNewrelicmysqlNetMaxConnections {
	m := metricNewrelicmysqlNetMaxConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxConnectionsAvailable struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_connections_available metric with initial data.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) init() {
	m.data.SetName("newrelicmysql.net.max_connections_available")
	m.data.SetDescription("The number of available connections (max_connections - threads_connected).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxConnectionsAvailable) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxConnectionsAvailable(cfg MetricConfig) metricNewrelicmysqlNetMaxConnectionsAvailable {
	m := metricNewrelicmysqlNetMaxConnectionsAvailable{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlNetMaxUsedConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.net.max_used_connections metric with initial data.
func (m *metricNewrelicmysqlNetMaxUsedConnections) init() {
	m.data.SetName("newrelicmysql.net.max_used_connections")
	m.data.SetDescription("The maximum number of connections that have been in use simultaneously since the server started.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlNetMaxUsedConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlNetMaxUsedConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlNetMaxUsedConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlNetMaxUsedConnections(cfg MetricConfig) metricNewrelicmysqlNetMaxUsedConnections {
	m := metricNewrelicmysqlNetMaxUsedConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceBytesReceived struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.bytes_received metric with initial data.
func (m *metricNewrelicmysqlPerformanceBytesReceived) init() {
	m.data.SetName("newrelicmysql.performance.bytes_received")
	m.data.SetDescription("The number of bytes received from all clients.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceBytesReceived) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceBytesReceived) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceBytesReceived) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceBytesReceived(cfg MetricConfig) metricNewrelicmysqlPerformanceBytesReceived {
	m := metricNewrelicmysqlPerformanceBytesReceived{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceBytesSent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.bytes_sent metric with initial data.
func (m *metricNewrelicmysqlPerformanceBytesSent) init() {
	m.data.SetName("newrelicmysql.performance.bytes_sent")
	m.data.SetDescription("The number of bytes sent to all clients.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceBytesSent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceBytesSent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceBytesSent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceBytesSent(cfg MetricConfig) metricNewrelicmysqlPerformanceBytesSent {
	m := metricNewrelicmysqlPerformanceBytesSent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpDiskTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_disk_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_disk_tables")
	m.data.SetDescription("The number of internal on-disk temporary tables created by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpDiskTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpDiskTables(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpDiskTables {
	m := metricNewrelicmysqlPerformanceCreatedTmpDiskTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_files metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_files")
	m.data.SetDescription("How many temporary files mysqld has created.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpFiles(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpFiles {
	m := metricNewrelicmysqlPerformanceCreatedTmpFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceCreatedTmpTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.created_tmp_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) init() {
	m.data.SetName("newrelicmysql.performance.created_tmp_tables")
	m.data.SetDescription("The number of internal temporary tables created by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceCreatedTmpTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceCreatedTmpTables(cfg MetricConfig) metricNewrelicmysqlPerformanceCreatedTmpTables {
	m := metricNewrelicmysqlPerformanceCreatedTmpTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerCommit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_commit metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) init() {
	m.data.SetName("newrelicmysql.performance.handler_commit")
	m.data.SetDescription("The number of internal COMMIT statements.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerCommit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerCommit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerCommit(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerCommit {
	m := metricNewrelicmysqlPerformanceHandlerCommit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerDelete struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_delete metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) init() {
	m.data.SetName("newrelicmysql.performance.handler_delete")
	m.data.SetDescription("The number of times that rows have been deleted from tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerDelete) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerDelete) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerDelete(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerDelete {
	m := metricNewrelicmysqlPerformanceHandlerDelete{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerPrepare struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_prepare metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) init() {
	m.data.SetName("newrelicmysql.performance.handler_prepare")
	m.data.SetDescription("A counter for the prepare phase of two-phase commit operations.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerPrepare) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerPrepare) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerPrepare(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerPrepare {
	m := metricNewrelicmysqlPerformanceHandlerPrepare{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadFirst struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_first metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_first")
	m.data.SetDescription("The number of times the first entry in an index was read.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadFirst) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadFirst(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadFirst {
	m := metricNewrelicmysqlPerformanceHandlerReadFirst{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadKey struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_key metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_key")
	m.data.SetDescription("The number of requests to read a row based on a key.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadKey) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadKey) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadKey(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadKey {
	m := metricNewrelicmysqlPerformanceHandlerReadKey{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadNext struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_next metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_next")
	m.data.SetDescription("The number of requests to read the next row in key order.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadNext) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadNext) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadNext(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadNext {
	m := metricNewrelicmysqlPerformanceHandlerReadNext{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadPrev struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_prev metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_prev")
	m.data.SetDescription("The number of requests to read the previous row in key order.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadPrev) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadPrev(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadPrev {
	m := metricNewrelicmysqlPerformanceHandlerReadPrev{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadRnd struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_rnd metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_rnd")
	m.data.SetDescription("The number of requests to read a row based on a fixed position.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadRnd) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadRnd(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadRnd {
	m := metricNewrelicmysqlPerformanceHandlerReadRnd{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerReadRndNext struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_read_rnd_next metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) init() {
	m.data.SetName("newrelicmysql.performance.handler_read_rnd_next")
	m.data.SetDescription("The number of requests to read the next row in the data file.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerReadRndNext) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerReadRndNext(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerReadRndNext {
	m := metricNewrelicmysqlPerformanceHandlerReadRndNext{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerRollback struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_rollback metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) init() {
	m.data.SetName("newrelicmysql.performance.handler_rollback")
	m.data.SetDescription("The number of requests for a storage engine to perform a rollback operation.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerRollback) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerRollback) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerRollback(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerRollback {
	m := metricNewrelicmysqlPerformanceHandlerRollback{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerUpdate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_update metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) init() {
	m.data.SetName("newrelicmysql.performance.handler_update")
	m.data.SetDescription("The number of requests to update a row in a table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerUpdate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerUpdate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerUpdate(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerUpdate {
	m := metricNewrelicmysqlPerformanceHandlerUpdate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceHandlerWrite struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.handler_write metric with initial data.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) init() {
	m.data.SetName("newrelicmysql.performance.handler_write")
	m.data.SetDescription("The number of requests to insert a row in a table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceHandlerWrite) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceHandlerWrite) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceHandlerWrite(cfg MetricConfig) metricNewrelicmysqlPerformanceHandlerWrite {
	m := metricNewrelicmysqlPerformanceHandlerWrite{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceKeyCacheUtilization struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.key_cache_utilization metric with initial data.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) init() {
	m.data.SetName("newrelicmysql.performance.key_cache_utilization")
	m.data.SetDescription("The key cache utilization percentage.")
	m.data.SetUnit("%")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceKeyCacheUtilization) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceKeyCacheUtilization(cfg MetricConfig) metricNewrelicmysqlPerformanceKeyCacheUtilization {
	m := metricNewrelicmysqlPerformanceKeyCacheUtilization{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceMaxPreparedStmtCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.max_prepared_stmt_count metric with initial data.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) init() {
	m.data.SetName("newrelicmysql.performance.max_prepared_stmt_count")
	m.data.SetDescription("Maximum number of prepared statements.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceMaxPreparedStmtCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceMaxPreparedStmtCount(cfg MetricConfig) metricNewrelicmysqlPerformanceMaxPreparedStmtCount {
	m := metricNewrelicmysqlPerformanceMaxPreparedStmtCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.open_files metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenFiles) init() {
	m.data.SetName("newrelicmysql.performance.open_files")
	m.data.SetDescription("The number of files that are currently open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceOpenFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenFiles) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenFiles(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenFiles {
	m := metricNewrelicmysqlPerformanceOpenFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.open_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenTables) init() {
	m.data.SetName("newrelicmysql.performance.open_tables")
	m.data.SetDescription("The number of tables that are currently open.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceOpenTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenTables) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenTables(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenTables {
	m := metricNewrelicmysqlPerformanceOpenTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceOpenedTables struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.opened_tables metric with initial data.
func (m *metricNewrelicmysqlPerformanceOpenedTables) init() {
	m.data.SetName("newrelicmysql.performance.opened_tables")
	m.data.SetDescription("The number of tables that have been opened. If this value is large, your table_open_cache value is probably too small.")
	m.data.SetUnit("{tables}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceOpenedTables) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceOpenedTables) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceOpenedTables) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceOpenedTables(cfg MetricConfig) metricNewrelicmysqlPerformanceOpenedTables {
	m := metricNewrelicmysqlPerformanceOpenedTables{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformancePerformanceSchemaDigestLost struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.performance_schema_digest_lost metric with initial data.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) init() {
	m.data.SetName("newrelicmysql.performance.performance_schema_digest_lost")
	m.data.SetDescription("Number of digest lost in performance schema.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformancePerformanceSchemaDigestLost) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformancePerformanceSchemaDigestLost(cfg MetricConfig) metricNewrelicmysqlPerformancePerformanceSchemaDigestLost {
	m := metricNewrelicmysqlPerformancePerformanceSchemaDigestLost{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformancePreparedStmtCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.prepared_stmt_count metric with initial data.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) init() {
	m.data.SetName("newrelicmysql.performance.prepared_stmt_count")
	m.data.SetDescription("Current number of prepared statements.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformancePreparedStmtCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformancePreparedStmtCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformancePreparedStmtCount(cfg MetricConfig) metricNewrelicmysqlPerformancePreparedStmtCount {
	m := metricNewrelicmysqlPerformancePreparedStmtCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheFreeBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_free_blocks metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) init() {
	m.data.SetName("newrelicmysql.performance.qcache_free_blocks")
	m.data.SetDescription("The number of free memory blocks in the query cache.")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheFreeBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheFreeBlocks(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheFreeBlocks {
	m := metricNewrelicmysqlPerformanceQcacheFreeBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheFreeMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_free_memory metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) init() {
	m.data.SetName("newrelicmysql.performance.qcache_free_memory")
	m.data.SetDescription("The amount of free memory for the query cache.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheFreeMemory) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheFreeMemory(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheFreeMemory {
	m := metricNewrelicmysqlPerformanceQcacheFreeMemory{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_hits metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheHits) init() {
	m.data.SetName("newrelicmysql.performance.qcache_hits")
	m.data.SetDescription("The number of query cache hits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheHits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheHits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheHits(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheHits {
	m := metricNewrelicmysqlPerformanceQcacheHits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheInserts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_inserts metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) init() {
	m.data.SetName("newrelicmysql.performance.qcache_inserts")
	m.data.SetDescription("The number of queries added to the query cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheInserts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheInserts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheInserts(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheInserts {
	m := metricNewrelicmysqlPerformanceQcacheInserts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheLowmemPrunes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_lowmem_prunes metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) init() {
	m.data.SetName("newrelicmysql.performance.qcache_lowmem_prunes")
	m.data.SetDescription("The number of queries that were deleted from the query cache because of low memory.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheLowmemPrunes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheLowmemPrunes(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheLowmemPrunes {
	m := metricNewrelicmysqlPerformanceQcacheLowmemPrunes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheNotCached struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_not_cached metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) init() {
	m.data.SetName("newrelicmysql.performance.qcache_not_cached")
	m.data.SetDescription("The number of queries that were not cached in the query cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQcacheNotCached) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheNotCached) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheNotCached(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheNotCached {
	m := metricNewrelicmysqlPerformanceQcacheNotCached{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheQueriesInCache struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_queries_in_cache metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) init() {
	m.data.SetName("newrelicmysql.performance.qcache_queries_in_cache")
	m.data.SetDescription("The number of queries registered in the query cache.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheQueriesInCache) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheQueriesInCache(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheQueriesInCache {
	m := metricNewrelicmysqlPerformanceQcacheQueriesInCache{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_size metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheSize) init() {
	m.data.SetName("newrelicmysql.performance.qcache_size")
	m.data.SetDescription("The amount of memory allocated for the query cache.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheSize(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheSize {
	m := metricNewrelicmysqlPerformanceQcacheSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQcacheTotalBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.qcache_total_blocks metric with initial data.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) init() {
	m.data.SetName("newrelicmysql.performance.qcache_total_blocks")
	m.data.SetDescription("The total number of blocks in the query cache.")
	m.data.SetUnit("{blocks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQcacheTotalBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQcacheTotalBlocks(cfg MetricConfig) metricNewrelicmysqlPerformanceQcacheTotalBlocks {
	m := metricNewrelicmysqlPerformanceQcacheTotalBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceQuestions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.questions metric with initial data.
func (m *metricNewrelicmysqlPerformanceQuestions) init() {
	m.data.SetName("newrelicmysql.performance.questions")
	m.data.SetDescription("The number of statements executed by the server (sent by clients).")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceQuestions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceQuestions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceQuestions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceQuestions(cfg MetricConfig) metricNewrelicmysqlPerformanceQuestions {
	m := metricNewrelicmysqlPerformanceQuestions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectFullJoin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_full_join metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) init() {
	m.data.SetName("newrelicmysql.performance.select_full_join")
	m.data.SetDescription("The number of joins that perform table scans because they do not use indexes. If this value is not 0, you should carefully check the indexes of your tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectFullJoin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectFullJoin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectFullJoin(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectFullJoin {
	m := metricNewrelicmysqlPerformanceSelectFullJoin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectFullRangeJoin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_full_range_join metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) init() {
	m.data.SetName("newrelicmysql.performance.select_full_range_join")
	m.data.SetDescription("The number of joins that used a range search on a reference table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectFullRangeJoin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectFullRangeJoin(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectFullRangeJoin {
	m := metricNewrelicmysqlPerformanceSelectFullRangeJoin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectRange struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_range metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectRange) init() {
	m.data.SetName("newrelicmysql.performance.select_range")
	m.data.SetDescription("The number of joins that used ranges on the first table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectRange) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectRange) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectRange) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectRange(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectRange {
	m := metricNewrelicmysqlPerformanceSelectRange{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectRangeCheck struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_range_check metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) init() {
	m.data.SetName("newrelicmysql.performance.select_range_check")
	m.data.SetDescription("The number of joins without keys that check for key usage after each row. If this is not 0, you should carefully check the indexes of your tables.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectRangeCheck) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectRangeCheck(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectRangeCheck {
	m := metricNewrelicmysqlPerformanceSelectRangeCheck{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSelectScan struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.select_scan metric with initial data.
func (m *metricNewrelicmysqlPerformanceSelectScan) init() {
	m.data.SetName("newrelicmysql.performance.select_scan")
	m.data.SetDescription("The number of joins that did a full scan of the first table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSelectScan) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSelectScan) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSelectScan) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSelectScan(cfg MetricConfig) metricNewrelicmysqlPerformanceSelectScan {
	m := metricNewrelicmysqlPerformanceSelectScan{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSlowQueries struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.slow_queries metric with initial data.
func (m *metricNewrelicmysqlPerformanceSlowQueries) init() {
	m.data.SetName("newrelicmysql.performance.slow_queries")
	m.data.SetDescription("The number of queries that have taken more than long_query_time seconds.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSlowQueries) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSlowQueries) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSlowQueries) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSlowQueries(cfg MetricConfig) metricNewrelicmysqlPerformanceSlowQueries {
	m := metricNewrelicmysqlPerformanceSlowQueries{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortMergePasses struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_merge_passes metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) init() {
	m.data.SetName("newrelicmysql.performance.sort_merge_passes")
	m.data.SetDescription("The number of merge passes that the sort algorithm has had to do. If this value is large, you should consider increasing the value of the sort_buffer_size system variable.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortMergePasses) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortMergePasses) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortMergePasses(cfg MetricConfig) metricNewrelicmysqlPerformanceSortMergePasses {
	m := metricNewrelicmysqlPerformanceSortMergePasses{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortRange struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_range metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortRange) init() {
	m.data.SetName("newrelicmysql.performance.sort_range")
	m.data.SetDescription("The number of sorts that were done using ranges.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortRange) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortRange) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortRange) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortRange(cfg MetricConfig) metricNewrelicmysqlPerformanceSortRange {
	m := metricNewrelicmysqlPerformanceSortRange{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_rows metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortRows) init() {
	m.data.SetName("newrelicmysql.performance.sort_rows")
	m.data.SetDescription("The number of sorted rows.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortRows) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortRows(cfg MetricConfig) metricNewrelicmysqlPerformanceSortRows {
	m := metricNewrelicmysqlPerformanceSortRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceSortScan struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.sort_scan metric with initial data.
func (m *metricNewrelicmysqlPerformanceSortScan) init() {
	m.data.SetName("newrelicmysql.performance.sort_scan")
	m.data.SetDescription("The number of sorts that were done by scanning the table.")
	m.data.SetUnit("{operations}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceSortScan) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceSortScan) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceSortScan) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceSortScan(cfg MetricConfig) metricNewrelicmysqlPerformanceSortScan {
	m := metricNewrelicmysqlPerformanceSortScan{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksImmediate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_immediate metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_immediate")
	m.data.SetDescription("The number of times that a request for a table lock could be granted immediately.")
	m.data.SetUnit("{locks}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksImmediate(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksImmediate {
	m := metricNewrelicmysqlPerformanceTableLocksImmediate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksImmediateRate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_immediate.rate metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_immediate.rate")
	m.data.SetDescription("The rate of times that a request for a table lock could be granted immediately.")
	m.data.SetUnit("{locks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksImmediateRate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksImmediateRate(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksImmediateRate {
	m := metricNewrelicmysqlPerformanceTableLocksImmediateRate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableLocksWaited struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_locks_waited metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) init() {
	m.data.SetName("newrelicmysql.performance.table_locks_waited")
	m.data.SetDescription("The total number of times that a request for a table lock could not be granted immediately.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceTableLocksWaited) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableLocksWaited) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableLocksWaited(cfg MetricConfig) metricNewrelicmysqlPerformanceTableLocksWaited {
	m := metricNewrelicmysqlPerformanceTableLocksWaited{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceTableOpenCache struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.table_open_cache metric with initial data.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) init() {
	m.data.SetName("newrelicmysql.performance.table_open_cache")
	m.data.SetDescription("The number of open tables for all threads.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceTableOpenCache) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceTableOpenCache) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceTableOpenCache(cfg MetricConfig) metricNewrelicmysqlPerformanceTableOpenCache {
	m := metricNewrelicmysqlPerformanceTableOpenCache{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadCacheSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.thread_cache_size metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) init() {
	m.data.SetName("newrelicmysql.performance.thread_cache_size")
	m.data.SetDescription("Thread cache size.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadCacheSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadCacheSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadCacheSize(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadCacheSize {
	m := metricNewrelicmysqlPerformanceThreadCacheSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsCached struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_cached metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsCached) init() {
	m.data.SetName("newrelicmysql.performance.threads_cached")
	m.data.SetDescription("The number of threads in the thread cache.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsCached) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsCached) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsCached) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsCached(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsCached {
	m := metricNewrelicmysqlPerformanceThreadsCached{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsConnected struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_connected metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) init() {
	m.data.SetName("newrelicmysql.performance.threads_connected")
	m.data.SetDescription("The number of currently open connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsConnected) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsConnected) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsConnected(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsConnected {
	m := metricNewrelicmysqlPerformanceThreadsConnected{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsCreated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_created metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) init() {
	m.data.SetName("newrelicmysql.performance.threads_created")
	m.data.SetDescription("The number of threads created to handle connections. If this value is large, you may want to increase the thread_cache_size value.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlPerformanceThreadsCreated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsCreated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsCreated(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsCreated {
	m := metricNewrelicmysqlPerformanceThreadsCreated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlPerformanceThreadsRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.performance.threads_running metric with initial data.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) init() {
	m.data.SetName("newrelicmysql.performance.threads_running")
	m.data.SetDescription("The number of threads that are not sleeping.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlPerformanceThreadsRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlPerformanceThreadsRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlPerformanceThreadsRunning(cfg MetricConfig) metricNewrelicmysqlPerformanceThreadsRunning {
	m := metricNewrelicmysqlPerformanceThreadsRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.query.count metric with initial data.
func (m *metricNewrelicmysqlQueryCount) init() {
	m.data.SetName("newrelicmysql.query.count")
	m.data.SetDescription("The number of statements executed by the server.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlQueryCount(cfg MetricConfig) metricNewrelicmysqlQueryCount {
	m := metricNewrelicmysqlQueryCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationExecMasterLogPos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.exec_master_log_pos metric with initial data.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) init() {
	m.data.SetName("newrelicmysql.replication.exec_master_log_pos")
	m.data.SetDescription("The position in the current master binary log file to which the replication SQL thread has read and executed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationExecMasterLogPos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationExecMasterLogPos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationExecMasterLogPos(cfg MetricConfig) metricNewrelicmysqlReplicationExecMasterLogPos {
	m := metricNewrelicmysqlReplicationExecMasterLogPos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationLastIoErrno struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.last_io_errno metric with initial data.
func (m *metricNewrelicmysqlReplicationLastIoErrno) init() {
	m.data.SetName("newrelicmysql.replication.last_io_errno")
	m.data.SetDescription("The error number of the most recent error that caused the replication I/O thread to stop.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationLastIoErrno) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationLastIoErrno) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationLastIoErrno) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationLastIoErrno(cfg MetricConfig) metricNewrelicmysqlReplicationLastIoErrno {
	m := metricNewrelicmysqlReplicationLastIoErrno{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationLastSQLErrno struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.last_sql_errno metric with initial data.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) init() {
	m.data.SetName("newrelicmysql.replication.last_sql_errno")
	m.data.SetDescription("The error number of the most recent error that caused the replication SQL thread to stop.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationLastSQLErrno) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationLastSQLErrno) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationLastSQLErrno(cfg MetricConfig) metricNewrelicmysqlReplicationLastSQLErrno {
	m := metricNewrelicmysqlReplicationLastSQLErrno{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationReadMasterLogPos struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.read_master_log_pos metric with initial data.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) init() {
	m.data.SetName("newrelicmysql.replication.read_master_log_pos")
	m.data.SetDescription("The position in the current master binary log file up to which the replication I/O thread has read.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationReadMasterLogPos) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationReadMasterLogPos) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationReadMasterLogPos(cfg MetricConfig) metricNewrelicmysqlReplicationReadMasterLogPos {
	m := metricNewrelicmysqlReplicationReadMasterLogPos{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationRelayLogSpace struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.relay_log_space metric with initial data.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) init() {
	m.data.SetName("newrelicmysql.replication.relay_log_space")
	m.data.SetDescription("The total combined size of all existing relay log files.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationRelayLogSpace) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationRelayLogSpace) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationRelayLogSpace(cfg MetricConfig) metricNewrelicmysqlReplicationRelayLogSpace {
	m := metricNewrelicmysqlReplicationRelayLogSpace{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSecondsBehindMaster struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.seconds_behind_master metric with initial data.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) init() {
	m.data.SetName("newrelicmysql.replication.seconds_behind_master")
	m.data.SetDescription("The number of seconds that the replica SQL thread is behind processing the master binary log.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSecondsBehindMaster) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSecondsBehindMaster(cfg MetricConfig) metricNewrelicmysqlReplicationSecondsBehindMaster {
	m := metricNewrelicmysqlReplicationSecondsBehindMaster{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveIoRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_io_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_io_running")
	m.data.SetDescription("Status of the replication I/O thread. 0=No/Stopped, 1=Yes/Running, 2=Connecting.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveIoRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveIoRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveIoRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveIoRunning {
	m := metricNewrelicmysqlReplicationSlaveIoRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_running")
	m.data.SetDescription("Whether the replica is currently running (both I/O and SQL threads are running). 1=Both running, 0=One or both stopped.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveRunning {
	m := metricNewrelicmysqlReplicationSlaveRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlReplicationSlaveSQLRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.replication.slave_sql_running metric with initial data.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) init() {
	m.data.SetName("newrelicmysql.replication.slave_sql_running")
	m.data.SetDescription("Status of the replication SQL thread. 0=No/Stopped, 1=Yes/Running.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlReplicationSlaveSQLRunning) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlReplicationSlaveSQLRunning(cfg MetricConfig) metricNewrelicmysqlReplicationSlaveSQLRunning {
	m := metricNewrelicmysqlReplicationSlaveSQLRunning{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricNewrelicmysqlUptime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills newrelicmysql.uptime metric with initial data.
func (m *metricNewrelicmysqlUptime) init() {
	m.data.SetName("newrelicmysql.uptime")
	m.data.SetDescription("The number of seconds that the server has been up.")
	m.data.SetUnit("s")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricNewrelicmysqlUptime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricNewrelicmysqlUptime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricNewrelicmysqlUptime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricNewrelicmysqlUptime(cfg MetricConfig) metricNewrelicmysqlUptime {
	m := metricNewrelicmysqlUptime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                                    MetricsBuilderConfig // config of the metrics builder.
	startTime                                                 pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                           int                  // maximum observed number of metrics per resource.
	metricsBuffer                                             pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                                 component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter                            map[string]filter.Filter
	resourceAttributeExcludeFilter                            map[string]filter.Filter
	metricNewrelicmysqlBinlogCacheDiskUse                     metricNewrelicmysqlBinlogCacheDiskUse
	metricNewrelicmysqlBinlogCacheUse                         metricNewrelicmysqlBinlogCacheUse
	metricNewrelicmysqlCommands                               metricNewrelicmysqlCommands
	metricNewrelicmysqlConnectionCount                        metricNewrelicmysqlConnectionCount
	metricNewrelicmysqlDbHandlerRollback                      metricNewrelicmysqlDbHandlerRollback
	metricNewrelicmysqlDbOpenedTables                         metricNewrelicmysqlDbOpenedTables
	metricNewrelicmysqlInnodbBufferPoolDirty                  metricNewrelicmysqlInnodbBufferPoolDirty
	metricNewrelicmysqlInnodbBufferPoolFree                   metricNewrelicmysqlInnodbBufferPoolFree
	metricNewrelicmysqlInnodbBufferPoolReadRequests           metricNewrelicmysqlInnodbBufferPoolReadRequests
	metricNewrelicmysqlInnodbBufferPoolReads                  metricNewrelicmysqlInnodbBufferPoolReads
	metricNewrelicmysqlInnodbBufferPoolTotal                  metricNewrelicmysqlInnodbBufferPoolTotal
	metricNewrelicmysqlInnodbBufferPoolUsed                   metricNewrelicmysqlInnodbBufferPoolUsed
	metricNewrelicmysqlInnodbBufferPoolUtilization            metricNewrelicmysqlInnodbBufferPoolUtilization
	metricNewrelicmysqlInnodbCurrentRowLocks                  metricNewrelicmysqlInnodbCurrentRowLocks
	metricNewrelicmysqlInnodbDataReads                        metricNewrelicmysqlInnodbDataReads
	metricNewrelicmysqlInnodbDataWrites                       metricNewrelicmysqlInnodbDataWrites
	metricNewrelicmysqlInnodbDataWritten                      metricNewrelicmysqlInnodbDataWritten
	metricNewrelicmysqlInnodbLogWaits                         metricNewrelicmysqlInnodbLogWaits
	metricNewrelicmysqlInnodbMutexOsWaits                     metricNewrelicmysqlInnodbMutexOsWaits
	metricNewrelicmysqlInnodbMutexSpinRounds                  metricNewrelicmysqlInnodbMutexSpinRounds
	metricNewrelicmysqlInnodbMutexSpinWaits                   metricNewrelicmysqlInnodbMutexSpinWaits
	metricNewrelicmysqlInnodbOsLogFsyncs                      metricNewrelicmysqlInnodbOsLogFsyncs
	metricNewrelicmysqlInnodbRowLockCurrentWaits              metricNewrelicmysqlInnodbRowLockCurrentWaits
	metricNewrelicmysqlInnodbRowLockTime                      metricNewrelicmysqlInnodbRowLockTime
	metricNewrelicmysqlInnodbRowLockWaits                     metricNewrelicmysqlInnodbRowLockWaits
	metricNewrelicmysqlMyisamKeyBufferBytesUnflushed          metricNewrelicmysqlMyisamKeyBufferBytesUnflushed
	metricNewrelicmysqlMyisamKeyBufferBytesUsed               metricNewrelicmysqlMyisamKeyBufferBytesUsed
	metricNewrelicmysqlMyisamKeyBufferSize                    metricNewrelicmysqlMyisamKeyBufferSize
	metricNewrelicmysqlMyisamKeyReadRequests                  metricNewrelicmysqlMyisamKeyReadRequests
	metricNewrelicmysqlMyisamKeyReads                         metricNewrelicmysqlMyisamKeyReads
	metricNewrelicmysqlMyisamKeyWriteRequests                 metricNewrelicmysqlMyisamKeyWriteRequests
	metricNewrelicmysqlMyisamKeyWrites                        metricNewrelicmysqlMyisamKeyWrites
	metricNewrelicmysqlNetAbortedClients                      metricNewrelicmysqlNetAbortedClients
	metricNewrelicmysqlNetAbortedConnects                     metricNewrelicmysqlNetAbortedConnects
	metricNewrelicmysqlNetConnections                         metricNewrelicmysqlNetConnections
	metricNewrelicmysqlNetMaxConnections                      metricNewrelicmysqlNetMaxConnections
	metricNewrelicmysqlNetMaxConnectionsAvailable             metricNewrelicmysqlNetMaxConnectionsAvailable
	metricNewrelicmysqlNetMaxUsedConnections                  metricNewrelicmysqlNetMaxUsedConnections
	metricNewrelicmysqlPerformanceBytesReceived               metricNewrelicmysqlPerformanceBytesReceived
	metricNewrelicmysqlPerformanceBytesSent                   metricNewrelicmysqlPerformanceBytesSent
	metricNewrelicmysqlPerformanceCreatedTmpDiskTables        metricNewrelicmysqlPerformanceCreatedTmpDiskTables
	metricNewrelicmysqlPerformanceCreatedTmpFiles             metricNewrelicmysqlPerformanceCreatedTmpFiles
	metricNewrelicmysqlPerformanceCreatedTmpTables            metricNewrelicmysqlPerformanceCreatedTmpTables
	metricNewrelicmysqlPerformanceHandlerCommit               metricNewrelicmysqlPerformanceHandlerCommit
	metricNewrelicmysqlPerformanceHandlerDelete               metricNewrelicmysqlPerformanceHandlerDelete
	metricNewrelicmysqlPerformanceHandlerPrepare              metricNewrelicmysqlPerformanceHandlerPrepare
	metricNewrelicmysqlPerformanceHandlerReadFirst            metricNewrelicmysqlPerformanceHandlerReadFirst
	metricNewrelicmysqlPerformanceHandlerReadKey              metricNewrelicmysqlPerformanceHandlerReadKey
	metricNewrelicmysqlPerformanceHandlerReadNext             metricNewrelicmysqlPerformanceHandlerReadNext
	metricNewrelicmysqlPerformanceHandlerReadPrev             metricNewrelicmysqlPerformanceHandlerReadPrev
	metricNewrelicmysqlPerformanceHandlerReadRnd              metricNewrelicmysqlPerformanceHandlerReadRnd
	metricNewrelicmysqlPerformanceHandlerReadRndNext          metricNewrelicmysqlPerformanceHandlerReadRndNext
	metricNewrelicmysqlPerformanceHandlerRollback             metricNewrelicmysqlPerformanceHandlerRollback
	metricNewrelicmysqlPerformanceHandlerUpdate               metricNewrelicmysqlPerformanceHandlerUpdate
	metricNewrelicmysqlPerformanceHandlerWrite                metricNewrelicmysqlPerformanceHandlerWrite
	metricNewrelicmysqlPerformanceKeyCacheUtilization         metricNewrelicmysqlPerformanceKeyCacheUtilization
	metricNewrelicmysqlPerformanceMaxPreparedStmtCount        metricNewrelicmysqlPerformanceMaxPreparedStmtCount
	metricNewrelicmysqlPerformanceOpenFiles                   metricNewrelicmysqlPerformanceOpenFiles
	metricNewrelicmysqlPerformanceOpenTables                  metricNewrelicmysqlPerformanceOpenTables
	metricNewrelicmysqlPerformanceOpenedTables                metricNewrelicmysqlPerformanceOpenedTables
	metricNewrelicmysqlPerformancePerformanceSchemaDigestLost metricNewrelicmysqlPerformancePerformanceSchemaDigestLost
	metricNewrelicmysqlPerformancePreparedStmtCount           metricNewrelicmysqlPerformancePreparedStmtCount
	metricNewrelicmysqlPerformanceQcacheFreeBlocks            metricNewrelicmysqlPerformanceQcacheFreeBlocks
	metricNewrelicmysqlPerformanceQcacheFreeMemory            metricNewrelicmysqlPerformanceQcacheFreeMemory
	metricNewrelicmysqlPerformanceQcacheHits                  metricNewrelicmysqlPerformanceQcacheHits
	metricNewrelicmysqlPerformanceQcacheInserts               metricNewrelicmysqlPerformanceQcacheInserts
	metricNewrelicmysqlPerformanceQcacheLowmemPrunes          metricNewrelicmysqlPerformanceQcacheLowmemPrunes
	metricNewrelicmysqlPerformanceQcacheNotCached             metricNewrelicmysqlPerformanceQcacheNotCached
	metricNewrelicmysqlPerformanceQcacheQueriesInCache        metricNewrelicmysqlPerformanceQcacheQueriesInCache
	metricNewrelicmysqlPerformanceQcacheSize                  metricNewrelicmysqlPerformanceQcacheSize
	metricNewrelicmysqlPerformanceQcacheTotalBlocks           metricNewrelicmysqlPerformanceQcacheTotalBlocks
	metricNewrelicmysqlPerformanceQuestions                   metricNewrelicmysqlPerformanceQuestions
	metricNewrelicmysqlPerformanceSelectFullJoin              metricNewrelicmysqlPerformanceSelectFullJoin
	metricNewrelicmysqlPerformanceSelectFullRangeJoin         metricNewrelicmysqlPerformanceSelectFullRangeJoin
	metricNewrelicmysqlPerformanceSelectRange                 metricNewrelicmysqlPerformanceSelectRange
	metricNewrelicmysqlPerformanceSelectRangeCheck            metricNewrelicmysqlPerformanceSelectRangeCheck
	metricNewrelicmysqlPerformanceSelectScan                  metricNewrelicmysqlPerformanceSelectScan
	metricNewrelicmysqlPerformanceSlowQueries                 metricNewrelicmysqlPerformanceSlowQueries
	metricNewrelicmysqlPerformanceSortMergePasses             metricNewrelicmysqlPerformanceSortMergePasses
	metricNewrelicmysqlPerformanceSortRange                   metricNewrelicmysqlPerformanceSortRange
	metricNewrelicmysqlPerformanceSortRows                    metricNewrelicmysqlPerformanceSortRows
	metricNewrelicmysqlPerformanceSortScan                    metricNewrelicmysqlPerformanceSortScan
	metricNewrelicmysqlPerformanceTableLocksImmediate         metricNewrelicmysqlPerformanceTableLocksImmediate
	metricNewrelicmysqlPerformanceTableLocksImmediateRate     metricNewrelicmysqlPerformanceTableLocksImmediateRate
	metricNewrelicmysqlPerformanceTableLocksWaited            metricNewrelicmysqlPerformanceTableLocksWaited
	metricNewrelicmysqlPerformanceTableOpenCache              metricNewrelicmysqlPerformanceTableOpenCache
	metricNewrelicmysqlPerformanceThreadCacheSize             metricNewrelicmysqlPerformanceThreadCacheSize
	metricNewrelicmysqlPerformanceThreadsCached               metricNewrelicmysqlPerformanceThreadsCached
	metricNewrelicmysqlPerformanceThreadsConnected            metricNewrelicmysqlPerformanceThreadsConnected
	metricNewrelicmysqlPerformanceThreadsCreated              metricNewrelicmysqlPerformanceThreadsCreated
	metricNewrelicmysqlPerformanceThreadsRunning              metricNewrelicmysqlPerformanceThreadsRunning
	metricNewrelicmysqlQueryCount                             metricNewrelicmysqlQueryCount
	metricNewrelicmysqlReplicationExecMasterLogPos            metricNewrelicmysqlReplicationExecMasterLogPos
	metricNewrelicmysqlReplicationLastIoErrno                 metricNewrelicmysqlReplicationLastIoErrno
	metricNewrelicmysqlReplicationLastSQLErrno                metricNewrelicmysqlReplicationLastSQLErrno
	metricNewrelicmysqlReplicationReadMasterLogPos            metricNewrelicmysqlReplicationReadMasterLogPos
	metricNewrelicmysqlReplicationRelayLogSpace               metricNewrelicmysqlReplicationRelayLogSpace
	metricNewrelicmysqlReplicationSecondsBehindMaster         metricNewrelicmysqlReplicationSecondsBehindMaster
	metricNewrelicmysqlReplicationSlaveIoRunning              metricNewrelicmysqlReplicationSlaveIoRunning
	metricNewrelicmysqlReplicationSlaveRunning                metricNewrelicmysqlReplicationSlaveRunning
	metricNewrelicmysqlReplicationSlaveSQLRunning             metricNewrelicmysqlReplicationSlaveSQLRunning
	metricNewrelicmysqlUptime                                 metricNewrelicmysqlUptime
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                                    mbc,
		startTime:                                                 pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                             pmetric.NewMetrics(),
		buildInfo:                                                 settings.BuildInfo,
		metricNewrelicmysqlBinlogCacheDiskUse:                     newMetricNewrelicmysqlBinlogCacheDiskUse(mbc.Metrics.NewrelicmysqlBinlogCacheDiskUse),
		metricNewrelicmysqlBinlogCacheUse:                         newMetricNewrelicmysqlBinlogCacheUse(mbc.Metrics.NewrelicmysqlBinlogCacheUse),
		metricNewrelicmysqlCommands:                               newMetricNewrelicmysqlCommands(mbc.Metrics.NewrelicmysqlCommands),
		metricNewrelicmysqlConnectionCount:                        newMetricNewrelicmysqlConnectionCount(mbc.Metrics.NewrelicmysqlConnectionCount),
		metricNewrelicmysqlDbHandlerRollback:                      newMetricNewrelicmysqlDbHandlerRollback(mbc.Metrics.NewrelicmysqlDbHandlerRollback),
		metricNewrelicmysqlDbOpenedTables:                         newMetricNewrelicmysqlDbOpenedTables(mbc.Metrics.NewrelicmysqlDbOpenedTables),
		metricNewrelicmysqlInnodbBufferPoolDirty:                  newMetricNewrelicmysqlInnodbBufferPoolDirty(mbc.Metrics.NewrelicmysqlInnodbBufferPoolDirty),
		metricNewrelicmysqlInnodbBufferPoolFree:                   newMetricNewrelicmysqlInnodbBufferPoolFree(mbc.Metrics.NewrelicmysqlInnodbBufferPoolFree),
		metricNewrelicmysqlInnodbBufferPoolReadRequests:           newMetricNewrelicmysqlInnodbBufferPoolReadRequests(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReadRequests),
		metricNewrelicmysqlInnodbBufferPoolReads:                  newMetricNewrelicmysqlInnodbBufferPoolReads(mbc.Metrics.NewrelicmysqlInnodbBufferPoolReads),
		metricNewrelicmysqlInnodbBufferPoolTotal:                  newMetricNewrelicmysqlInnodbBufferPoolTotal(mbc.Metrics.NewrelicmysqlInnodbBufferPoolTotal),
		metricNewrelicmysqlInnodbBufferPoolUsed:                   newMetricNewrelicmysqlInnodbBufferPoolUsed(mbc.Metrics.NewrelicmysqlInnodbBufferPoolUsed),
		metricNewrelicmysqlInnodbBufferPoolUtilization:            newMetricNewrelicmysqlInnodbBufferPoolUtilization(mbc.Metrics.NewrelicmysqlInnodbBufferPoolUtilization),
		metricNewrelicmysqlInnodbCurrentRowLocks:                  newMetricNewrelicmysqlInnodbCurrentRowLocks(mbc.Metrics.NewrelicmysqlInnodbCurrentRowLocks),
		metricNewrelicmysqlInnodbDataReads:                        newMetricNewrelicmysqlInnodbDataReads(mbc.Metrics.NewrelicmysqlInnodbDataReads),
		metricNewrelicmysqlInnodbDataWrites:                       newMetricNewrelicmysqlInnodbDataWrites(mbc.Metrics.NewrelicmysqlInnodbDataWrites),
		metricNewrelicmysqlInnodbDataWritten:                      newMetricNewrelicmysqlInnodbDataWritten(mbc.Metrics.NewrelicmysqlInnodbDataWritten),
		metricNewrelicmysqlInnodbLogWaits:                         newMetricNewrelicmysqlInnodbLogWaits(mbc.Metrics.NewrelicmysqlInnodbLogWaits),
		metricNewrelicmysqlInnodbMutexOsWaits:                     newMetricNewrelicmysqlInnodbMutexOsWaits(mbc.Metrics.NewrelicmysqlInnodbMutexOsWaits),
		metricNewrelicmysqlInnodbMutexSpinRounds:                  newMetricNewrelicmysqlInnodbMutexSpinRounds(mbc.Metrics.NewrelicmysqlInnodbMutexSpinRounds),
		metricNewrelicmysqlInnodbMutexSpinWaits:                   newMetricNewrelicmysqlInnodbMutexSpinWaits(mbc.Metrics.NewrelicmysqlInnodbMutexSpinWaits),
		metricNewrelicmysqlInnodbOsLogFsyncs:                      newMetricNewrelicmysqlInnodbOsLogFsyncs(mbc.Metrics.NewrelicmysqlInnodbOsLogFsyncs),
		metricNewrelicmysqlInnodbRowLockCurrentWaits:              newMetricNewrelicmysqlInnodbRowLockCurrentWaits(mbc.Metrics.NewrelicmysqlInnodbRowLockCurrentWaits),
		metricNewrelicmysqlInnodbRowLockTime:                      newMetricNewrelicmysqlInnodbRowLockTime(mbc.Metrics.NewrelicmysqlInnodbRowLockTime),
		metricNewrelicmysqlInnodbRowLockWaits:                     newMetricNewrelicmysqlInnodbRowLockWaits(mbc.Metrics.NewrelicmysqlInnodbRowLockWaits),
		metricNewrelicmysqlMyisamKeyBufferBytesUnflushed:          newMetricNewrelicmysqlMyisamKeyBufferBytesUnflushed(mbc.Metrics.NewrelicmysqlMyisamKeyBufferBytesUnflushed),
		metricNewrelicmysqlMyisamKeyBufferBytesUsed:               newMetricNewrelicmysqlMyisamKeyBufferBytesUsed(mbc.Metrics.NewrelicmysqlMyisamKeyBufferBytesUsed),
		metricNewrelicmysqlMyisamKeyBufferSize:                    newMetricNewrelicmysqlMyisamKeyBufferSize(mbc.Metrics.NewrelicmysqlMyisamKeyBufferSize),
		metricNewrelicmysqlMyisamKeyReadRequests:                  newMetricNewrelicmysqlMyisamKeyReadRequests(mbc.Metrics.NewrelicmysqlMyisamKeyReadRequests),
		metricNewrelicmysqlMyisamKeyReads:                         newMetricNewrelicmysqlMyisamKeyReads(mbc.Metrics.NewrelicmysqlMyisamKeyReads),
		metricNewrelicmysqlMyisamKeyWriteRequests:                 newMetricNewrelicmysqlMyisamKeyWriteRequests(mbc.Metrics.NewrelicmysqlMyisamKeyWriteRequests),
		metricNewrelicmysqlMyisamKeyWrites:                        newMetricNewrelicmysqlMyisamKeyWrites(mbc.Metrics.NewrelicmysqlMyisamKeyWrites),
		metricNewrelicmysqlNetAbortedClients:                      newMetricNewrelicmysqlNetAbortedClients(mbc.Metrics.NewrelicmysqlNetAbortedClients),
		metricNewrelicmysqlNetAbortedConnects:                     newMetricNewrelicmysqlNetAbortedConnects(mbc.Metrics.NewrelicmysqlNetAbortedConnects),
		metricNewrelicmysqlNetConnections:                         newMetricNewrelicmysqlNetConnections(mbc.Metrics.NewrelicmysqlNetConnections),
		metricNewrelicmysqlNetMaxConnections:                      newMetricNewrelicmysqlNetMaxConnections(mbc.Metrics.NewrelicmysqlNetMaxConnections),
		metricNewrelicmysqlNetMaxConnectionsAvailable:             newMetricNewrelicmysqlNetMaxConnectionsAvailable(mbc.Metrics.NewrelicmysqlNetMaxConnectionsAvailable),
		metricNewrelicmysqlNetMaxUsedConnections:                  newMetricNewrelicmysqlNetMaxUsedConnections(mbc.Metrics.NewrelicmysqlNetMaxUsedConnections),
		metricNewrelicmysqlPerformanceBytesReceived:               newMetricNewrelicmysqlPerformanceBytesReceived(mbc.Metrics.NewrelicmysqlPerformanceBytesReceived),
		metricNewrelicmysqlPerformanceBytesSent:                   newMetricNewrelicmysqlPerformanceBytesSent(mbc.Metrics.NewrelicmysqlPerformanceBytesSent),
		metricNewrelicmysqlPerformanceCreatedTmpDiskTables:        newMetricNewrelicmysqlPerformanceCreatedTmpDiskTables(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpDiskTables),
		metricNewrelicmysqlPerformanceCreatedTmpFiles:             newMetricNewrelicmysqlPerformanceCreatedTmpFiles(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpFiles),
		metricNewrelicmysqlPerformanceCreatedTmpTables:            newMetricNewrelicmysqlPerformanceCreatedTmpTables(mbc.Metrics.NewrelicmysqlPerformanceCreatedTmpTables),
		metricNewrelicmysqlPerformanceHandlerCommit:               newMetricNewrelicmysqlPerformanceHandlerCommit(mbc.Metrics.NewrelicmysqlPerformanceHandlerCommit),
		metricNewrelicmysqlPerformanceHandlerDelete:               newMetricNewrelicmysqlPerformanceHandlerDelete(mbc.Metrics.NewrelicmysqlPerformanceHandlerDelete),
		metricNewrelicmysqlPerformanceHandlerPrepare:              newMetricNewrelicmysqlPerformanceHandlerPrepare(mbc.Metrics.NewrelicmysqlPerformanceHandlerPrepare),
		metricNewrelicmysqlPerformanceHandlerReadFirst:            newMetricNewrelicmysqlPerformanceHandlerReadFirst(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadFirst),
		metricNewrelicmysqlPerformanceHandlerReadKey:              newMetricNewrelicmysqlPerformanceHandlerReadKey(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadKey),
		metricNewrelicmysqlPerformanceHandlerReadNext:             newMetricNewrelicmysqlPerformanceHandlerReadNext(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadNext),
		metricNewrelicmysqlPerformanceHandlerReadPrev:             newMetricNewrelicmysqlPerformanceHandlerReadPrev(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadPrev),
		metricNewrelicmysqlPerformanceHandlerReadRnd:              newMetricNewrelicmysqlPerformanceHandlerReadRnd(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadRnd),
		metricNewrelicmysqlPerformanceHandlerReadRndNext:          newMetricNewrelicmysqlPerformanceHandlerReadRndNext(mbc.Metrics.NewrelicmysqlPerformanceHandlerReadRndNext),
		metricNewrelicmysqlPerformanceHandlerRollback:             newMetricNewrelicmysqlPerformanceHandlerRollback(mbc.Metrics.NewrelicmysqlPerformanceHandlerRollback),
		metricNewrelicmysqlPerformanceHandlerUpdate:               newMetricNewrelicmysqlPerformanceHandlerUpdate(mbc.Metrics.NewrelicmysqlPerformanceHandlerUpdate),
		metricNewrelicmysqlPerformanceHandlerWrite:                newMetricNewrelicmysqlPerformanceHandlerWrite(mbc.Metrics.NewrelicmysqlPerformanceHandlerWrite),
		metricNewrelicmysqlPerformanceKeyCacheUtilization:         newMetricNewrelicmysqlPerformanceKeyCacheUtilization(mbc.Metrics.NewrelicmysqlPerformanceKeyCacheUtilization),
		metricNewrelicmysqlPerformanceMaxPreparedStmtCount:        newMetricNewrelicmysqlPerformanceMaxPreparedStmtCount(mbc.Metrics.NewrelicmysqlPerformanceMaxPreparedStmtCount),
		metricNewrelicmysqlPerformanceOpenFiles:                   newMetricNewrelicmysqlPerformanceOpenFiles(mbc.Metrics.NewrelicmysqlPerformanceOpenFiles),
		metricNewrelicmysqlPerformanceOpenTables:                  newMetricNewrelicmysqlPerformanceOpenTables(mbc.Metrics.NewrelicmysqlPerformanceOpenTables),
		metricNewrelicmysqlPerformanceOpenedTables:                newMetricNewrelicmysqlPerformanceOpenedTables(mbc.Metrics.NewrelicmysqlPerformanceOpenedTables),
		metricNewrelicmysqlPerformancePerformanceSchemaDigestLost: newMetricNewrelicmysqlPerformancePerformanceSchemaDigestLost(mbc.Metrics.NewrelicmysqlPerformancePerformanceSchemaDigestLost),
		metricNewrelicmysqlPerformancePreparedStmtCount:           newMetricNewrelicmysqlPerformancePreparedStmtCount(mbc.Metrics.NewrelicmysqlPerformancePreparedStmtCount),
		metricNewrelicmysqlPerformanceQcacheFreeBlocks:            newMetricNewrelicmysqlPerformanceQcacheFreeBlocks(mbc.Metrics.NewrelicmysqlPerformanceQcacheFreeBlocks),
		metricNewrelicmysqlPerformanceQcacheFreeMemory:            newMetricNewrelicmysqlPerformanceQcacheFreeMemory(mbc.Metrics.NewrelicmysqlPerformanceQcacheFreeMemory),
		metricNewrelicmysqlPerformanceQcacheHits:                  newMetricNewrelicmysqlPerformanceQcacheHits(mbc.Metrics.NewrelicmysqlPerformanceQcacheHits),
		metricNewrelicmysqlPerformanceQcacheInserts:               newMetricNewrelicmysqlPerformanceQcacheInserts(mbc.Metrics.NewrelicmysqlPerformanceQcacheInserts),
		metricNewrelicmysqlPerformanceQcacheLowmemPrunes:          newMetricNewrelicmysqlPerformanceQcacheLowmemPrunes(mbc.Metrics.NewrelicmysqlPerformanceQcacheLowmemPrunes),
		metricNewrelicmysqlPerformanceQcacheNotCached:             newMetricNewrelicmysqlPerformanceQcacheNotCached(mbc.Metrics.NewrelicmysqlPerformanceQcacheNotCached),
		metricNewrelicmysqlPerformanceQcacheQueriesInCache:        newMetricNewrelicmysqlPerformanceQcacheQueriesInCache(mbc.Metrics.NewrelicmysqlPerformanceQcacheQueriesInCache),
		metricNewrelicmysqlPerformanceQcacheSize:                  newMetricNewrelicmysqlPerformanceQcacheSize(mbc.Metrics.NewrelicmysqlPerformanceQcacheSize),
		metricNewrelicmysqlPerformanceQcacheTotalBlocks:           newMetricNewrelicmysqlPerformanceQcacheTotalBlocks(mbc.Metrics.NewrelicmysqlPerformanceQcacheTotalBlocks),
		metricNewrelicmysqlPerformanceQuestions:                   newMetricNewrelicmysqlPerformanceQuestions(mbc.Metrics.NewrelicmysqlPerformanceQuestions),
		metricNewrelicmysqlPerformanceSelectFullJoin:              newMetricNewrelicmysqlPerformanceSelectFullJoin(mbc.Metrics.NewrelicmysqlPerformanceSelectFullJoin),
		metricNewrelicmysqlPerformanceSelectFullRangeJoin:         newMetricNewrelicmysqlPerformanceSelectFullRangeJoin(mbc.Metrics.NewrelicmysqlPerformanceSelectFullRangeJoin),
		metricNewrelicmysqlPerformanceSelectRange:                 newMetricNewrelicmysqlPerformanceSelectRange(mbc.Metrics.NewrelicmysqlPerformanceSelectRange),
		metricNewrelicmysqlPerformanceSelectRangeCheck:            newMetricNewrelicmysqlPerformanceSelectRangeCheck(mbc.Metrics.NewrelicmysqlPerformanceSelectRangeCheck),
		metricNewrelicmysqlPerformanceSelectScan:                  newMetricNewrelicmysqlPerformanceSelectScan(mbc.Metrics.NewrelicmysqlPerformanceSelectScan),
		metricNewrelicmysqlPerformanceSlowQueries:                 newMetricNewrelicmysqlPerformanceSlowQueries(mbc.Metrics.NewrelicmysqlPerformanceSlowQueries),
		metricNewrelicmysqlPerformanceSortMergePasses:             newMetricNewrelicmysqlPerformanceSortMergePasses(mbc.Metrics.NewrelicmysqlPerformanceSortMergePasses),
		metricNewrelicmysqlPerformanceSortRange:                   newMetricNewrelicmysqlPerformanceSortRange(mbc.Metrics.NewrelicmysqlPerformanceSortRange),
		metricNewrelicmysqlPerformanceSortRows:                    newMetricNewrelicmysqlPerformanceSortRows(mbc.Metrics.NewrelicmysqlPerformanceSortRows),
		metricNewrelicmysqlPerformanceSortScan:                    newMetricNewrelicmysqlPerformanceSortScan(mbc.Metrics.NewrelicmysqlPerformanceSortScan),
		metricNewrelicmysqlPerformanceTableLocksImmediate:         newMetricNewrelicmysqlPerformanceTableLocksImmediate(mbc.Metrics.NewrelicmysqlPerformanceTableLocksImmediate),
		metricNewrelicmysqlPerformanceTableLocksImmediateRate:     newMetricNewrelicmysqlPerformanceTableLocksImmediateRate(mbc.Metrics.NewrelicmysqlPerformanceTableLocksImmediateRate),
		metricNewrelicmysqlPerformanceTableLocksWaited:            newMetricNewrelicmysqlPerformanceTableLocksWaited(mbc.Metrics.NewrelicmysqlPerformanceTableLocksWaited),
		metricNewrelicmysqlPerformanceTableOpenCache:              newMetricNewrelicmysqlPerformanceTableOpenCache(mbc.Metrics.NewrelicmysqlPerformanceTableOpenCache),
		metricNewrelicmysqlPerformanceThreadCacheSize:             newMetricNewrelicmysqlPerformanceThreadCacheSize(mbc.Metrics.NewrelicmysqlPerformanceThreadCacheSize),
		metricNewrelicmysqlPerformanceThreadsCached:               newMetricNewrelicmysqlPerformanceThreadsCached(mbc.Metrics.NewrelicmysqlPerformanceThreadsCached),
		metricNewrelicmysqlPerformanceThreadsConnected:            newMetricNewrelicmysqlPerformanceThreadsConnected(mbc.Metrics.NewrelicmysqlPerformanceThreadsConnected),
		metricNewrelicmysqlPerformanceThreadsCreated:              newMetricNewrelicmysqlPerformanceThreadsCreated(mbc.Metrics.NewrelicmysqlPerformanceThreadsCreated),
		metricNewrelicmysqlPerformanceThreadsRunning:              newMetricNewrelicmysqlPerformanceThreadsRunning(mbc.Metrics.NewrelicmysqlPerformanceThreadsRunning),
		metricNewrelicmysqlQueryCount:                             newMetricNewrelicmysqlQueryCount(mbc.Metrics.NewrelicmysqlQueryCount),
		metricNewrelicmysqlReplicationExecMasterLogPos:            newMetricNewrelicmysqlReplicationExecMasterLogPos(mbc.Metrics.NewrelicmysqlReplicationExecMasterLogPos),
		metricNewrelicmysqlReplicationLastIoErrno:                 newMetricNewrelicmysqlReplicationLastIoErrno(mbc.Metrics.NewrelicmysqlReplicationLastIoErrno),
		metricNewrelicmysqlReplicationLastSQLErrno:                newMetricNewrelicmysqlReplicationLastSQLErrno(mbc.Metrics.NewrelicmysqlReplicationLastSQLErrno),
		metricNewrelicmysqlReplicationReadMasterLogPos:            newMetricNewrelicmysqlReplicationReadMasterLogPos(mbc.Metrics.NewrelicmysqlReplicationReadMasterLogPos),
		metricNewrelicmysqlReplicationRelayLogSpace:               newMetricNewrelicmysqlReplicationRelayLogSpace(mbc.Metrics.NewrelicmysqlReplicationRelayLogSpace),
		metricNewrelicmysqlReplicationSecondsBehindMaster:         newMetricNewrelicmysqlReplicationSecondsBehindMaster(mbc.Metrics.NewrelicmysqlReplicationSecondsBehindMaster),
		metricNewrelicmysqlReplicationSlaveIoRunning:              newMetricNewrelicmysqlReplicationSlaveIoRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveIoRunning),
		metricNewrelicmysqlReplicationSlaveRunning:                newMetricNewrelicmysqlReplicationSlaveRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveRunning),
		metricNewrelicmysqlReplicationSlaveSQLRunning:             newMetricNewrelicmysqlReplicationSlaveSQLRunning(mbc.Metrics.NewrelicmysqlReplicationSlaveSQLRunning),
		metricNewrelicmysqlUptime:                                 newMetricNewrelicmysqlUptime(mbc.Metrics.NewrelicmysqlUptime),
		resourceAttributeIncludeFilter:                            make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:                            make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["newrelicmysql.instance.endpoint"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsInclude)
	}
	if mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["newrelicmysql.instance.endpoint"] = filter.CreateFilter(mbc.ResourceAttributes.NewrelicmysqlInstanceEndpoint.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricNewrelicmysqlBinlogCacheDiskUse.emit(ils.Metrics())
	mb.metricNewrelicmysqlBinlogCacheUse.emit(ils.Metrics())
	mb.metricNewrelicmysqlCommands.emit(ils.Metrics())
	mb.metricNewrelicmysqlConnectionCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlDbHandlerRollback.emit(ils.Metrics())
	mb.metricNewrelicmysqlDbOpenedTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolDirty.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolFree.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReadRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolTotal.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolUsed.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbBufferPoolUtilization.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbCurrentRowLocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbDataWritten.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbLogWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexOsWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexSpinRounds.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbMutexSpinWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbOsLogFsyncs.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockCurrentWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockTime.emit(ils.Metrics())
	mb.metricNewrelicmysqlInnodbRowLockWaits.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUnflushed.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUsed.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyBufferSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyReadRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyReads.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyWriteRequests.emit(ils.Metrics())
	mb.metricNewrelicmysqlMyisamKeyWrites.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetAbortedClients.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetAbortedConnects.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxConnectionsAvailable.emit(ils.Metrics())
	mb.metricNewrelicmysqlNetMaxUsedConnections.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceBytesReceived.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceBytesSent.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpDiskTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpFiles.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceCreatedTmpTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerCommit.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerDelete.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerPrepare.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadFirst.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadKey.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadNext.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadPrev.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadRnd.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerReadRndNext.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerRollback.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerUpdate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceHandlerWrite.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceKeyCacheUtilization.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceMaxPreparedStmtCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenFiles.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceOpenedTables.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformancePerformanceSchemaDigestLost.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformancePreparedStmtCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheFreeBlocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheFreeMemory.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheHits.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheInserts.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheLowmemPrunes.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheNotCached.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheQueriesInCache.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQcacheTotalBlocks.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceQuestions.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectFullJoin.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectFullRangeJoin.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectRange.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectRangeCheck.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSelectScan.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSlowQueries.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortMergePasses.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortRange.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortRows.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceSortScan.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksImmediate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksImmediateRate.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableLocksWaited.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceTableOpenCache.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadCacheSize.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsCached.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsConnected.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsCreated.emit(ils.Metrics())
	mb.metricNewrelicmysqlPerformanceThreadsRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlQueryCount.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationExecMasterLogPos.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationLastIoErrno.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationLastSQLErrno.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationReadMasterLogPos.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationRelayLogSpace.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSecondsBehindMaster.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveIoRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlReplicationSlaveSQLRunning.emit(ils.Metrics())
	mb.metricNewrelicmysqlUptime.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordNewrelicmysqlBinlogCacheDiskUseDataPoint adds a data point to newrelicmysql.binlog.cache_disk_use metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlBinlogCacheDiskUseDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlBinlogCacheDiskUse.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlBinlogCacheUseDataPoint adds a data point to newrelicmysql.binlog.cache_use metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlBinlogCacheUseDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlBinlogCacheUse.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlCommandsDataPoint adds a data point to newrelicmysql.commands metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlCommandsDataPoint(ts pcommon.Timestamp, inputVal string, commandAttributeValue AttributeCommand) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlCommands, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlCommands.recordDataPoint(mb.startTime, ts, val, commandAttributeValue.String())
	return nil
}

// RecordNewrelicmysqlConnectionCountDataPoint adds a data point to newrelicmysql.connection.count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlConnectionCountDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlConnectionCount, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlConnectionCount.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// RecordNewrelicmysqlDbHandlerRollbackDataPoint adds a data point to newrelicmysql.db.handler_rollback metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlDbHandlerRollbackDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlDbHandlerRollback.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlDbOpenedTablesDataPoint adds a data point to newrelicmysql.db.opened_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlDbOpenedTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlDbOpenedTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolDirtyDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_dirty metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolDirtyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolDirty.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolFreeDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_free metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadRequestsDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_read_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReadRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolReadsDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolTotalDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_total metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolUsedDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_used metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbBufferPoolUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbBufferPoolUtilizationDataPoint adds a data point to newrelicmysql.innodb.buffer_pool_utilization metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbBufferPoolUtilizationDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlInnodbBufferPoolUtilization.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbCurrentRowLocksDataPoint adds a data point to newrelicmysql.innodb.current_row_locks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbCurrentRowLocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbCurrentRowLocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataReadsDataPoint adds a data point to newrelicmysql.innodb.data_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataWritesDataPoint adds a data point to newrelicmysql.innodb.data_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbDataWrittenDataPoint adds a data point to newrelicmysql.innodb.data_written metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbDataWrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbDataWritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbLogWaitsDataPoint adds a data point to newrelicmysql.innodb.log_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbLogWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbLogWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexOsWaitsDataPoint adds a data point to newrelicmysql.innodb.mutex_os_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexOsWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexOsWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexSpinRoundsDataPoint adds a data point to newrelicmysql.innodb.mutex_spin_rounds metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexSpinRoundsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexSpinRounds.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbMutexSpinWaitsDataPoint adds a data point to newrelicmysql.innodb.mutex_spin_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbMutexSpinWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbMutexSpinWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbOsLogFsyncsDataPoint adds a data point to newrelicmysql.innodb.os_log_fsyncs metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbOsLogFsyncsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbOsLogFsyncs.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockCurrentWaitsDataPoint adds a data point to newrelicmysql.innodb.row_lock_current_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockCurrentWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockCurrentWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockTimeDataPoint adds a data point to newrelicmysql.innodb.row_lock_time metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlInnodbRowLockWaitsDataPoint adds a data point to newrelicmysql.innodb.row_lock_waits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlInnodbRowLockWaitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlInnodbRowLockWaits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferBytesUnflushedDataPoint adds a data point to newrelicmysql.myisam.key_buffer_bytes_unflushed metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferBytesUnflushedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUnflushed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferBytesUsedDataPoint adds a data point to newrelicmysql.myisam.key_buffer_bytes_used metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferBytesUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferBytesUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyBufferSizeDataPoint adds a data point to newrelicmysql.myisam.key_buffer_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyBufferSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyBufferSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyReadRequestsDataPoint adds a data point to newrelicmysql.myisam.key_read_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyReadRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyReadRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyReadsDataPoint adds a data point to newrelicmysql.myisam.key_reads metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyReadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyReads.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyWriteRequestsDataPoint adds a data point to newrelicmysql.myisam.key_write_requests metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyWriteRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyWriteRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlMyisamKeyWritesDataPoint adds a data point to newrelicmysql.myisam.key_writes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlMyisamKeyWritesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlMyisamKeyWrites.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetAbortedClientsDataPoint adds a data point to newrelicmysql.net.aborted_clients metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetAbortedClientsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetAbortedClients.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetAbortedConnectsDataPoint adds a data point to newrelicmysql.net.aborted_connects metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetAbortedConnectsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetAbortedConnects.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetConnectionsDataPoint adds a data point to newrelicmysql.net.connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxConnectionsDataPoint adds a data point to newrelicmysql.net.max_connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxConnectionsAvailableDataPoint adds a data point to newrelicmysql.net.max_connections_available metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxConnectionsAvailableDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxConnectionsAvailable.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlNetMaxUsedConnectionsDataPoint adds a data point to newrelicmysql.net.max_used_connections metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlNetMaxUsedConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlNetMaxUsedConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceBytesReceivedDataPoint adds a data point to newrelicmysql.performance.bytes_received metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceBytesReceivedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceBytesReceived.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceBytesSentDataPoint adds a data point to newrelicmysql.performance.bytes_sent metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceBytesSentDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceBytesSent.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpDiskTablesDataPoint adds a data point to newrelicmysql.performance.created_tmp_disk_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpDiskTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpDiskTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpFilesDataPoint adds a data point to newrelicmysql.performance.created_tmp_files metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceCreatedTmpTablesDataPoint adds a data point to newrelicmysql.performance.created_tmp_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceCreatedTmpTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceCreatedTmpTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerCommitDataPoint adds a data point to newrelicmysql.performance.handler_commit metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerCommitDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerCommit.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerDeleteDataPoint adds a data point to newrelicmysql.performance.handler_delete metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerDeleteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerDelete.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerPrepareDataPoint adds a data point to newrelicmysql.performance.handler_prepare metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerPrepareDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerPrepare.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadFirstDataPoint adds a data point to newrelicmysql.performance.handler_read_first metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadFirstDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadFirst.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadKeyDataPoint adds a data point to newrelicmysql.performance.handler_read_key metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadKeyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadKey.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadNextDataPoint adds a data point to newrelicmysql.performance.handler_read_next metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadNextDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadNext.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadPrevDataPoint adds a data point to newrelicmysql.performance.handler_read_prev metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadPrevDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadPrev.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadRndDataPoint adds a data point to newrelicmysql.performance.handler_read_rnd metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadRndDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadRnd.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerReadRndNextDataPoint adds a data point to newrelicmysql.performance.handler_read_rnd_next metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerReadRndNextDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerReadRndNext.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerRollbackDataPoint adds a data point to newrelicmysql.performance.handler_rollback metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerRollbackDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerRollback.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerUpdateDataPoint adds a data point to newrelicmysql.performance.handler_update metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerUpdateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerUpdate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceHandlerWriteDataPoint adds a data point to newrelicmysql.performance.handler_write metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceHandlerWriteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceHandlerWrite.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceKeyCacheUtilizationDataPoint adds a data point to newrelicmysql.performance.key_cache_utilization metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceKeyCacheUtilizationDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricNewrelicmysqlPerformanceKeyCacheUtilization.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceMaxPreparedStmtCountDataPoint adds a data point to newrelicmysql.performance.max_prepared_stmt_count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceMaxPreparedStmtCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceMaxPreparedStmtCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenFilesDataPoint adds a data point to newrelicmysql.performance.open_files metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenTablesDataPoint adds a data point to newrelicmysql.performance.open_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceOpenedTablesDataPoint adds a data point to newrelicmysql.performance.opened_tables metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceOpenedTablesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceOpenedTables.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformancePerformanceSchemaDigestLostDataPoint adds a data point to newrelicmysql.performance.performance_schema_digest_lost metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformancePerformanceSchemaDigestLostDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformancePerformanceSchemaDigestLost.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformancePreparedStmtCountDataPoint adds a data point to newrelicmysql.performance.prepared_stmt_count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformancePreparedStmtCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformancePreparedStmtCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheFreeBlocksDataPoint adds a data point to newrelicmysql.performance.qcache_free_blocks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheFreeBlocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheFreeBlocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheFreeMemoryDataPoint adds a data point to newrelicmysql.performance.qcache_free_memory metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheFreeMemoryDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheFreeMemory.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheHitsDataPoint adds a data point to newrelicmysql.performance.qcache_hits metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheHitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheHits.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheInsertsDataPoint adds a data point to newrelicmysql.performance.qcache_inserts metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheInsertsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheInserts.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheLowmemPrunesDataPoint adds a data point to newrelicmysql.performance.qcache_lowmem_prunes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheLowmemPrunesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheLowmemPrunes.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheNotCachedDataPoint adds a data point to newrelicmysql.performance.qcache_not_cached metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheNotCachedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheNotCached.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheQueriesInCacheDataPoint adds a data point to newrelicmysql.performance.qcache_queries_in_cache metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheQueriesInCacheDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheQueriesInCache.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheSizeDataPoint adds a data point to newrelicmysql.performance.qcache_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQcacheTotalBlocksDataPoint adds a data point to newrelicmysql.performance.qcache_total_blocks metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQcacheTotalBlocksDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQcacheTotalBlocks.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceQuestionsDataPoint adds a data point to newrelicmysql.performance.questions metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceQuestionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceQuestions.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectFullJoinDataPoint adds a data point to newrelicmysql.performance.select_full_join metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectFullJoinDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectFullJoin.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectFullRangeJoinDataPoint adds a data point to newrelicmysql.performance.select_full_range_join metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectFullRangeJoinDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectFullRangeJoin.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectRangeDataPoint adds a data point to newrelicmysql.performance.select_range metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectRangeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectRange.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectRangeCheckDataPoint adds a data point to newrelicmysql.performance.select_range_check metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectRangeCheckDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectRangeCheck.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSelectScanDataPoint adds a data point to newrelicmysql.performance.select_scan metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSelectScanDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSelectScan.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSlowQueriesDataPoint adds a data point to newrelicmysql.performance.slow_queries metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSlowQueriesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSlowQueries.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortMergePassesDataPoint adds a data point to newrelicmysql.performance.sort_merge_passes metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortMergePassesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortMergePasses.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortRangeDataPoint adds a data point to newrelicmysql.performance.sort_range metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortRangeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortRange.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortRowsDataPoint adds a data point to newrelicmysql.performance.sort_rows metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortRowsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortRows.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceSortScanDataPoint adds a data point to newrelicmysql.performance.sort_scan metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceSortScanDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceSortScan.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksImmediateDataPoint adds a data point to newrelicmysql.performance.table_locks_immediate metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksImmediateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksImmediate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksImmediateRateDataPoint adds a data point to newrelicmysql.performance.table_locks_immediate.rate metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksImmediateRateDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksImmediateRate.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableLocksWaitedDataPoint adds a data point to newrelicmysql.performance.table_locks_waited metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableLocksWaitedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableLocksWaited.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceTableOpenCacheDataPoint adds a data point to newrelicmysql.performance.table_open_cache metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceTableOpenCacheDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceTableOpenCache.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadCacheSizeDataPoint adds a data point to newrelicmysql.performance.thread_cache_size metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadCacheSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadCacheSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsCachedDataPoint adds a data point to newrelicmysql.performance.threads_cached metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsCachedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsCached.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsConnectedDataPoint adds a data point to newrelicmysql.performance.threads_connected metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsConnectedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsConnected.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsCreatedDataPoint adds a data point to newrelicmysql.performance.threads_created metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsCreatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsCreated.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlPerformanceThreadsRunningDataPoint adds a data point to newrelicmysql.performance.threads_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlPerformanceThreadsRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlPerformanceThreadsRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlQueryCountDataPoint adds a data point to newrelicmysql.query.count metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlQueryCountDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlQueryCount, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlQueryCount.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// RecordNewrelicmysqlReplicationExecMasterLogPosDataPoint adds a data point to newrelicmysql.replication.exec_master_log_pos metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationExecMasterLogPosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationExecMasterLogPos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationLastIoErrnoDataPoint adds a data point to newrelicmysql.replication.last_io_errno metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationLastIoErrnoDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationLastIoErrno.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationLastSQLErrnoDataPoint adds a data point to newrelicmysql.replication.last_sql_errno metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationLastSQLErrnoDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationLastSQLErrno.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationReadMasterLogPosDataPoint adds a data point to newrelicmysql.replication.read_master_log_pos metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationReadMasterLogPosDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationReadMasterLogPos.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationRelayLogSpaceDataPoint adds a data point to newrelicmysql.replication.relay_log_space metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationRelayLogSpaceDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationRelayLogSpace.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSecondsBehindMasterDataPoint adds a data point to newrelicmysql.replication.seconds_behind_master metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSecondsBehindMasterDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSecondsBehindMaster.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveIoRunningDataPoint adds a data point to newrelicmysql.replication.slave_io_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveIoRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveIoRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveRunningDataPoint adds a data point to newrelicmysql.replication.slave_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlReplicationSlaveSQLRunningDataPoint adds a data point to newrelicmysql.replication.slave_sql_running metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlReplicationSlaveSQLRunningDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricNewrelicmysqlReplicationSlaveSQLRunning.recordDataPoint(mb.startTime, ts, val)
}

// RecordNewrelicmysqlUptimeDataPoint adds a data point to newrelicmysql.uptime metric.
func (mb *MetricsBuilder) RecordNewrelicmysqlUptimeDataPoint(ts pcommon.Timestamp, inputVal string) error {
	val, err := strconv.ParseInt(inputVal, 10, 64)
	if err != nil {
		return fmt.Errorf("failed to parse int64 for NewrelicmysqlUptime, value was %s: %w", inputVal, err)
	}
	mb.metricNewrelicmysqlUptime.recordDataPoint(mb.startTime, ts, val)
	return nil
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
